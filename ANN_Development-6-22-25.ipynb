{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9de4292-ca97-413d-b753-f70b4f4a9a9f",
   "metadata": {},
   "source": [
    "The goal of this workbook is to build and train the artificial neural network (ANNs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e3083-6fe3-4990-b7c0-b6f867bb71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First run the functions workbook \n",
    "%run Functions-6-22-25.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349fb59a-4ced-4a8a-be60-b8ec2d47cfab",
   "metadata": {},
   "source": [
    "The first ANN will be trained from Data_Set_3. The ANN will have a binary task to identify which files are simulated with a native oxide coated Si wafer substrate, and which files have a SLG substrate.  \n",
    "\n",
    "This ANN will be refered to as ANN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b744b-e180-4be3-9b76-814699714884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_CL(path):\n",
    "\n",
    "    os.chdir(r\"XXXX\") # PATH WHERE OPTICAL PROPERTIES ARE STORED\n",
    "    file =  r\"XXXX\" # FILE FOR SLG\n",
    "    SLG = pd.read_csv(file) # READ SLG\n",
    "    SLG.name = 'SLG'\n",
    "    E = SLG['Energy (eV)'] # TAKE ENERGY\n",
    "\n",
    "    os.chdir(path)\n",
    "    files = glob.glob(path + \"/*.csv\") # Loads in the files for the 1st path\n",
    "\n",
    "\n",
    "    Answer_Key_Bulk_Thickness = []\n",
    "    Answer_Key_EMA_Thickness = []\n",
    "    Answer_Key_NTVE_Thickness = []\n",
    "    Answer_Key_AngOff = []\n",
    "\n",
    "    Answer_Key_Voidfrac =[]\n",
    "\n",
    "    Answer_Key_Einf =[]\n",
    "    Answer_Key_Amp =[]\n",
    "    Answer_Key_Et = []\n",
    "    Answer_Key_Br =[]\n",
    "    Answer_Key_Eo =[]\n",
    "    Answer_Key_Eg =[]\n",
    "    Answer_Key_Ep =[]\n",
    "    Answer_Key_EMA_bool = []\n",
    "    Answer_Key_EMA_thickness = []\n",
    "    Answer_Key_Substrate =[]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    file_name = []\n",
    "    data =[]\n",
    "\n",
    "    #Answer_Key_AngOff =[]\n",
    "\n",
    "    for i in range(len(files)): # Iterate for every simulated file\n",
    "\n",
    "\n",
    "            #Load in data from CSV files\n",
    "            df = pd.read_csv(files[i], index_col=None)\n",
    "\n",
    "            #Get Answer Key from CSV file\n",
    "            a = str(df.iloc[0, 0])\n",
    "\n",
    "            \n",
    "            #print(a)\n",
    "            #Store Answer Key as \"Filename\"\n",
    "            file_name.append(a)\n",
    "            #print(a)\n",
    "            # Remove Answer Key from the data frame to avoid NAN values\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "            # Remove data that we do not want to train the Neural Network on. This may change with other iterations\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            df.insert(0, 'Energy (eV)', E)\n",
    "\n",
    "\n",
    "            #stores the data the neural network will be trained on. (N,C,S in this case)\n",
    "            data.append(df)\n",
    "\n",
    "\n",
    "            #Splits the answer key (a) into different sections delimited by the \"_\" symbol\n",
    "            a = a.split('\\\\')[-1].split('_')\n",
    "            #print(len(a))\n",
    "            #print(a)\n",
    "            \n",
    "            #Stores the Ep parameter for this data set\n",
    "            Answer_Key_Ep.append(float(a[1]))\n",
    "        \n",
    "            #Stores the Eg parameter for this data set\n",
    "            Answer_Key_Eg.append(float(a[3]))\n",
    "\n",
    "            #Stores the Eo parameter for this data set\n",
    "            Answer_Key_Eo.append(float(a[5]))\n",
    "\n",
    "            #Stores the Br parameter for this data set\n",
    "            Answer_Key_Br.append(float(a[7]))\n",
    "\n",
    "            #Stores the Amp parameter for this data set\n",
    "            Answer_Key_Amp.append(float(a[9]))\n",
    "\n",
    "            #Stores the Einf parameter for this data set\n",
    "            Answer_Key_Einf.append(float(a[13]))  \n",
    "\n",
    "\n",
    "            if len(a) == 33: \n",
    "                \n",
    "                Answer_Key_EMA_bool.append(0) # EMA 0 means no EMA\n",
    "                Answer_Key_EMA_thickness.append(0)\n",
    "                Answer_Key_Bulk_Thickness.append(float(a[14].split('nm')[0]))\n",
    "                Answer_Key_Substrate.append(1) # 1 means soda-lime glass\n",
    "    \n",
    "            if len(a) == 51:\n",
    "\n",
    "                Answer_Key_EMA_bool.append(1)  # EMA 1 means there is an EMA layer\n",
    "                Answer_Key_EMA_thickness.append(float(a[17].split('nm')[0]))\n",
    "                Answer_Key_Bulk_Thickness.append(float(a[32].split('nm')[0]))\n",
    "                Answer_Key_Substrate.append(1) # 1 means soda-lime glass\n",
    "\n",
    "            if len(a) == 18: \n",
    "\n",
    "                Answer_Key_EMA_bool.append(0) # EMA 0 means no EMA\n",
    "                Answer_Key_EMA_thickness.append(0)\n",
    "                Answer_Key_Bulk_Thickness.append(float(a[14].split('nm')[0]))\n",
    "                Answer_Key_Substrate.append(0) # 0 means Si wafer\n",
    "    \n",
    "            if len(a) == 36:\n",
    "\n",
    "                Answer_Key_EMA_bool.append(1)  # EMA 1 means there is an EMA layer\n",
    "                Answer_Key_EMA_thickness.append(float(a[17].split('nm')[0]))\n",
    "                Answer_Key_Bulk_Thickness.append(float(a[32].split('nm')[0]))\n",
    "                Answer_Key_Substrate.append(0) # 0 means Si wafer\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    train_data = np.array(data)\n",
    "    train_files = np.array(file_name)\n",
    "    train_label_Ep =np.array(Answer_Key_Ep)\n",
    "    train_label_Eg =np.array(Answer_Key_Eg)\n",
    "    train_label_Eo =np.array(Answer_Key_Eo)\n",
    "    train_label_Br =np.array(Answer_Key_Br) \n",
    "    train_label_Amp =np.array(Answer_Key_Amp)\n",
    "    train_label_Einf =np.array(Answer_Key_Einf)\n",
    "    train_label_BulkT = np.array(Answer_Key_Bulk_Thickness)\n",
    "    train_label_EMA_bool = np.array(Answer_Key_EMA_bool)\n",
    "    train_label_EMA_Thickness = np.array(Answer_Key_EMA_thickness)\n",
    "    train_label_substrate = np.array(Answer_Key_Substrate)\n",
    "    \n",
    "\n",
    "    return(\n",
    "        train_files, \n",
    "        train_data,\n",
    "        train_label_Ep,\n",
    "        train_label_Eg,\n",
    "        train_label_Eo, \n",
    "        train_label_Br, \n",
    "        train_label_Amp, \n",
    "        train_label_Einf, \n",
    "        train_label_BulkT,\n",
    "        train_label_EMA_bool,\n",
    "        train_label_EMA_Thickness,\n",
    "        train_label_substrate\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081b96b-caf2-4dca-8ba3-98e4255554da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First load in the training from Data_Set_3. \n",
    "\n",
    "# Identify the location of the data. Please put path here.\n",
    "path = r'C:\\Users\\bordo\\Documents\\UToledo\\Research\\ML\\a-Si-Paper\\Cody-Lorentz_Reviewer_Comments\\SE_Sim\\test'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 3 in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_CL\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df9602-666e-4508-9181-0c1615b38597",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(train_Ep[idx])\n",
    "print(train_Eg[idx])\n",
    "print(train_Eo[idx])\n",
    "print(train_Br[idx])\n",
    "print(train_Amp[idx])\n",
    "print(train_Einf[idx])\n",
    "print(train_BulkT[idx])\n",
    "print(train_EMA_bool[idx])\n",
    "print(train_EMAT[idx])\n",
    "print(train_Sub_bool[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc9442-8059-4ba7-a9b2-4554395ea204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d8792-7cb3-432e-9bf4-c5e657148c35",
   "metadata": {},
   "source": [
    "Now that the data reading function is working. The data can be loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5f01c-4242-4e6e-a210-b961bf366e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load in the training from Data_Set_3. \n",
    "\n",
    "# Identify the location of the data. Please put path here.\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 3 in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_CL\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f4493-0614-41a5-8163-0d7af28312a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next load in the validation data from Data_Set_3.\n",
    "\n",
    "# Identify the location of the data. Please put path here.\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 3 in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_CL\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d62c9-3ea8-4b7d-b2e4-f191800d2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the data in a way that is compatable with the ANN. \n",
    "\n",
    "# Identify the input data used for training the ANN\n",
    "x_train = train_data # the train_data variable was collected earlier. It contains the ellipsometric spectra for each simulation \n",
    "\n",
    "# Identify what the output should look like. In this case, a binary identifying the substrate.\n",
    "y_train = train_Sub_bool # the train_Sub_bool variable was collected earlier. It contains a binary where \"0\" means Si wafer and \"1\" means SLG\n",
    "\n",
    "# Identify the input for the validation set \n",
    "x_val = val_data\n",
    "\n",
    "# Identify the output for the validation set\n",
    "y_val = val_Sub_bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc782d-5253-49c9-af08-91c6c522f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to build the model \n",
    "\n",
    "# Define the Input layer of the ANN.\n",
    "input_shape = (697, 4) # 697 data points with the 4 columns being (Photon Energy, N, C, S)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "x = tfl.Flatten()(input_NCS) # flatten the data into one array with length 2,788\n",
    "\n",
    "# Define the hidden layers of the ANN. \n",
    "#'leaky_relu' is chosen as the activation function because it can handle negative numbers. N, C, and S can all be negative. \n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=16, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=8, activation='leaky_relu')(x)\n",
    "\n",
    "# Now define the output layer. The sigmoid activation function is chosen because it is bound between [0,1]. Perfect for binary problems. \n",
    "x = tfl.Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 5000  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Binary Crossentropy. It is used for binary tasks.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.BinaryCrossentropy()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in this model post-training.#############################\n",
    "os.chdir(r\"XXXX\") # put path to your model here\n",
    "model = load_model('ANN1.keras')\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=50, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN1.weights.h5')\n",
    "model.save('ANN1.keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fa28e-54e7-4ce4-a370-f616998637bf",
   "metadata": {},
   "source": [
    "The next ANN will take ellipsometric spectra as an input and make a binary decision on if a surface layer described as a Bruggeman Effective Medium Approximation (EMA) with 50% void and 50% bulk film is appropriate for the analysis. \n",
    "\n",
    "This ANN will be refered to as ANN2 and is also trained using Data_Set_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0490195-b9d6-4af4-bce4-d4271cd01166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next reformat the data in a way that is compatable with the ANN. \n",
    "\n",
    "# Identify the input data used for training the ANN\n",
    "x_train = train_data # the train_data variable was collected earlier. It contains the ellipsometric spectra for each simulation \n",
    "\n",
    "# Identify what the output should look like. In this case, a boolean identifying the presence of a surface layer.\n",
    "y_train = train_EMA_bool # the train_EMA_bool variable was collected earlier. It contains a boolean where \"0\" means no surface layer and \"1\" means there is a surface layer \n",
    "\n",
    "# Identify the input for the validation set \n",
    "x_val = val_data\n",
    "\n",
    "# Identify the output for the validation set\n",
    "y_val = val_EMA_bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e86f4-ae75-42e6-897b-7a0eb92282a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now to build the model \n",
    "\n",
    "# Define the Input layer of the ANN.\n",
    "input_shape = (697, 4) # 697 data points with the 4 columns being (Photon Energy, N, C, S)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "x = tfl.Flatten()(input_NCS) # flatten the data into one array with length 2,788\n",
    "\n",
    "# Define the hidden layers of the ANN. \n",
    "#'leaky_relu' is chosen as the activation function because it can handle negative numbers. N, C, and S can all be negative. \n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=16, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=8, activation='leaky_relu')(x)\n",
    "\n",
    "# Now define the output layer. The sigmoid activation function is chosen because it is bound between [0,1]. Perfect for binary problems. \n",
    "x = tfl.Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-6 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 2500  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Binary Crossentropy. It is used for binary tasks.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.BinaryCrossentropy()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "os.chdir(r\"XXXX\") # put path to your model here\n",
    "model = load_model('ANN2.keras')\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN2.weights.h5')\n",
    "model.save('ANN2.keras')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b0757-bcad-4533-b577-bdd76ae2cf69",
   "metadata": {},
   "source": [
    "ANN1 can predict the substrate, ANN2 can predict if a surface layer is needed. Now ANNs that can generate the parameter values for the Cody-Loretnz oscillator and component layer thicknesses are needed.\n",
    "\n",
    "The achitecture was developed previously via trial and error in a direct search. The current architecture is good enough for this demonstration, but further optimization could be done by changing the hyperparameters such as: number of layers, types of hidden layers used, number of nodes in each layer, activation functions, learning rate etc. \n",
    "\n",
    "ANN3 will be the one trained to handel the Si / Si native oxide / bulk structure using data from Data_Set_1.\n",
    "ANN4 will be the one trained to handel the SLG / SLG bulk interface / bulk / surface layer structure using data from Data_Set_2.\n",
    "\n",
    "The different training set sizes are 10, 100, 1000, 10000, 50000, 100000. \n",
    "The size of the validation and test data are 1, 10, 1000, 5000, 10000 always being 10% the size of the training set. \n",
    "\n",
    "The first model will be trained ANN3 with a training set size of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed56970-8ac4-435b-aeae-b8f5a02136c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 10 \n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 1 with only 10 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b0dd9-f731-4daa-bbf2-3754455fd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 1 \n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 1 with only 1 file in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71afb64c-9416-41b3-9b71-6c8d9c9f5d90",
   "metadata": {},
   "source": [
    "These next ANNs will be performing a regression based task instead of a binary task. When performing regression tasks, it can be benificial to standardize data such that the mean value is always 0 and the standard deviation is 1. This allows all parameters to have an equal weighting when training the model despite the parameters having different units and magnitudes (such as Amplitude, Eg, and Bulk thickness).\n",
    "\n",
    "The function below will be used to standardize data for ANN3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d004350a-f38b-42e3-84f2-302b0d9d132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardize_data_ANN3(path, filename):\n",
    "\n",
    "    #Path should be the directory you want to save summary csv files at. \n",
    "    #Filename will be used to name the summary files\n",
    "   \n",
    "    ####### Standardize the data ##############\n",
    "\n",
    "    #Generate mean values from training data.\n",
    "    mean_Ep = np.mean(train_Ep)\n",
    "    mean_Eg = np.mean(train_Eg)\n",
    "    mean_Eo = np.mean(train_Eo)\n",
    "    mean_Br = np.mean(train_Br)\n",
    "    mean_Amp = np.mean(train_Amp)\n",
    "    mean_Einf = np.mean(train_Einf) \n",
    "    mean_BulkT = np.mean(train_BulkT)\n",
    "    \n",
    "    # Store the mean values in  a dictionary for future use\n",
    "    Standard_Means = {\n",
    "    \n",
    "        'Mean_Ep' :  mean_Ep,\n",
    "        'Mean_Eg' :  mean_Eg,\n",
    "        'Mean_Eo' :  mean_Eo,\n",
    "        'Mean_Br' :  mean_Br,\n",
    "        'Mean_Amp' : mean_Amp,\n",
    "        'Mean_Einf': mean_Einf,\n",
    "        'Mean_BulkT': mean_BulkT \n",
    "    }\n",
    "    \n",
    "    # Generate standard deviations from training data.\n",
    "    std_Ep = np.std(train_Ep)\n",
    "    std_Eg = np.std(train_Eg)\n",
    "    std_Eo = np.std(train_Eo)\n",
    "    std_Br = np.std(train_Br)\n",
    "    std_Amp = np.std(train_Amp)\n",
    "    std_Einf = np.std(train_Einf) \n",
    "    std_BulkT = np.std(train_BulkT)\n",
    "    \n",
    "    # Store the standard deviations in  a dictionary for future use\n",
    "    Standard_Std = {\n",
    "    \n",
    "        'std_Ep' :  std_Ep,\n",
    "        'std_Eg' :  std_Eg,\n",
    "        'std_Eo' :  std_Eo,\n",
    "        'std_Br' :  std_Br,\n",
    "        'std_Amp' : std_Amp,\n",
    "        'std_Einf': std_Einf,\n",
    "        'std_BulkT': std_BulkT,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Generate standardized parameter values for training set \n",
    "    standardized_train_Ep = (train_Ep - mean_Ep) / std_Ep\n",
    "    standardized_train_Eg = (train_Eg - mean_Eg) / std_Eg\n",
    "    standardized_train_Eo = (train_Eo - mean_Eo) / std_Eo\n",
    "    standardized_train_Br = (train_Br - mean_Br) / std_Br\n",
    "    standardized_train_Amp = (train_Amp - mean_Amp) / std_Amp\n",
    "    standardized_train_BulkT = (train_BulkT - mean_BulkT) / std_BulkT\n",
    "    \n",
    "    # Generate standardized parameter values for validation\n",
    "    standardized_val_Ep = (val_Ep - mean_Ep) / std_Ep\n",
    "    standardized_val_Eg = (val_Eg - mean_Eg) / std_Eg\n",
    "    standardized_val_Eo = (val_Eo - mean_Eo) / std_Eo\n",
    "    standardized_val_Br = (val_Br - mean_Br) / std_Br\n",
    "    standardized_val_Amp = (val_Amp - mean_Amp) / std_Amp\n",
    "    standardized_val_BulkT = (val_BulkT - mean_BulkT) / std_BulkT\n",
    "    \n",
    "    \n",
    "    #Now the data will be formatted to be input into the ANN\n",
    "    # x_train will be the input data for training\n",
    "    # y_train will be the output data for training (the output data is commonly refered to as \"Training Labels\")\n",
    "    \n",
    "    x_train = train_data\n",
    "\n",
    "    # The ANNs will have data in the format of [ Ep, Eg, Eo, Br, Amp, BulkT] where \"BulkT\" is the thickness of the bulk film and the other\n",
    "    # parameters are the parameters associated with the Cody-lorentz oscillator. The training data is defined to follow this format. \n",
    "    y_train = np.column_stack((\n",
    "        standardized_train_Ep, \n",
    "        standardized_train_Eg, \n",
    "        standardized_train_Eo, \n",
    "        standardized_train_Br, \n",
    "        standardized_train_Amp, \n",
    "        standardized_train_BulkT,\n",
    "    ))\n",
    "       \n",
    "    # x_val will be the input data for the validation set\n",
    "    # y_val will be the output data for the validation set\n",
    "    x_val = val_data\n",
    "    \n",
    "    y_val = np.column_stack((\n",
    "      \n",
    "     standardized_val_Ep,\n",
    "     standardized_val_Eg,\n",
    "     standardized_val_Eo,\n",
    "     standardized_val_Br,\n",
    "     standardized_val_Amp,\n",
    "     standardized_val_BulkT,\n",
    "    \n",
    "    ))\n",
    "\n",
    "\n",
    "    # Store data in path location\n",
    "    os.chdir(path) # path where data mean and standard deviations will be store\n",
    "    \n",
    "    ######## Store Mean ###############################\n",
    "    import csv\n",
    "    # Define the filename\n",
    "    name =  filename + \"_Mean.csv\"\n",
    "    \n",
    "    # Get the headers from the dictionary\n",
    "    headers = Standard_Means.keys()\n",
    "    \n",
    "    # Open a file for writing\n",
    "    with open(name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Write the data rows\n",
    "        row = [Standard_Means[key] for key in headers]  # Extract the values directly\n",
    "        writer.writerow(row)\n",
    "        \n",
    "    print(f\"Data saved to {filename}\")\n",
    "    \n",
    "    ######## Store std ###############################\n",
    "    \n",
    "    name =   filename + \"_std.csv\"\n",
    "    \n",
    "    # Get the headers from the dictionary\n",
    "    headers = Standard_Std.keys()\n",
    "    \n",
    "    # Open a file for writing\n",
    "    with open(name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Write the data rows\n",
    "        row = [Standard_Std[key] for key in headers]  # Extract the values directly\n",
    "        writer.writerow(row)\n",
    "        \n",
    "    print(f\"Data saved to {filename}\")\n",
    "    \n",
    "    return ( x_train, y_train, x_val, y_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e54d4-1daa-41bd-88f9-3f481ea82aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set1_10' # 1st part of the file name\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN3(path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bf862-1396-45d0-b921-093f4d285e65",
   "metadata": {},
   "source": [
    "Now the training and validation data is defined. The next ANN can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cf142-6e17-41e0-8626-95aef4af2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 6 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT]\n",
    "x = tfl.Dense(units=6, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 2  # How many batches will be done before a decay step. \n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "# os.chdir(r\"XXXX\") # put path to your model here\n",
    "# model = load_model('ANN3_10.keras')\n",
    "\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN3_10.weights.h5') #### Filename is ANN3_(# of training files)\n",
    "model.save('ANN3_10.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22247be5-dd86-4ebd-9b89-323caeca091e",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36854d81-d338-477c-af42-b07ac2d02b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 100\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 1 with 100 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40667f8-f603-45bd-9dab-0c81a5dfea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 10 \n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 1 with 10 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3ef11-90d1-446a-8052-3b4d6ff99158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function to standardize data and store the mean and standard deviation for later use. \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set1_100'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN3(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaf7ff-bc08-4208-8ae4-59fb0858d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 6 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT]\n",
    "x = tfl.Dense(units=6, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 5  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "# os.chdir(r\"XXXX\") # put path to your model here\n",
    "# model = load_model('ANN3_100.keras')\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN3_100.weights.h5') #### Filename is ANN3_(# of training files)\n",
    "model.save('ANN3_100.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768508c-b643-431a-a64d-359ac0be5c31",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5150bf4-b2fb-4dc5-b6c8-f816746ead20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 1000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 1 with only 1000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72082b85-999c-488f-84bc-06b3b2d5f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 100 \n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 1 with only 100 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab15f9-969b-468f-b832-b3377888738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set1_1000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN3(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0c5f0-23ae-464f-a353-819b7a19f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 6 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT]\n",
    "x = tfl.Dense(units=6, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 50  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "# os.chdir(r\"XXXX\") # put path to your model here\n",
    "# model = load_model('ANN3_1000.keras')\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\") ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "model.save_weights('ANN3_1000.weights.h5') #### Filename is ANN3_(# of training files)\n",
    "model.save('ANN3_1000.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd2f1e-001a-4f04-ab58-b678174bfbc5",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4f01e1-68e6-427c-ab3e-232451456a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 10000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 1 with only 10000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7252acf-9104-414f-9f50-52371464b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 1000 \n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 1 with only 1000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_CL\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb705b2-6b03-43ed-8fe3-d950a4494398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set1_10000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN3(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc7ec5-2dd7-437a-99ef-d4e1cd56d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 6 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT]\n",
    "x = tfl.Dense(units=6, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-6 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 500  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "os.chdir(r\"XXXX\") # put path to your model here\n",
    "model = load_model('ANN3_10000.keras')\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN3_10000.weights.h5') #### Filename is ANN3_(# of training files)\n",
    "model.save('ANN3_10000.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f94d832-8572-48c8-8f53-5e37781ccd53",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82171975-32a7-4559-b614-760a891d9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 50000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 1 with only 50000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0613a6b-eaec-4f38-8556-e5f3aed8ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 5000 \n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 1 with only 5000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853a06b-f95e-412b-8715-5b41b73a7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set1_50000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN3(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a174f1-2097-449e-bae7-c83d3203e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 6 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT]\n",
    "x = tfl.Dense(units=6, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-6 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 2500  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "#os.chdir(r\"XXXX\") # put path to your model here\n",
    "#model = load_model('ANN3_50000.keras')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN3_50000.weights.h5') #### Filename is ANN3_(# of training files)\n",
    "model.save('ANN3_50000.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3efe1f-3f6a-4192-b72b-de879757162d",
   "metadata": {},
   "source": [
    "The final model will be trained with a training set size of 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8e92c-5c65-4503-b022-3ea85af9ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 100000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 1 with only 100000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0a753-a17e-4f4e-804b-ad5d2bd0ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 10000 \n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 1 with only 10000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bb27f-12c4-4007-a283-6728dc7d8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set1_100000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN3(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909b080-8c60-4ac3-84dd-aa5dbf45b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 6 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT]\n",
    "x = tfl.Dense(units=6, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-6 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 5000  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "os.chdir(r\"XXXX\") # put path to your model here\n",
    "model = load_model('ANN3_100000.keras')\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN3_100000.weights.h5') #### Filename is ANN3_(# of training files)\n",
    "model.save('ANN3_100000.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebcd9b-1dca-412a-8932-88a654fdc131",
   "metadata": {},
   "source": [
    "Now moving on to ANN4 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c0db1-7f65-4cdd-bda1-b9b1a58982ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 10\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 2 with 10 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be51f0-2b87-4c0d-b43e-46de27d88f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 1\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 2 with 1 file in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd02de21-a797-43ba-b30b-b3429e58ab00",
   "metadata": {},
   "source": [
    "These next ANNs will be performing a regression based task instead of a binary task. When performing regression tasks, it can be benificial to standardize data such that the mean value is always 0 and the standard deviation is 1. This allows all parameters to have an equal weighting when training the model despite the parameters having different units and magnitudes (such as Amplitude, Eg, and Bulk thickness).\n",
    "\n",
    "The function below will be used to standardize data for ANN4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964179b-d726-46ad-9ead-c5660c14a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new function to standardize the data is defined to handel the additional EMA term describing the surface layer/\n",
    "# This function is nearly identical otherwise\n",
    "\n",
    "def Standardize_data_ANN4(path, filename):\n",
    "   \n",
    "    ####### Standardize the data ##############\n",
    "\n",
    "    #Generate mean values from training data.\n",
    "    mean_Ep = np.mean(train_Ep)\n",
    "    mean_Eg = np.mean(train_Eg)\n",
    "    mean_Eo = np.mean(train_Eo)\n",
    "    mean_Br = np.mean(train_Br)\n",
    "    mean_Amp = np.mean(train_Amp)\n",
    "    mean_Einf = np.mean(train_Einf) \n",
    "    mean_BulkT = np.mean(train_BulkT)\n",
    "    mean_EMAT = np.mean(train_EMAT)\n",
    "    \n",
    "    # Store the mean values in  a dictionary for future use\n",
    "    Standard_Means = {\n",
    "    \n",
    "        'Mean_Ep' :  mean_Ep,\n",
    "        'Mean_Eg' :  mean_Eg,\n",
    "        'Mean_Eo' :  mean_Eo,\n",
    "        'Mean_Br' :  mean_Br,\n",
    "        'Mean_Amp' : mean_Amp,\n",
    "        'Mean_Einf': mean_Einf,\n",
    "        'Mean_BulkT': mean_BulkT,\n",
    "        'Mean_EMAT': mean_EMAT,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Generate standard deviations from training data.\n",
    "    std_Ep = np.std(train_Ep)\n",
    "    std_Eg = np.std(train_Eg)\n",
    "    std_Eo = np.std(train_Eo)\n",
    "    std_Br = np.std(train_Br)\n",
    "    std_Amp = np.std(train_Amp)\n",
    "    std_Einf = np.std(train_Einf) \n",
    "    std_BulkT = np.std(train_BulkT)\n",
    "    std_EMAT = np.std(train_EMAT)\n",
    "    \n",
    "    \n",
    "    # Store the standard deviations in  a dictionary for future use\n",
    "    Standard_Std = {\n",
    "    \n",
    "        'std_Ep' :  std_Ep,\n",
    "        'std_Eg' :  std_Eg,\n",
    "        'std_Eo' :  std_Eo,\n",
    "        'std_Br' :  std_Br,\n",
    "        'std_Amp' : std_Amp,\n",
    "        'std_Einf': std_Einf,\n",
    "        'std_BulkT': std_BulkT,\n",
    "        'std_EMAT': std_EMAT,\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Generate standardized parameter values for training set \n",
    "    standardized_train_Ep = (train_Ep - mean_Ep) / std_Ep\n",
    "    standardized_train_Eg = (train_Eg - mean_Eg) / std_Eg\n",
    "    standardized_train_Eo = (train_Eo - mean_Eo) / std_Eo\n",
    "    standardized_train_Br = (train_Br - mean_Br) / std_Br\n",
    "    standardized_train_Amp = (train_Amp - mean_Amp) / std_Amp\n",
    "    standardized_train_BulkT = (train_BulkT - mean_BulkT) / std_BulkT\n",
    "    standardized_train_EMAT = (train_EMAT - mean_EMAT) / std_EMAT\n",
    "    \n",
    "    # Generate standardized parameter values for validation\n",
    "    standardized_val_Ep = (val_Ep - mean_Ep) / std_Ep\n",
    "    standardized_val_Eg = (val_Eg - mean_Eg) / std_Eg\n",
    "    standardized_val_Eo = (val_Eo - mean_Eo) / std_Eo\n",
    "    standardized_val_Br = (val_Br - mean_Br) / std_Br\n",
    "    standardized_val_Amp = (val_Amp - mean_Amp) / std_Amp\n",
    "    standardized_val_BulkT = (val_BulkT - mean_BulkT) / std_BulkT\n",
    "    standardized_val_EMAT = (val_EMAT - mean_EMAT) / std_EMAT\n",
    "    \n",
    "    \n",
    "    #Now the data will be formatted to be input into the ANN\n",
    "    # x_train will be the input data for training\n",
    "    # y_train will be the output data for training (the output data is commonly refered to as \"Training Labels\")\n",
    "    \n",
    "    x_train = train_data\n",
    "\n",
    "    # The ANNs will have data in the format of [ Ep, Eg, Eo, Br, Amp, BulkT, EMAT] where \"BulkT\" is the thickness of the bulk film and \n",
    "    # \"EMAT\" is the thickness of the surface layer described by Bruggeman Effective Medium Approximation. The other\n",
    "    # parameters are the parameters associated with the Cody-lorentz oscillator. The training data is defined to follow this format. \n",
    "    y_train = np.column_stack((\n",
    "        standardized_train_Ep, \n",
    "        standardized_train_Eg, \n",
    "        standardized_train_Eo, \n",
    "        standardized_train_Br, \n",
    "        standardized_train_Amp, \n",
    "        standardized_train_BulkT,\n",
    "        standardized_train_EMAT,\n",
    "    ))\n",
    "       \n",
    "    # x_val will be the input data for the validation set\n",
    "    # y_val will be the output data for the validation set\n",
    "    x_val = val_data\n",
    "    \n",
    "    y_val = np.column_stack((\n",
    "      \n",
    "     standardized_val_Ep,\n",
    "     standardized_val_Eg,\n",
    "     standardized_val_Eo,\n",
    "     standardized_val_Br,\n",
    "     standardized_val_Amp,\n",
    "     standardized_val_BulkT,\n",
    "     standardized_val_EMAT,\n",
    "    \n",
    "    ))\n",
    "\n",
    "\n",
    "    # Store data in path location\n",
    "    os.chdir(path)\n",
    "    \n",
    "    ######## Store Mean ###############################\n",
    "    import csv\n",
    "    # Define the filename\n",
    "    name =  filename + \"_Mean.csv\"\n",
    "    \n",
    "    # Get the headers from the dictionary\n",
    "    headers = Standard_Means.keys()\n",
    "    \n",
    "    # Open a file for writing\n",
    "    with open(name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Write the data rows\n",
    "        row = [Standard_Means[key] for key in headers]  # Extract the values directly\n",
    "        writer.writerow(row)\n",
    "        \n",
    "    print(f\"Data saved to {filename}\")\n",
    "    \n",
    "    ######## Store std ###############################\n",
    "    \n",
    "    name =   filename + \"_std.csv\"\n",
    "    \n",
    "    # Get the headers from the dictionary\n",
    "    headers = Standard_Std.keys()\n",
    "    \n",
    "    # Open a file for writing\n",
    "    with open(name, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the headers\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Write the data rows\n",
    "        row = [Standard_Std[key] for key in headers]  # Extract the values directly\n",
    "        writer.writerow(row)\n",
    "        \n",
    "    print(f\"Data saved to {filename}\")\n",
    "    \n",
    "    return ( x_train, y_train, x_val, y_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a28cb-0d65-4279-a1cd-391f8b5e4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set2_10'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN4(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845af4b6-c972-4d98-a92b-65038f185574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 7 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT, EMAT]\n",
    "x = tfl.Dense(units=7, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 2  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "#os.chdir(r\"XXXX\") # put path to your model here\n",
    "#model = load_model('ANN4_10.keras')\n",
    "\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN4_10.weights.h5') #### Filename is ANN4_(# of training files)\n",
    "model.save('ANN4_10.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fea018-9c2c-4165-8fb3-e24e87acb343",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12796b28-5054-4a78-9570-787b591038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 100\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 2 with 100 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca2e18-6731-4ee3-87f7-3599e4a8dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 10\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 2 with 10 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380d92c-46fe-43a6-aa07-56098fbb3fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set2_100'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN4(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc140d6-ef96-403b-af14-ce347f96c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 7 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT, EMAT]\n",
    "x = tfl.Dense(units=7, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 5  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "# os.chdir(r\"XXXX\") # put path to your model here\n",
    "# model = load_model('ANN4_100.keras')\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN4_100.weights.h5') #### Filename is ANN4_(# of training files)\n",
    "model.save('ANN4_100.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f84fb-eac2-421c-a708-1e89a6963a5b",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c905d6-c4a5-4e54-8537-a83ff5c4e923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 1000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 2 with 1000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43d7a3-dc0e-41da-a329-273b82880e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 100\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 2 with 100 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4904e2-5b57-4671-af05-2b9ea15e91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set2_1000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN4(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467860aa-db68-480e-bc2f-38a270395400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 7 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT, EMAT]\n",
    "x = tfl.Dense(units=7, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 50  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "# os.chdir(r\"XXXX\") # put path to your model here\n",
    "# model = load_model('ANN4_1000.keras')\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN4_1000.weights.h5') #### Filename is ANN4_(# of training files)\n",
    "model.save('ANN4_1000.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa0097-a5b2-4385-b519-602be9aa2cd7",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79a4f1-64d2-4a16-b37a-16bc230a7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 10000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 2 with 10000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951949ae-8de1-49b9-abf4-52e2db7b9be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 1000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 2 with 1000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43c00e-0e02-4f03-bbba-01faab859ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set2_10000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN4(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68cedf-6e19-4438-b2ac-4843bceb26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 7 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT, EMAT]\n",
    "x = tfl.Dense(units=7, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-6 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 500  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "#os.chdir(r\"XXXX\") # put path to your model here\n",
    "#model = load_model('ANN4_10000.keras')\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN4_10000_test.weights.h5') #### Filename is ANN4_(# of training files)\n",
    "model.save('ANN4_10000_test.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0d714-faa7-4314-9bbf-39b909084ff8",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347edadf-bf00-4982-98f4-f27cc8fe5bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 50000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 2 with 50000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a911eb-a8c7-4847-b36d-aa08d6fb96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 100\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 2 with 5000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b5a68-58b9-4071-9c67-e2aff76170f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX'   ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set2_50000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN4(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c8caa-0eb7-478f-ac16-42cbf33a416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 7 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT, EMAT]\n",
    "x = tfl.Dense(units=7, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-3 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 2500  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    #min_delta=0.00001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "#os.chdir(r\"C:\\Users\\bordo\\Documents\\UToledo\\Research\\ML\\a-Si-Paper\\Cody-Lorentz_Reviewer_Comments\\Models\") # put path to your model here\n",
    "#model = load_model('ANN4_50000.keras')\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN4_50000v2.weights.h5') #### Filename is ANN4_(# of training files)\n",
    "model.save('ANN4_50000v2.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3f392-bf47-45bc-bddf-0baa2444620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=5, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN4_50000v2.weights.h5') #### Filename is ANN4_(# of training files)\n",
    "model.save('ANN4_50000v2.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8c1c0-793c-4156-a57d-b1c483fc62df",
   "metadata": {},
   "source": [
    "The next model will be trained with a training set size of 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e387802-edfd-494d-a242-647d045dc1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data size = 100000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TRAINING DATA OF DATA SET 2 with 100000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool  = unison_shuffled_copies( train_files, train_data, train_Ep, train_Eg, train_Eo, train_Br, train_Amp, train_Einf, train_BulkT, train_EMA_bool, train_EMAT, train_Sub_bool )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc24e4a-6be9-4b18-b288-ede07f76961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data size = 10000\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE VALIDATION DATA OF DATA SET 2 with 10000 files in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool  = unison_shuffled_copies( val_files, val_data, val_Ep, val_Eg, val_Eo, val_Br, val_Amp, val_Einf, val_BulkT, val_EMA_bool, val_EMAT, val_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045ef7c-1465-4704-b479-e3ed1c7474a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new function:\n",
    "path = r'XXXX' ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO STORE THE MEAN AND STANDARD DEVIATIONS ####################\n",
    "filename = 'Data_Set2_100000'\n",
    "\n",
    "# Get Standardized data\n",
    "x_train, y_train, x_val, y_val = Standardize_data_ANN4(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a22c97-d1cd-435f-a59c-d412b47fe6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (697, 4)\n",
    "input_NCS = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "# Flatten the input\n",
    "x = tfl.Flatten()(input_NCS)\n",
    "\n",
    "# Hidden Layers\n",
    "x = tfl.Dense(units=2048, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=1024, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=512, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=256, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=128, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=64, activation='leaky_relu')(x)\n",
    "x = tfl.Dense(units=32, activation='leaky_relu')(x)\n",
    "\n",
    "# Output layer has 7 units. These units corrispond to [ Ep, Eg, Eo, Br, Amp, BulkT, EMAT]\n",
    "x = tfl.Dense(units=7, activation= None)(x)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=input_NCS, outputs= x)\n",
    "\n",
    "# Define the learning rate. The learning rate will exponetially decay with time as the model learns. This technique sometimes helps models converge.\n",
    "initial_learning_rate = 1e-5 # The learning rate may need to be adjusted with training size.\n",
    "decay_steps = 5000  # How many batches will be done before a decay step\n",
    "decay_rate = 0.75   # How much the learning rate decays at each step. \n",
    "\n",
    "#This scheduler exponentially decays the learning rate based on the parameters above.\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model. \n",
    "# The optimizer used is Adam. This is an extremly popular optimizer for training ANNs.\n",
    "# The loss function is Mean Squared Error. Used for regression tasks\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    loss =  tf.keras.losses.MeanSquaredError()\n",
    ")\n",
    "\n",
    "# Define early stopping. This technique calculates the loss from the validation data after every pass through the training data. \n",
    "# It will keep track of the validation loss and stop the training after the validation loss becomes stagnate. This ensures that \n",
    "# the model does not overtrain on the training data\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=10,              # Number of epochs with no improvement after which training will be stopped\n",
    "    min_delta=0.0001, \n",
    "    verbose=1,               # Verbosity mode\n",
    "    restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "\n",
    "################# Optionally, an existing model can be loaded in. Below is the code to load in a pre-trained model .#############################\n",
    "os.chdir(r\"XXXX\") # put path to your model here\n",
    "model = load_model('ANN4_100000.keras')\n",
    "\n",
    "\n",
    "# Train the model.\n",
    "history = model.fit(\n",
    "    x=x_train, # Input data\n",
    "    y=y_train, # Output data \n",
    "    batch_size=32, # Batch size (adjust as needed)\n",
    "    epochs=200, # Number of epochs (adjust as needed)\n",
    "    validation_data=(x_val, y_val), # Validation data\n",
    "    verbose=1, # Verbosity mode\n",
    "    callbacks=[early_stopping] # Ends training when performance on validation data stops improving\n",
    ")\n",
    "\n",
    "# Change the directory to location to save the model \n",
    "os.chdir(r\"XXXX\")  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO SAVE THE NEW MODEL in the XXXX space ####################\n",
    "\n",
    "##################### To save the model use the code below: ################################\n",
    "model.save_weights('ANN4_100000.weights.h5') #### Filename is ANN4_(# of training files)\n",
    "model.save('ANN4_100000.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac0bba-9b28-42ba-8e76-4a9dc3aff755",
   "metadata": {},
   "source": [
    "All of the ANNs have been trained. Their perfromance will be evaluated in the next workbook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
