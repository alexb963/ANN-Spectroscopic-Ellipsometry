{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6427c7bb-20ad-48f6-8ab5-0342fcc45c05",
   "metadata": {},
   "source": [
    "The goal of this Notebook will be evaluate the performance of each ANN1, ANN2, and each version of ANN3 and ANN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48561e-00a9-4b4b-9152-5f5621514e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we will run the functions workbook \n",
    "%run Functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2f661-7e3f-4692-af49-7f6bc31609fc",
   "metadata": {},
   "source": [
    "First, load in the experimental data, the parameters determined from the traditional least-square regression process on the completeEASE software, and the optical properties of the substrates and void."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29616cb7-399b-4a0f-938a-8f1b344affd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path where optical properties are\n",
    "os.chdir(r\"XXXX\") ########## PLEASE PUT THE DIRECTORY TO THE FOLDER LABELED \"Optical Properties\" in the XXXX space ####################\n",
    "# File name for soda lime glass substrate\n",
    "file =  \"Junda_SLG.csv\"\n",
    "# Read in SLG optical properties\n",
    "SLG = pd.read_csv(file)\n",
    "SLG.name = 'SLG'\n",
    "\n",
    "SLG['e'] = SLG['e1'].to_numpy() +  1j * SLG['e2'].to_numpy()\n",
    "\n",
    "\n",
    "# File name for void\n",
    "file =  \"Void.csv\"\n",
    "# Read in void optical properties\n",
    "Void = pd.read_csv(file)\n",
    "Void.name = \"Void\"\n",
    "\n",
    "# Define the photon energy range as variable E for future use. \n",
    "E = SLG['Energy (eV)']\n",
    "# Define the same range in wavelength for future use\n",
    "wv = SLG['Wavelength (nm)']\n",
    "\n",
    "# File name for Si wafer from CompleteEASE database \n",
    "file =  \"Si_JAW.csv\"\n",
    "# load in Si wafer\n",
    "Si_JAW = pd.read_csv(file)\n",
    "Si_JAW.name = \"Si_JAW\"\n",
    "\n",
    "# File name for native oxide from CompleteEASE database \n",
    "file =  \"NTVE_JAW.csv\"\n",
    "NTVE_JAW = pd.read_csv(file)\n",
    "NTVE_JAW.name = \"NTVE_JAW\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293263ae-1377-4b6b-8d83-35c5523d3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load in the experimental data and the associated LSR parameters for evaluation and comparisons\n",
    "\n",
    "# Location of files\n",
    "os.chdir(r\"XXXX\") ########## PLEASE PUT THE DIRECTORY TO THE FOLDER LABELED \"Traditional_LSR\" in the XXXX space ####################\n",
    "\n",
    "############ MV1519 Sample Load in ############################\n",
    "\n",
    "file = \"MV1519-Data.csv\" # File name for experimental data for sample MV1519\n",
    "MV1519 = pd.read_csv(file) # read file as pandas dataframe\n",
    "MV1519.name = 'MV1519' # name dataframe  \n",
    "MV1519 =  MV1519.drop(MV1519.columns[0], axis=1) # Reformat data by removing the first column\n",
    "\n",
    "\n",
    "SE_data = [] # list that will eventually contain the experimental data. Each entry in the list will be a pandas dataframe with SE data in  (E,N,C,S)\n",
    "\n",
    "for i in range(81): # iterate for the 81 point map.\n",
    "    \n",
    "    temp = MV1519.iloc[:, (i*3):(3 *(i+1))] # read a single set of (N,C,S)\n",
    "    temp.insert(0, 'Energy (eV)', E) # add Photon Enery E in eV\n",
    "    SE_data.append(temp) #add data to list\n",
    "    \n",
    "MV1519 = np.array(SE_data) # reformat data to an np array.\n",
    "\n",
    "\n",
    "# Load in the parameters from the LSR analysis done on CompleteEASE.\n",
    "file = \"MV1519-CL-Params.csv\" # file with parameters from LSR\n",
    "MV1519_params = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "############ MV1523 Sample Load in ############################\n",
    "\n",
    "file = \"MV1523-Data.csv\" # File name for experimental data for sample MV1523\n",
    "MV1523 = pd.read_csv(file) # read file as pandas dataframe\n",
    "MV1523.name = 'MV1523' # name file \n",
    "MV1523 =  MV1523.drop(MV1523.columns[0], axis=1) # Reformat data by removing the first column\n",
    "\n",
    "\n",
    "SE_data = [] # list that will eventually contain the experimental data. Each entry in the list will be a pandas dataframe with SE data in  (E,N,C,S)\n",
    "\n",
    "for i in range(81): # iterate for the 81 point map.\n",
    "    \n",
    "    temp = MV1523.iloc[:, (i*3):(3 *(i+1))] # read a single set of (N,C,S)\n",
    "    temp.insert(0, 'Energy (eV)', E) # add Photon Enery E in eV\n",
    "    SE_data.append(temp) #add data to list\n",
    "    \n",
    "MV1523 = np.array(SE_data) # reformat data to an np array.\n",
    "\n",
    "# Load in the parameters from the LSR analysis done on CompleteEASE.\n",
    "file = \"MV1523-CL-Params.csv\" # file with parameters from least-square analysis\n",
    "MV1523_params = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "\n",
    "############ MV1530 Sample Load in ############################\n",
    "\n",
    "file = \"MV1530-Data.csv\" # File name for experimental data for sample MV1530\n",
    "MV1530 = pd.read_csv(file) # read file as pandas dataframe\n",
    "MV1530.name = 'MV1530' # name file \n",
    "MV1530 =  MV1530.drop(MV1530.columns[0], axis=1) # Reformat data by removing the first column\n",
    "\n",
    "\n",
    "SE_data = [] # list that will eventually contain the experimental data. Each entry in the list will be a pandas dataframe with SE data in  (E,N,C,S)\n",
    "\n",
    "for i in range(81): # iterate for the 81 point map.\n",
    "    \n",
    "    temp = MV1530.iloc[:, (i*3):(3 *(i+1))] # read a single set of (N,C,S)\n",
    "    temp.insert(0, 'Energy (eV)', E)  # add Photon Enery E in eV\n",
    "    SE_data.append(temp) #add data to list\n",
    "\n",
    "    \n",
    "MV1530 = np.array(SE_data) # reformat data to an np array.\n",
    "\n",
    "\n",
    "# Now we want to load in the parameters from the least-square regression analysis done on CompleteEASE.\n",
    "file = \"MV1530-SLG_Interface-Params.csv\" # file with parameters from least-square analysis\n",
    "MV1530_params = pd.read_csv(file) # read in files and pandas dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f46fd2c-2e9b-4c46-881c-30d15c358da6",
   "metadata": {},
   "source": [
    "Now load in each ANN for evaluation. Starting with ANN1 and ANN2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56335841-c19d-4d35-98da-517886d3baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load In Model\n",
    "\n",
    "# move to location of the pre-trained models\n",
    "os.chdir(r\"XXXX\") ########## PLEASE PUT THE DIRECTORY TO THE FOLDER LABELED \"Models\" in the XXXX space ####################\n",
    "\n",
    "# Load in models that make boolean predictions on structure\n",
    "ANN1 = load_model('ANN1.keras')\n",
    "ANN2 = load_model('ANN2.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c5f5a-1677-451b-8f83-d4189f4ad300",
   "metadata": {},
   "source": [
    "ANN1 and ANN2 are trained with simulated spectroscopic ellipsometry data from Data_Set_3. The associated test set will be loaded in to evaluate the performance of ANN1 and ANN2. The test set contains simulated data generated in the same way, but this data is not used during taining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86824b4-3d07-47d3-aaa2-a3e4d7f6cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in test data from Data_Set_3.\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TEST DATA OF DATA SET 3 in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_CL\"\n",
    "test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool  = unison_shuffled_copies( test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5de7d-7145-4aaa-822d-6afc1c12f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "\n",
    "print(test_files[idx])\n",
    "print( test_Ep[idx])\n",
    "print( test_Eg[idx])\n",
    "print( test_Eo[idx])\n",
    "print( test_Br[idx])\n",
    "print( test_Amp[idx])\n",
    "print( test_Einf[idx])\n",
    "print( test_BulkT[idx])\n",
    "print(test_EMA_bool[idx])\n",
    "print(test_Sub_bool[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a215c-2ca5-4ae9-8df3-47d35a37bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of ANN1 on the test data. ANN1 is trained to predict the variable \"test_Sub_bool\".\n",
    "# This variable is binary with \"0\" Si wafer substrate and \"1\" meaning SLG substrate\n",
    "\n",
    "# Generate predictions from ANN1\n",
    "ANN1_Predictions = ANN1.predict (test_data)\n",
    "\n",
    "# Since the decision is binary, predictions < 0.5 will be rounded to 0 and predictions >= 0.5 will be rounded up to 1. \n",
    "num_correct = 0\n",
    "num_wrong = 0\n",
    "\n",
    "for i in range(len(test_data)): # iterate over training set\n",
    "\n",
    "    if ANN1_Predictions[i][0] >= 0.5 and test_Sub_bool[i] >= 0.5: # If both prediction and ground truth are 1\n",
    "        num_correct = num_correct + 1 # add one to correct count\n",
    "\n",
    "    elif ANN1_Predictions[i][0] < 0.5 and test_Sub_bool[i] < 0.5: # If both prediction and ground truth are 0\n",
    "        num_correct = num_correct + 1 # add one to correct count \n",
    "\n",
    "    else: # if prediction and ground truth do not match \n",
    "        num_wrong = num_wrong + 1 # add to wrong count\n",
    "\n",
    "print( 'Fraction Correct: ' + str( num_correct / ( num_correct + num_wrong) ) ) # print fraction of correct predictions\n",
    "print( 'Fraction Wrong: ' + str( num_wrong / ( num_correct + num_wrong) ) )  # print fraction of wrong predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3332c-9a54-4e73-86cc-3ad48d2e42a8",
   "metadata": {},
   "source": [
    "ANN1 was able to predict every substrate in the test set correctly! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe39d0-4fcb-4912-928f-d9b48f39c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of ANN2 on the test data. ANN2 is trained to predict the variable \"test_EMA_bool\".\n",
    "# This variable is binary with \"0\" meaning no surface layer was used to simulate the data and \"1\" meaning a surface layer was used to simulate the data\n",
    "\n",
    "# Generate predictions from ANN2\n",
    "ANN2_Predictions = ANN2.predict(test_data)\n",
    "\n",
    "# Since the decision is binary, predictions < 0.5 will be rounded to 0 and predictions >= 0.5 will be rounded up to 1. \n",
    "num_correct = 0\n",
    "num_wrong = 0\n",
    "num_wrong_index =[]\n",
    "false_positive_SL_index = []\n",
    "false_negative_SL_index = []\n",
    "\n",
    "for i in range(len(test_data)): # iterate over training set\n",
    "\n",
    "    if ANN2_Predictions[i][0] >= 0.5 and test_EMA_bool[i] >= 0.5: # If both prediction and ground truth are 1\n",
    "        num_correct = num_correct + 1 # add one to correct count\n",
    "\n",
    "    elif ANN2_Predictions[i][0] < 0.5 and test_EMA_bool[i] < 0.5: # If both prediction and ground truth are 0\n",
    "        num_correct = num_correct + 1 # add one to correct count \n",
    "\n",
    "    elif ANN2_Predictions[i][0] > 0.5 and test_EMA_bool[i] < 0.5: # Flase positive\n",
    "        num_wrong = num_wrong + 1 # add to wrong count\n",
    "        num_wrong_index.append(i) # stores the index of any wrong predictions\n",
    "        false_positive_SL_index.append(i)\n",
    "\n",
    "    elif ANN2_Predictions[i][0] < 0.5 and test_EMA_bool[i] > 0.5: # Flase negative\n",
    "        num_wrong = num_wrong + 1 # add to wrong count\n",
    "        num_wrong_index.append(i) # stores the index of any wrong predictions\n",
    "        false_negative_SL_index.append(i)\n",
    "\n",
    "\n",
    "print( 'Fraction Correct: ' + str( num_correct / ( num_correct + num_wrong) ) )\n",
    "print( 'Fraction Wrong: ' + str( num_wrong / ( num_correct + num_wrong) ) ) \n",
    "print( 'Number Correct: ' + str( num_correct )  )\n",
    "print( 'Number Wrong: ' + str( num_wrong )  )\n",
    "print( 'Number of False Positive: ' + str( len(false_positive_SL_index)  ))\n",
    "print( 'Number of False Negative: ' + str( len(false_negative_SL_index)  ))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d86637-a74c-44c4-bcc5-55a6967c1b05",
   "metadata": {},
   "source": [
    "The ANN2 was correct nearly 90% of the time and ANN1 was correct 100% of the time. Now these ANNs will be provided the experimental data. \n",
    "Now the trials that were predicted incorrectly will be examined.\n",
    "\n",
    "The substrate, presence of surface layer, and surface layer thickness will be looked at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ba7cb-8596-421c-8165-499cbfd352b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Si_sub = 0  # number of failed trials with Si substrate \n",
    "SL = 0 # number of failed trials with a surface layer\n",
    "SL_small = 0 # number of failed trials where the surface layer thickness is < 0.5 nm\n",
    "bulk_small = 0 # number of failed trials where bulk thickness < 75 nm \n",
    "bulk_avg = 0 # bulk AVG for failed trials\n",
    "Eo_avg = 0\n",
    "Eg_avg = 0\n",
    "Ep_avg = 0\n",
    "Br_avg = 0\n",
    "Amp_avg = 0\n",
    "\n",
    "\n",
    "for i in range(len(num_wrong_index)): # iterate for every failed trial\n",
    "\n",
    "    if test_Sub_bool[num_wrong_index[i]] < 0.5: # if substrate is Si wafer\n",
    "        Si_sub = Si_sub + 1 # add 1 to Si_sub\n",
    "\n",
    "    if test_EMA_bool[num_wrong_index[i]] > 0.5: # if there is a surface layer\n",
    "        SL = SL + 1 # add 1 to SL \n",
    "\n",
    "        if test_EMAT[num_wrong_index[i]] <= 0.5: # if surface layer thickness is less than 0.5 nm \n",
    "            SL_small = SL_small + 1 # add 1 to SL_small\n",
    "\n",
    "    if test_BulkT[num_wrong_index[i]] <= 75: \n",
    "        bulk_small = bulk_small + 1\n",
    "    \n",
    "    bulk_avg = bulk_avg + test_BulkT[num_wrong_index[i]]\n",
    "    Eo_avg = Eo_avg + test_Eo[num_wrong_index[i]]\n",
    "    Eg_avg = Eg_avg + test_Eg[num_wrong_index[i]]\n",
    "    Ep_avg = Ep_avg + test_Ep[num_wrong_index[i]]\n",
    "    Br_avg = Br_avg + test_Br[num_wrong_index[i]]\n",
    "    Amp_avg = Amp_avg + test_Amp[num_wrong_index[i]]\n",
    "    #print(test_Amp[num_wrong_index[i]])\n",
    "\n",
    "bulk_avg = bulk_avg / len(num_wrong_index)\n",
    "Eo_avg = Eo_avg / len(num_wrong_index)\n",
    "Eg_avg = Eg_avg / len(num_wrong_index)\n",
    "Ep_avg = Ep_avg / len(num_wrong_index)\n",
    "Br_avg = Br_avg / len(num_wrong_index)\n",
    "Amp_avg = Amp_avg / len(num_wrong_index)\n",
    "\n",
    "print('Number of failures with Si Substrate: ' + str( Si_sub ) )\n",
    "print('Number of failures with Surface Layer: ' + str( SL ) )\n",
    "print('Number of failures with Surface Layer less than 0.5 nm: ' + str( SL_small ) )\n",
    "print('Number of failures with Bulk less than 75 nm: ' + str( bulk_small ) )\n",
    "print('Bulk Avereage: ' + str( bulk_avg) )\n",
    "print('Eo Avereage: ' + str( Eo_avg) )\n",
    "print('Eg Avereage: ' + str( Eg_avg) )\n",
    "print('Ep Avereage: ' + str( Ep_avg) )\n",
    "print('Br Avereage: ' + str( Br_avg) )\n",
    "print('Amp Avereage: ' + str( Amp_avg) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea6b62-d822-47c1-bdd0-336bbd2d859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the trials that failed. Starting with the substrate\n",
    "\n",
    "data = test_Sub_bool[num_wrong_index] # det subset of data from failed trials \n",
    "\n",
    "# Define the range and number of bins for histogram\n",
    "min_value = 0\n",
    "max_value = 1\n",
    "num_bins = 2\n",
    "\n",
    "\n",
    "# Create bins using np.linspace to generate equally spaced bins\n",
    "bins = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "# Count the number of data points in each bin\n",
    "hist, edges = np.histogram(data, bins)\n",
    "\n",
    "# Print the counts for each bin\n",
    "for count, edge in zip(hist, edges[:-1]):\n",
    "    print(f'Bin {edge:.2f} to {edges[edges.tolist().index(edge) + 1]:.2f}: {count}')\n",
    "\n",
    "labels = ['c-Si Substrate', 'SLG Substrate']\n",
    "\n",
    "# Optional: Plot the histogram\n",
    "plt.hist(data, bins, edgecolor='black')\n",
    "plt.xticks(edges)\n",
    "plt.xticks([0.25, 0.75], labels=labels )\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea43c1b-f49a-463a-9594-9bc486ee5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the trials that failed. Starting with the substrate\n",
    "\n",
    "data = test_EMA_bool[num_wrong_index] # det subset of data from failed trials \n",
    "\n",
    "# Define the range and number of bins for histogram\n",
    "min_value = 0\n",
    "max_value = 1\n",
    "num_bins = 2\n",
    "\n",
    "\n",
    "# Create bins using np.linspace to generate equally spaced bins\n",
    "bins = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "# Count the number of data points in each bin\n",
    "hist, edges = np.histogram(data, bins)\n",
    "\n",
    "# Print the counts for each bin\n",
    "for count, edge in zip(hist, edges[:-1]):\n",
    "    print(f'Bin {edge:.2f} to {edges[edges.tolist().index(edge) + 1]:.2f}: {count}')\n",
    "\n",
    "labels = ['No Surface Layer', 'Surface Layer']\n",
    "\n",
    "# Optional: Plot the histogram\n",
    "plt.hist(data, bins, edgecolor='black')\n",
    "plt.xticks([0.25, 0.75], labels=labels )\n",
    "plt.tick_params(axis='both', direction='in', length=0)\n",
    "#plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5509bb7-2c67-483d-b0e2-cc0f7665ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the trials that failed. Starting with the substrate\n",
    "\n",
    "data = test_EMAT[num_wrong_index] # det subset of data from failed trials \n",
    "\n",
    "# Define the range and number of bins for histogram\n",
    "min_value = 0.1\n",
    "max_value = 2\n",
    "num_bins = 10\n",
    "\n",
    "\n",
    "# Create bins using np.linspace to generate equally spaced bins\n",
    "bins = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "# Count the number of data points in each bin\n",
    "hist, edges = np.histogram(data, bins)\n",
    "\n",
    "# Print the counts for each bin\n",
    "for count, edge in zip(hist, edges[:-1]):\n",
    "    print(f'Bin {edge:.2f} to {edges[edges.tolist().index(edge) + 1]:.2f}: {count}')\n",
    "\n",
    "\n",
    "# Optional: Plot the histogram\n",
    "plt.hist(data, bins, edgecolor='black')\n",
    "plt.xticks(edges)\n",
    "plt.tick_params(axis='both', direction='in', length=6)\n",
    "plt.xlabel('Surface Layer Thickness (nm)')\n",
    "plt.ylabel('Frequency')\n",
    "#plt.title('Histogram of Surface Layer Thickness Failures')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7480c15-0da8-4add-9bb0-9ed49a315428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the trials that failed. Starting with the substrate\n",
    "\n",
    "data = test_BulkT[false_negative_SL_index] # det subset of data from failed trials \n",
    "\n",
    "# Define the range and number of bins for histogram\n",
    "min_value = 25\n",
    "max_value = 125\n",
    "num_bins = 10\n",
    "\n",
    "\n",
    "# Create bins using np.linspace to generate equally spaced bins\n",
    "bins = np.linspace(min_value, max_value, num_bins + 1)\n",
    "\n",
    "# Count the number of data points in each bin\n",
    "hist, edges = np.histogram(data, bins)\n",
    "\n",
    "# Print the counts for each bin\n",
    "for count, edge in zip(hist, edges[:-1]):\n",
    "    print(f'Bin {edge:.2f} to {edges[edges.tolist().index(edge) + 1]:.2f}: {count}')\n",
    "\n",
    "\n",
    "# Optional: Plot the histogram\n",
    "plt.hist(data, bins, edgecolor='black')\n",
    "plt.xticks(edges)\n",
    "plt.tick_params(axis='both', direction='in', length=6)\n",
    "plt.xlabel('Bulk Thickness (nm)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64984c7-16f7-4270-ac50-b47f6c88a816",
   "metadata": {},
   "source": [
    "According to the traditional Least-square regression analysis, samples MV1519 and MV1523 are not sensitive to a surface layer in the analysis, but the MV1530 sample is sensitive to a surface layer. Also, samples MV1519 and MV1523 are fabricated on Si wafer while sample MV1530 is fabricated on SLG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563fac0-d670-4030-a740-651a4a70d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the ANN1 on the experimental data. .\n",
    "# This variable is binary with \"0\" meaning Si wafer  needed \"1\" meaning SLG for the substrate\n",
    "# MV1519 and MV1523 both have Si wafer substrate, but MV1530 has an SLG substrate\n",
    "\n",
    "# Generate predictions from ANN1 on experimental data \n",
    "ANN1_Predictions_MV1519 = ANN1.predict(MV1519)\n",
    "ANN1_Predictions_MV1523 = ANN1.predict(MV1523)\n",
    "ANN1_Predictions_MV1530 = ANN1.predict(MV1530)\n",
    "\n",
    "# score values for MV1519\n",
    "num_correct_MV1519 = 0\n",
    "num_wrong_MV1519 = 0\n",
    "\n",
    "# score values for MV1523\n",
    "num_correct_MV1523 = 0\n",
    "num_wrong_MV1523 = 0\n",
    "\n",
    "# score values for MV1530\n",
    "num_correct_MV1530 = 0\n",
    "num_wrong_MV1530 = 0\n",
    "\n",
    "for i in range(81): # iterate over each 81 point map\n",
    "\n",
    "    # Check values for MV1519 \n",
    "    if ANN1_Predictions_MV1519[i][0] < 0.5: # we know MV1519 value should always be '0' according the LSR\n",
    "        num_correct_MV1519 = num_correct_MV1519 + 1\n",
    "\n",
    "    else: num_wrong_MV1519 = num_wrong_MV1519 + 1\n",
    "\n",
    "    # Check values for MV1523\n",
    "    if ANN1_Predictions_MV1523[i][0] < 0.5: # we know MV1523 value should always be '0' according the LSR\n",
    "        num_correct_MV1523 = num_correct_MV1523 + 1\n",
    "\n",
    "    else: num_wrong_MV1523 = num_wrong_MV1523 + 1\n",
    "\n",
    "    # Check values for MV1530\n",
    "    if ANN1_Predictions_MV1530[i][0] >= 0.5: # we know MV1530 value should always be '1' according the LSR\n",
    "        num_correct_MV1530 = num_correct_MV1530 + 1\n",
    "\n",
    "    else: num_wrong_MV1530 = num_wrong_MV1530 + 1\n",
    "\n",
    "\n",
    "\n",
    "print( 'Fraction Correct MV1519: ' + str( num_correct_MV1519 / ( num_correct_MV1519 + num_wrong_MV1519 ) ) )\n",
    "print( 'Fraction Wrong MV1519: ' + str( num_wrong_MV1519  / ( num_correct_MV1519  + num_wrong_MV1519 ) ) ) \n",
    "print( 'Fraction Correct MV1523: ' + str( num_correct_MV1523 / ( num_correct_MV1523 + num_wrong_MV1523 ) ) )\n",
    "print( 'Fraction Wrong MV1523: ' + str( num_wrong_MV1523  / ( num_correct_MV1523  + num_wrong_MV1523 ) ) ) \n",
    "print( 'Fraction Correct MV1530: ' + str( num_correct_MV1530 / ( num_correct_MV1530 + num_wrong_MV1530 ) ) )\n",
    "print( 'Fraction Wrong MV1530: ' + str( num_wrong_MV1530  / ( num_correct_MV1530  + num_wrong_MV1530 ) ) ) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce2d99-8782-44da-b446-ff9c42e36633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the ANN2 on the experimental data. .\n",
    "# This variable is binary with \"0\" meaning no surface layer is needed \"1\" meaning a surface layer is needed\n",
    "# MV1519 and MV1523 both do not have surface layers in traditional analysis, but MV1530 needs a surface layer\n",
    "\n",
    "# Generate predictions from Surface_Layer_ANN on experimental data \n",
    "ANN2_Predictions_MV1519 = ANN2.predict(MV1519)\n",
    "ANN2_Predictions_MV1523 = ANN2.predict(MV1523)\n",
    "ANN2_Predictions_MV1530 = ANN2.predict(MV1530)\n",
    "\n",
    "# score values for MV1519\n",
    "num_correct_MV1519 = 0\n",
    "num_wrong_MV1519 = 0\n",
    "\n",
    "# score values for MV1523\n",
    "num_correct_MV1523 = 0\n",
    "num_wrong_MV1523 = 0\n",
    "\n",
    "# score values for MV1530\n",
    "num_correct_MV1530 = 0\n",
    "num_wrong_MV1530 = 0\n",
    "\n",
    "for i in range(81): # iterate over each 81 point map\n",
    "\n",
    "    # Check values for MV1519 \n",
    "    if ANN2_Predictions_MV1519[i][0] < 0.5: # we know MV1519 value should always be '0' according the LSR\n",
    "        num_correct_MV1519 = num_correct_MV1519 + 1\n",
    "\n",
    "    else: num_wrong_MV1519 = num_wrong_MV1519 + 1\n",
    "\n",
    "    # Check values for MV1523\n",
    "    if ANN2_Predictions_MV1523[i][0] < 0.5: # we know MV1523 value should always be '0' according the LSR\n",
    "        num_correct_MV1523 = num_correct_MV1523 + 1\n",
    "\n",
    "    else: num_wrong_MV1523 = num_wrong_MV1523 + 1\n",
    "\n",
    "    # Check values for MV1530\n",
    "    if ANN2_Predictions_MV1530[i][0] >= 0.5: # we know MV1530 value should always be '1' according the LSR\n",
    "        num_correct_MV1530 = num_correct_MV1530 + 1\n",
    "\n",
    "    else: num_wrong_MV1530 = num_wrong_MV1530 + 1\n",
    "\n",
    "\n",
    "\n",
    "print( 'Fraction Correct MV1519: ' + str( num_correct_MV1519 / ( num_correct_MV1519 + num_wrong_MV1519 ) ) )\n",
    "print( 'Fraction Wrong MV1519: ' + str( num_wrong_MV1519  / ( num_correct_MV1519  + num_wrong_MV1519 ) ) ) \n",
    "print( 'Fraction Correct MV1523: ' + str( num_correct_MV1523 / ( num_correct_MV1523 + num_wrong_MV1523 ) ) )\n",
    "print( 'Fraction Wrong MV1523: ' + str( num_wrong_MV1523  / ( num_correct_MV1523  + num_wrong_MV1523 ) ) ) \n",
    "print( 'Fraction Correct MV1530: ' + str( num_correct_MV1530 / ( num_correct_MV1530 + num_wrong_MV1530 ) ) )\n",
    "print( 'Fraction Wrong MV1530: ' + str( num_wrong_MV1530  / ( num_correct_MV1530  + num_wrong_MV1530 ) ) ) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b91742-bcf7-40f7-93b7-459d24afa3dd",
   "metadata": {},
   "source": [
    "ANN1 and ANN2 are in complete agreement with the traditional least-square regression analysis! Now evaluate the performance of the ANN3 and ANN4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da61e4-f2e6-4712-8729-1f3f71968bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mean and standard deviation for each size training set\n",
    "\n",
    "# change directory to location of files \n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU STORED THE MEAN AND STANDARD DEVIATIONS FOR EACH DATA SET ####################\n",
    "############ THE MEAN AND STANDARD DEVIATIONS SHOULD HAVE BEEN MADE IN THE \"Model_Development\" WORKBOOK ########################\n",
    "\n",
    "file = 'Data_Set1_10_Mean.csv' # mean for data set of size 10\n",
    "Data_Set1_10_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_10_std.csv'  # mean standard deviation for data set of size 10\n",
    "Data_Set1_10_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_100_Mean.csv' # mean for data set of size 100\n",
    "Data_Set1_100_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_100_std.csv'  # mean standard deviation for data set of size 100\n",
    "Data_Set1_100_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_1000_Mean.csv' # mean for data set of size 1000\n",
    "Data_Set1_1000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_1000_std.csv'  # mean standard deviation for data set of size 1000\n",
    "Data_Set1_1000_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_10000_Mean.csv' # mean for data set of size 10000\n",
    "Data_Set1_10000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_10000_std.csv'  # mean standard deviation for data set of size 10000\n",
    "Data_Set1_10000_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_50000_Mean.csv' # mean for data set of size 50000\n",
    "Data_Set1_50000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_50000_std.csv'  # mean standard deviation for data set of size 50000\n",
    "Data_Set1_50000_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_100000_Mean.csv' # mean for data set of size 50000\n",
    "Data_Set1_100000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set1_100000_std.csv'  # mean standard deviation for data set of size 50000\n",
    "Data_Set1_100000_std = pd.read_csv(file) # read in files and pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14516468-6d03-43ff-a38e-69727e8e4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all the ANNs \n",
    "\n",
    "# move to location of the pre-trained models\n",
    "os.chdir(r\"XXXX\") ########## PLEASE PUT THE DIRECTORY TO THE FOLDER LABELED \"Models\" in the XXXX space ####################\n",
    "\n",
    "# Load in models for the different versions of ANN3\n",
    "ANN3_10 = load_model('ANN3_10.keras')\n",
    "ANN3_100 = load_model('ANN3_100.keras')\n",
    "ANN3_1000 = load_model('ANN3_1000.keras')\n",
    "ANN3_10000 = load_model('ANN3_10000.keras')\n",
    "ANN3_50000 = load_model('ANN3_50000.keras')\n",
    "ANN3_100000 = load_model('ANN3_100000.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1281520-95f4-4fb8-affb-e5d0b02fd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the 10000 test data set.\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TEST DATA OF DATA SET 1 in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_CL\"\n",
    "test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool  = unison_shuffled_copies( test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool )\n",
    "\n",
    "\n",
    "# Save test parameters for later analysis\n",
    "y_test_DS1 = {\n",
    "\n",
    "    'Ep' :  test_Ep,\n",
    "    'Eg' :  test_Eg,\n",
    "    'Eo' :  test_Eo,\n",
    "    'Br' :  test_Br,\n",
    "    'Amp' : test_Amp,\n",
    "    'BulkT': test_BulkT,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4fcb89-0ab1-4e41-bf80-ac4100002c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data \n",
    "\n",
    "ANN3_10_test = ANN3_10.predict(test_data)\n",
    "ANN3_100_test = ANN3_100.predict(test_data)\n",
    "ANN3_1000_test = ANN3_1000.predict(test_data)\n",
    "ANN3_10000_test = ANN3_10000.predict(test_data)\n",
    "ANN3_50000_test = ANN3_50000.predict(test_data)\n",
    "ANN3_100000_test = ANN3_100000.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf21423-dd6e-480c-8e61-6ff7c6e9d390",
   "metadata": {},
   "source": [
    "A function is defined to unstandardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb4f01-eece-42e3-a368-4ca5269a49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to help un-standardize the data once it is generated by the ANN\n",
    "def UnStandardize_ANN_Output(Standard_Means, Standard_Std, Model_Predictions): \n",
    "    \"\"\"\n",
    "    Standard_Means: A pandas dataframe containing all the mean values\n",
    "    Standard_Std: A pandas dataframe containing all the standard deviation values\n",
    "    Model_Predictions: The output of an ANN model we want to unstandardize\n",
    "    \"\"\"\n",
    "    \n",
    "    Model_Predicted_Ep = (Model_Predictions[:,0] * Standard_Std['std_Ep'][0]) + Standard_Means['Mean_Ep'][0]\n",
    "    Model_Predicted_Eg = (Model_Predictions[:,1] * Standard_Std['std_Eg'][0]) + Standard_Means['Mean_Eg'][0]\n",
    "    Model_Predicted_Eo = (Model_Predictions[:,2] * Standard_Std['std_Eo'][0]) + Standard_Means['Mean_Eo'][0]\n",
    "    Model_Predicted_Br = (Model_Predictions[:,3] * Standard_Std['std_Br'][0]) + Standard_Means['Mean_Br'][0]\n",
    "    Model_Predicted_Amp = (Model_Predictions[:,4] * Standard_Std['std_Amp'][0]) + Standard_Means['Mean_Amp'][0]\n",
    "    Model_Predicted_BulkT = (Model_Predictions[:,5] * Standard_Std['std_BulkT'][0]) + Standard_Means['Mean_BulkT'][0]\n",
    "\n",
    "    \n",
    "    if len( Model_Predictions[0]) == 7: # If there is an EMA layer included\n",
    "        \n",
    "        Model_Predicted_EMAT = (Model_Predictions[:,6] * Standard_Std['std_EMAT'][0]) + Standard_Means['Mean_EMAT'][0]\n",
    "        \n",
    "        dict1 = {\n",
    "        'Ep' : Model_Predicted_Ep,\n",
    "        'Eg' :  Model_Predicted_Eg, \n",
    "        'Eo' :  Model_Predicted_Eo,\n",
    "        'Br' :  Model_Predicted_Br, \n",
    "        'Amp':  Model_Predicted_Amp,\n",
    "        'BulkT': Model_Predicted_BulkT,\n",
    "        'EMAT': Model_Predicted_EMAT \n",
    "        }\n",
    "    \n",
    "        return(dict1)\n",
    "\n",
    "    if len(Model_Predictions[0]) == 6: # If there is NO EMA layer included\n",
    "        \n",
    "        dict1 = {\n",
    "        'Ep' :  Model_Predicted_Ep,\n",
    "        'Eg' :  Model_Predicted_Eg, \n",
    "        'Eo' :  Model_Predicted_Eo,\n",
    "        'Br' :  Model_Predicted_Br, \n",
    "        'Amp':  Model_Predicted_Amp,\n",
    "        'BulkT': Model_Predicted_BulkT \n",
    "        }\n",
    "        return(dict1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3450fd06-9943-4bc1-af44-a11a2d0b8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstandardize the data \n",
    "\n",
    "ANN3_10_test = UnStandardize_ANN_Output( Data_Set1_10_Mean, Data_Set1_10_std, ANN3_10_test)\n",
    "ANN3_100_test = UnStandardize_ANN_Output( Data_Set1_100_Mean, Data_Set1_100_std, ANN3_100_test)\n",
    "ANN3_1000_test = UnStandardize_ANN_Output( Data_Set1_1000_Mean, Data_Set1_1000_std, ANN3_1000_test)\n",
    "ANN3_10000_test = UnStandardize_ANN_Output( Data_Set1_10000_Mean, Data_Set1_10000_std, ANN3_10000_test)\n",
    "ANN3_50000_test = UnStandardize_ANN_Output( Data_Set1_50000_Mean, Data_Set1_50000_std, ANN3_50000_test)\n",
    "ANN3_100000_test = UnStandardize_ANN_Output( Data_Set1_100000_Mean, Data_Set1_100000_std, ANN3_100000_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1afd478-5fa2-4539-9ab3-cb25930fde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate predictions on the experimental data \n",
    "\n",
    "ANN3_10_MV1519 = ANN3_10.predict(MV1519)\n",
    "ANN3_10_MV1523 = ANN3_10.predict(MV1523)\n",
    "\n",
    "ANN3_100_MV1519 = ANN3_100.predict(MV1519)\n",
    "ANN3_100_MV1523 = ANN3_100.predict(MV1523)\n",
    "\n",
    "ANN3_1000_MV1519 = ANN3_1000.predict(MV1519)\n",
    "ANN3_1000_MV1523 = ANN3_1000.predict(MV1523)\n",
    "\n",
    "ANN3_10000_MV1519 = ANN3_10000.predict(MV1519)\n",
    "ANN3_10000_MV1523 = ANN3_10000.predict(MV1523)\n",
    "\n",
    "ANN3_50000_MV1519 = ANN3_50000.predict(MV1519)\n",
    "ANN3_50000_MV1523 = ANN3_50000.predict(MV1523)\n",
    "\n",
    "ANN3_100000_MV1519 = ANN3_100000.predict(MV1519)\n",
    "ANN3_100000_MV1523 = ANN3_100000.predict(MV1523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839c0f1-cace-41b9-af9b-7711ee211291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now unstandardize the predictions\n",
    "\n",
    "ANN3_10_MV1519 = UnStandardize_ANN_Output( Data_Set1_10_Mean, Data_Set1_10_std, ANN3_10_MV1519)\n",
    "ANN3_10_MV1523 = UnStandardize_ANN_Output( Data_Set1_10_Mean, Data_Set1_10_std, ANN3_10_MV1523)\n",
    "\n",
    "ANN3_100_MV1519 = UnStandardize_ANN_Output( Data_Set1_100_Mean, Data_Set1_100_std, ANN3_100_MV1519)\n",
    "ANN3_100_MV1523 = UnStandardize_ANN_Output( Data_Set1_100_Mean, Data_Set1_100_std, ANN3_100_MV1523)\n",
    "\n",
    "ANN3_1000_MV1519 = UnStandardize_ANN_Output( Data_Set1_1000_Mean, Data_Set1_1000_std, ANN3_1000_MV1519)\n",
    "ANN3_1000_MV1523 = UnStandardize_ANN_Output( Data_Set1_1000_Mean, Data_Set1_1000_std, ANN3_1000_MV1523)\n",
    "\n",
    "ANN3_10000_MV1519 = UnStandardize_ANN_Output( Data_Set1_10000_Mean, Data_Set1_10000_std, ANN3_10000_MV1519)\n",
    "ANN3_10000_MV1523 = UnStandardize_ANN_Output( Data_Set1_10000_Mean, Data_Set1_10000_std, ANN3_10000_MV1523)\n",
    "\n",
    "ANN3_50000_MV1519 = UnStandardize_ANN_Output( Data_Set1_50000_Mean, Data_Set1_50000_std, ANN3_50000_MV1519)\n",
    "ANN3_50000_MV1523 = UnStandardize_ANN_Output( Data_Set1_50000_Mean, Data_Set1_50000_std, ANN3_50000_MV1523)\n",
    "\n",
    "ANN3_100000_MV1519 = UnStandardize_ANN_Output( Data_Set1_100000_Mean, Data_Set1_100000_std, ANN3_100000_MV1519)\n",
    "ANN3_100000_MV1523 = UnStandardize_ANN_Output( Data_Set1_100000_Mean, Data_Set1_100000_std, ANN3_100000_MV1523)\n",
    "\n",
    "\n",
    "# Now store the predictions\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY FOR WHERE YOU WOULD LIKE THE ANN PREDICTIONS TO BE STORED ####################\n",
    "\n",
    "df = pd.DataFrame(ANN3_10_MV1519)\n",
    "df.to_csv('ANN3_10_MV1519.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100_MV1519)\n",
    "df.to_csv('ANN3_100_MV1519.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_1000_MV1519)\n",
    "df.to_csv('ANN3_1000_MV1519.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_10000_MV1519)\n",
    "df.to_csv('ANN3_10000_MV1519.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_50000_MV1519)\n",
    "df.to_csv('ANN3_50000_MV1519.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100000_MV1519)\n",
    "df.to_csv('ANN3_100000_MV1519.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_10_MV1523)\n",
    "df.to_csv('ANN3_10_MV1523.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100_MV1523)\n",
    "df.to_csv('ANN3_100_MV1523.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_1000_MV1523)\n",
    "df.to_csv('ANN3_1000_MV1523.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_10000_MV1523)\n",
    "df.to_csv('ANN3_10000_MV1523.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_50000_MV1523)\n",
    "df.to_csv('ANN3_50000_MV1523.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100000_MV1523)\n",
    "df.to_csv('ANN3_100000_MV1523.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33431a89-e629-40b2-93fb-65bb07e811bc",
   "metadata": {},
   "source": [
    "Now the predictions ANN3 are ready. Ellipsometric spectra will be generated for each prediction and compared to the experimental data by calculating the unweighted error function between ANN and experiment. The traditional least-square regression (LSR) unweighted error function will also be calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b4c75-e697-4fe4-ab8c-7b28deed8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate the NCS spectra for the traditionaLSR analysis \n",
    "\n",
    "############## MV1519 LSR ###########################################\n",
    "LSR_MV1519_NCS =[]\n",
    "LSR_MV1519_CL_List =[]\n",
    "for i in range(len(MV1519_params['Amp'])):\n",
    "    \n",
    "    #first get Cody-Lorentz oscillator parameters\n",
    "    E_inf = 1 # range fixed to 1\n",
    "    Ep  = float(MV1519_params['Ep'][i])\n",
    "    Amp  = float(MV1519_params['Amp'][i])\n",
    "    Br =   float(MV1519_params['Br'][i])\n",
    "    Eo =   float(MV1519_params['Eo'][i])\n",
    "    Eg =   float(MV1519_params['Eg'][i])\n",
    "    Et = 0 #this term being 0 effectivly neglects the Urbach energy and simplifies the equations.\n",
    "    Egt = Eg + Et\n",
    "    \n",
    "    # Now we generate our Cody-Lorentz material\n",
    "    CL = Get_CL_Material(E, Ep, Eg, Eo, Br, Amp, Egt, E_inf, wv)\n",
    "    LSR_MV1519_CL_List.append(CL)\n",
    "    #Generate Thicknesses\n",
    "    Bulk_Thickness = float(MV1519_params['BulkT'][i])\n",
    "    NTVE_JAW_Thickness = 1.68\n",
    "    \n",
    "    #define Structure \n",
    "    Structure = [Void, CL, NTVE_JAW, Si_JAW]\n",
    "    #Define material thicknesses\n",
    "    Mat_Thick = np.array( [Bulk_Thickness, NTVE_JAW_Thickness] )\n",
    "    #Define angle of incidence\n",
    "    Theta_Incident = 64.93\n",
    "    df = SE_Sim(Structure, Theta_Incident,  Mat_Thick, write_data=False, NCS=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    LSR_MV1519_NCS.append(df)\n",
    "\n",
    "############## MV1523 LSR ###########################################\n",
    "\n",
    "LSR_MV1523_NCS =[]\n",
    "LSR_MV1523_CL_List =[]\n",
    "for i in range(len(MV1523_params['Amp'])):\n",
    "    \n",
    "    #first get Cody-Lorentz Oscillator parameters\n",
    "    E_inf = 1 # range fixed to 1\n",
    "    Ep  = float(MV1523_params['Ep'][i])\n",
    "    Amp  = float(MV1523_params['Amp'][i])\n",
    "    Br =   float(MV1523_params['Br'][i])\n",
    "    Eo =   float(MV1523_params['Eo'][i])\n",
    "    Eg =   float(MV1523_params['Eg'][i])\n",
    "    Et = 0 #this term being 0 effectivly neglects the Urbach energy and simplifies the equations.\n",
    "    Egt = Eg + Et\n",
    "    \n",
    "    # Now we generate our Cody-Lorentz material\n",
    "    CL = Get_CL_Material(E, Ep, Eg, Eo, Br, Amp, Egt, E_inf, wv)\n",
    "    LSR_MV1523_CL_List.append(CL)\n",
    "    #Generate Thicknesses\n",
    "    Bulk_Thickness = float(MV1523_params['BulkT'][i])\n",
    "    NTVE_JAW_Thickness = 1.65\n",
    "    \n",
    "    #define Structure \n",
    "    Structure = [Void, CL, NTVE_JAW, Si_JAW]\n",
    "    #Define material thicknesses\n",
    "    Mat_Thick = np.array( [Bulk_Thickness, NTVE_JAW_Thickness] )\n",
    "    #Define angle of incidence\n",
    "    Theta_Incident = 64.93\n",
    "    df = SE_Sim(Structure, Theta_Incident,  Mat_Thick, write_data=False, NCS=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    LSR_MV1523_NCS.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86141947-b9f8-4e73-8e0d-23d40f9e6ad9",
   "metadata": {},
   "source": [
    "Define a function that can calculate the ellipsometric spectra for a given set of ANN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1980e13-9eba-48d6-8dfe-2920201bb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCS_Genorator_ANN3(ANN_List, NTVE_JAW_Thickness, E, wv): \n",
    "\n",
    "    \"\"\"\n",
    "    The goal of this function is to generate ellipsometric spectra for each ANN output in the ANN_List. \n",
    "    ANN_List should be a list of dictionaries. Each dictionary contains a set of predictions for a given ANN model\n",
    "    \"\"\"\n",
    "    NCS_List = [] #stores the ellipsometric specta\n",
    "    CL_List = []\n",
    "    for j in range(len(ANN_List)): # iterate over the entire list of ANN output\n",
    "        print(j) # just to make sure function is working\n",
    "        modX_NCS_List = [] # list containing values for a specific model\n",
    "        modX_CL_List = [] # list containing values for a specific model\n",
    "            \n",
    "        for i in range(len(ANN_List[j]['Eg'])): #iterate over each prediction for a given ANN\n",
    "    \n",
    "            #first get Cody-Loretnz Oscillator parameters\n",
    "            E_inf = 1 # range fixed to 1\n",
    "            Ep  = ANN_List[j]['Ep'][i]\n",
    "            Amp  = ANN_List[j]['Amp'][i]\n",
    "            Br =   ANN_List[j]['Br'][i]\n",
    "            Eo =   ANN_List[j]['Eo'][i]\n",
    "            Eg =   ANN_List[j]['Eg'][i]\n",
    "            Et = 0 #this term being 0 effectivly neglects the Urbach energy and simplifies the equations.\n",
    "            Egt = Eg + Et\n",
    "        \n",
    "            # Now we generate our Cody-Lorentz material\n",
    "            CL = Get_CL_Material(E, Ep, Eg, Eo, Br, Amp, Egt, E_inf, wv)\n",
    "            modX_CL_List.append(CL)\n",
    "            #Now we define our structure\n",
    "\n",
    "            Bulk_Thickness = ANN_List[j]['BulkT'][i]\n",
    "            #EMA_Thickness =  mod5_SE_predicted_EMAT[i][0]\n",
    "              \n",
    "        \n",
    "            #define Structure \n",
    "            Structure = [Void,  CL, NTVE_JAW, Si_JAW]\n",
    "            Mat_Thick =  np.array( [ Bulk_Thickness, NTVE_JAW_Thickness ] )\n",
    "            Theta_Incident = 64.93\n",
    "            df = SE_Sim(Structure, Theta_Incident,  Mat_Thick, write_data=False, NCS=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            modX_NCS_List.append(df) # store prediction in the list for that model\n",
    "            \n",
    "        \n",
    "        NCS_List.append(modX_NCS_List) # store that entire model's predictions as 1 term in this list\n",
    "        CL_List.append(modX_CL_List)\n",
    "    return(NCS_List, CL_List)\n",
    "        \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22069c26-9616-40a8-9d11-2388acd53076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put outputs in lists \n",
    "MV1519_list = [ANN3_10_MV1519, ANN3_100_MV1519, ANN3_1000_MV1519, ANN3_10000_MV1519, ANN3_50000_MV1519, ANN3_100000_MV1519]\n",
    "MV1523_list = [ANN3_10_MV1523, ANN3_100_MV1523, ANN3_1000_MV1523, ANN3_10000_MV1523, ANN3_50000_MV1523, ANN3_100000_MV1523]\n",
    "\n",
    "# Calculate ellipsometric spectra\n",
    "ANN3_MV1519_NCS, ANN3_MV1519_CL = NCS_Genorator_ANN3(MV1519_list, 1.68, E, wv) # native oxide thickness treated as fixed 1.68nm\n",
    "ANN3_MV1523_NCS, ANN3_MV1523_CL = NCS_Genorator_ANN3(MV1523_list, 1.65, E, wv) # native oxide thickness treated as fixed 1.65nm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95abe58-3532-46fd-84b1-25b8f0b80ebd",
   "metadata": {},
   "source": [
    "Now we can compare these spectra to the experimental data and the least-square regression. The unweighted error function will be generate for each point to help make comparisons. The average error will be desplayed. A function that makes these comparisons is defined to compare the data from ANN3. This function will also export figures containing this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0100b4c-3773-4951-b10c-daf960a8cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define the unweighted error function\n",
    "\n",
    "def SE_Sigma(array, df, m):\n",
    "    \"\"\"\n",
    "    Calculate the unweighted error function between the measured ellipsometric spectra and the simulated spectra\n",
    "    \n",
    "    Parameters:\n",
    "    - array: NumPy array - Measured ellipsometric spectra\n",
    "    - df: Pandas DataFrame - Simulated ellipsometric spectra\n",
    "    - m: number of fit parameters for optical and structural model\n",
    "    \n",
    "    Returns:\n",
    "    - Sigma: The unweighted error function\n",
    "    \"\"\"\n",
    "    # Ensure the array has the same shape as the DataFrame\n",
    "    if array.shape != df.shape:\n",
    "        raise ValueError(\"NumPy array and DataFrame must have the same shape\")\n",
    "    \n",
    "    # Convert the NumPy array to a DataFrame for easier handling\n",
    "    n = array.shape[0] * array.shape[1]\n",
    "    array_df = pd.DataFrame(array, columns=df.columns, index=df.index) \n",
    "    array_df.columns = ['N', 'C', 'S']\n",
    "    \n",
    "    # Calculate Sigma\n",
    "\n",
    "    #Calculate (N - N')^2  we will call this variable N_diff\n",
    "    N_diff = ( (df['N'] - array_df['N'])**2 ).sum()\n",
    "\n",
    "    #Calculate (C - C')^2  we will call this variable C_diff\n",
    "    C_diff = ( (df['C'] - array_df['C'])**2 ).sum()\n",
    "\n",
    "    #Calculate (S - S')^2  we will call this variable S_diff\n",
    "    S_diff = ( (df['S'] - array_df['S'])**2 ).sum()\n",
    "\n",
    "    #Now we calculate n\n",
    "    n = array.shape[0] # should be 697 for the intended spectral range\n",
    "\n",
    "    #Now Put everything together to get Sigma\n",
    "\n",
    "    Sigma = ( (1/(3*n - m)) * (N_diff + C_diff + S_diff ) )**(1/2)\n",
    "\n",
    "    return Sigma    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5fa65b-6036-479f-a47a-79b32d98fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCS_Comparison_ANN3(ANN_NCS_List, Experimental_NCS, Least_Square_NCS, param_num, path, generate_images=False, foldername = ''):\n",
    "\n",
    "    \"\"\"\n",
    "    Goal of function is to iterate over every ANN NCS list and compare each ANN list to the corresponsinf experimental \n",
    "    and least-square regression analysis. \n",
    "\n",
    "    If generate_images is set to True, then pictures for wach spectra will be generated.\n",
    "    \"\"\"\n",
    "\n",
    "    #iterate over the ANN_NCS_list\n",
    "\n",
    "    Sigma_mod_avg_list = [] # list to store the average value of sigma for each model\n",
    "    Sigma_mod_grand_list =[] # list to store every value for sigma for each model\n",
    "\n",
    "    Sigma_lsq_avg_list =[]\n",
    "    Sigma_lsq_grand_list = []\n",
    "\n",
    "    for k in range(len(ANN_NCS_List)): #iterate for each ANN model\n",
    "\n",
    "        #create path to store images if  generate_images=True\n",
    "        folder = str(  'ANN3_' + str(10**(k + 1)) + '_' + foldername)\n",
    "        if k == 4: \n",
    "            folder = 'ANN3_' + str(50000) +  '_' + foldername\n",
    "        if k == 5: \n",
    "            folder = 'ANN3_' + str(100000)  + '_' + foldername\n",
    "        \n",
    "        if generate_images:\n",
    "            os.makedirs(path + folder, exist_ok=True)\n",
    "            os.chdir(path + folder)\n",
    "\n",
    "        #initalize values\n",
    "        Sigma_mod_list =[]\n",
    "        Sigma_mod_avg = 0 #stores the average value of Sigma \n",
    "        \n",
    "        Sigma_LeastSquare_avg = 0 # stores the average value of Sigma for least square regression \n",
    "        Sigma_LeastSquare_list = []\n",
    "\n",
    "        for i in range(len(ANN_NCS_List[k])):# iterate for each measurement. In this case, there are 81 measurements \n",
    "\n",
    "            #Calcualte Sigma between ANN prediction N,C,S spectra and real SE data\n",
    "            #print('Exp:', Experimental_NCS[i].shape)\n",
    "            #print('ANN:', ANN_NCS_List[k][i].shape)\n",
    "            Sigma_mod = SE_Sigma(Experimental_NCS[i], ANN_NCS_List[k][i], param_num)\n",
    "            Sigma_mod_avg = Sigma_mod_avg + Sigma_mod\n",
    "            Sigma_mod_list.append(Sigma_mod)\n",
    "            \n",
    "            #plt.text(5, 1, 'ANN Model : ' + str(round(Sigma_mod, 5)) , fontsize=10, ha='center')\n",
    "            \n",
    "            \n",
    "            #Calcualte MSE between LSR N,C,S spectra and real SE data\n",
    "            #print('Exp:', Experimental_NCS[i].shape)\n",
    "            #print('LSR:', Least_Square_NCS[i].shape)\n",
    "            Sigma_LSQ = SE_Sigma(Experimental_NCS[i], Least_Square_NCS[i], param_num)\n",
    "            Sigma_LeastSquare_avg = Sigma_LeastSquare_avg + Sigma_LSQ\n",
    "            Sigma_LeastSquare_list.append(Sigma_LSQ)\n",
    "\n",
    "            def format_ticks(x, _):\n",
    "                return f'{x * 1000:.0f}'  # Multiply by scale_factor\n",
    "\n",
    "            \n",
    "            # Create a figure with 3 stacked subplots\n",
    "            fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 5)) \n",
    "\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            # Plot N for experiment, ANN, and LSR\n",
    "            axs[0].scatter(E,  Experimental_NCS[i][:, 0], facecolor='none', edgecolor = 'blue', s =10,   label='Experiment'  )\n",
    "            axs[0].plot(E,  Least_Square_NCS[i]['N'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[0].plot(E,  ANN_NCS_List[k][i]['N'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[0].set_xlim(0.7, 5.8)\n",
    "            axs[0].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "            axs[0].set_ylim(0.2, 1)\n",
    "            axs[0].set_yticks(np.arange(0.3, 0.95, 0.3))\n",
    "            \n",
    "            axs[0].set_ylabel('$N$= $Cos2$' + r'$\\Psi$ ', fontsize=10, labelpad=9)  # Title for y-axis\n",
    "            axs[0].text(4.15, 0.85, 'ANN : ' + str(\"{:.4f}\".format(Sigma_mod)), fontsize =10)\n",
    "            axs[0].text(4.15, 0.75, 'LSR  : ' + str(\"{:.4f}\".format(Sigma_LSQ)), fontsize =10 )\n",
    "            axs[0].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[0].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[0].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "\n",
    "   \n",
    "            # Plot N for experiment, ANN, and LSR\n",
    "            #axs[1].scatter(E,  Experimental_NCS[i][:, 0], facecolor='none', edgecolor = 'blue', s =10,   label='Experiment'  )\n",
    "            axs[1].plot(E,  Experimental_NCS[i][:, 0] - Least_Square_NCS[i]['N'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[1].plot(E, Experimental_NCS[i][:, 0] - ANN_NCS_List[k][i]['N'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[1].set_xlim(0.7, 5.8)\n",
    "            axs[1].set_ylim(-0.018, 0.018)\n",
    "            axs[1].set_ylabel(r\"$(N - N') \\times 10^{3}$\", fontsize=10, labelpad=5)  # Title for y-axis\n",
    "            axs[1].set_yticks(np.arange(-0.010, 0.0101, 0.010))\n",
    "            #axs[1].set_yticks(np.arange(-0.0015, 0.0151, 0.005), minor=True)\n",
    "            axs[1].set_xticks(np.arange(1, 5.5, 1))\n",
    "            axs[1].tick_params(axis='both', which='both', direction='in', length=4)  # Adjust labelsize as needed\n",
    "            axs[1].set_xticks(np.arange(1, 5.5, 1))\n",
    "            axs[1].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[1].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            #axs[1].text( 5, 0.825, 'ANN : ' + str(round(Sigma_mod, 5)))\n",
    "            #axs[1].text(5, 0.75, 'LSR  : ' + str(round(Sigma_LSQ, 5)))\n",
    "            axs[1].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "   \n",
    "            # Plot C for experiment, ANN, and LSR\n",
    "            axs[2].scatter(E,  Experimental_NCS[i][:, 1], facecolor='none', edgecolor = 'blue', s =10,  label='Experiment'  )\n",
    "            axs[2].plot(E,  Least_Square_NCS[i]['C'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[2].plot(E,  ANN_NCS_List[k][i]['C'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[2].set_xlim(0.7, 5.8)\n",
    "            axs[2].set_ylabel('$C$= $Sin2$' + r'$\\Psi$' + '$Cos$' + r'$\\Delta$', fontsize=10, labelpad=0)  # Title for y-axis\n",
    "\n",
    "            axs[2].set_ylim(-0.9, -0.3)\n",
    "            axs[2].set_yticks(np.arange(-0.8, -0.35, 0.2))\n",
    "            axs[2].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[2].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            \n",
    "            axs[2].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[2].set_xticks(np.arange(1, 5.5, 1))\n",
    "            \n",
    "            \n",
    "\n",
    "            # Plot C for experiment, ANN, and LSR\n",
    "            #axs[3].scatter(E,  Experimental_NCS[i][:, 1], facecolor='none', edgecolor = 'blue', s =10,  label='Experiment'  )\n",
    "            axs[3].plot(E,  Experimental_NCS[i][:, 1] - Least_Square_NCS[i]['C'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[3].plot(E,  Experimental_NCS[i][:, 1] - ANN_NCS_List[k][i]['C'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[3].set_xlim(0.7, 5.8)\n",
    "            axs[3].set_ylabel(r\"$(C - C') \\times 10^{3}$\", fontsize=10)  # Title for y-axis\n",
    "            axs[3].set_ylim(-0.018, 0.018)\n",
    "            axs[3].set_yticks(np.arange(-0.010, 0.0101, 0.010))\n",
    "            #axs[3].set_yticks(np.arange(-0.015, 0.0151, 0.005), minor=True)\n",
    "            axs[3].tick_params(axis='both', which='both', direction='in',  labelsize=10, length=4)  # Adjust labelsize as needed\n",
    "            axs[3].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[3].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            axs[3].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "            axs[3].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "            # Plot S for experiment, ANN, and LSR\n",
    "            axs[4].scatter(E,  Experimental_NCS[i][:, 2], facecolor='none', edgecolor = 'blue',  s =10, label='Experiment'  )\n",
    "            axs[4].plot(E,  Least_Square_NCS[i]['S'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[4].plot(E,  ANN_NCS_List[k][i]['S'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[4].set_xlim(0.7, 5.8)\n",
    "            axs[4].set_ylabel('$S$= $Sin2$' + r'$\\Psi$' + '$Sin$' + r'$\\Delta$' , fontsize=10, labelpad=7 )  # Title for y-axis\n",
    "            axs[4].set_ylim(-0.15, 1)\n",
    "            axs[4].set_yticks(np.arange(-0, 0.85, 0.4))\n",
    "            axs[4].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[4].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[4].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            axs[4].legend(loc='lower right', ncol=1, frameon=False, fontsize=10 )\n",
    "            axs[4].set_xlabel('Photon Energy (eV)', fontsize=10)  # Title for x-axis\n",
    "            axs[4].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "\n",
    "            # Plot S for experiment, ANN, and LSR\n",
    "            #axs[5].scatter(E,  Experimental_NCS[i][:, 2], facecolor='none', edgecolor = 'blue',  s =10, label='Experiment'  )\n",
    "            axs[5].plot(E,  Experimental_NCS[i][:, 2] - Least_Square_NCS[i]['S'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[5].plot(E,   Experimental_NCS[i][:, 2] -ANN_NCS_List[k][i]['S'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[5].set_xlim(0.7, 5.8)\n",
    "            axs[5].set_ylabel(r\"$(S - S') \\times 10^{3}$\", fontsize=10)  # Title for y-axis\n",
    "            axs[5].set_ylim(-0.018, 0.018)\n",
    "            axs[5].set_yticks(np.arange(-0.010, 0.0101, 0.010))\n",
    "            #axs[5].set_yticks(np.arange(-0.015, 0.0151, 0.005), minor=True)\n",
    "            axs[5].tick_params(axis='both', which='both', direction='in',  labelsize=10, length=4)  # Adjust labelsize as needed\n",
    "            axs[5].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[5].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            \n",
    "            #axs[5].legend(loc='best', ncol=1, frameon=False)\n",
    "            axs[5].set_xlabel('Photon Energy (eV)', fontsize=10)  # Title for x-axis\n",
    "            axs[5].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "            axs[5].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "           \n",
    "            plt.subplots_adjust(hspace=0, wspace=0.3)\n",
    "\n",
    "            # Set ticks to point inwards\n",
    "            for ax in axs:\n",
    "                ax.tick_params(direction='in', length=4)  # You can adjust 'length' as needed\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_linewidth(2)  # Set thickness of axis lines\n",
    "            \n",
    "\n",
    "            \n",
    "            if  generate_images: \n",
    "                plt.savefig(\"plot_\" + str(i) + \".jpg\",  dpi=300, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "           \n",
    "            # Close the plot to free up memory (optional)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #Now time to save all of the data \n",
    "        Sigma_mod_avg = Sigma_mod_avg / len(ANN_NCS_List[k])\n",
    "        Sigma_LeastSquare_avg = Sigma_LeastSquare_avg /  len(ANN_NCS_List[k])\n",
    "\n",
    "        #save average Sigma values \n",
    "        Sigma_mod_avg_list.append(Sigma_mod_avg)\n",
    "        Sigma_lsq_avg_list.append(Sigma_LeastSquare_avg)\n",
    "\n",
    "        #save each individual value\n",
    "        Sigma_mod_grand_list.append(Sigma_mod_list)\n",
    "        Sigma_lsq_grand_list.append( Sigma_LeastSquare_list)\n",
    "        \n",
    "        print('Mod ' + str(k + 1) + ' Avg Sigma: ' + str(Sigma_mod_avg))\n",
    "        print('LSQ ' + 'Avg Sigma: ' + str(Sigma_LeastSquare_avg))\n",
    "\n",
    "\n",
    "    return (Sigma_mod_grand_list, Sigma_lsq_grand_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0050af97-5bb9-4af6-a266-662848686c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generating error between ANN and experiment and LSR and experiment for MV1519 ################\n",
    "\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED####################\n",
    "\n",
    "# If you do not want to make plots, set \"generate_images=False\"\n",
    "# If you do want to make plots, set \"generate_images=True\"\n",
    "\n",
    "MV1519_Sigma, MV1519_LSR_Sigma = NCS_Comparison_ANN3(ANN3_MV1519_NCS, # simulated spectra from ANN\n",
    "                                                     MV1519[:, :, 1:] , # excperimental data\n",
    "                                                     LSR_MV1519_NCS,  # traditional LSR data\n",
    "                                                     6, # number of fit parameters\n",
    "                                                     path,  # path to save images\n",
    "                                                     generate_images=False, # makes images when true\n",
    "                                                     foldername = 'MV1519' # name for the folder created to contain images. \n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb965cca-8ac4-49dd-828f-eecb7631b3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generating error between ANN and experiment and LSR and experiment for MV1523 ################\n",
    "\n",
    "path = r'XXXX' ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED ####################\n",
    "\n",
    "# If you do not want to make plots, set \"generate_images=False\"\n",
    "# If you do want to make plots, set \"generate_images=True\"\n",
    "\n",
    "MV1523_Sigma, MV1523_LSR_Sigma = NCS_Comparison_ANN3(ANN3_MV1523_NCS, # simulated spectra from ANN\n",
    "                                                     MV1523[:, :, 1:] , # excperimental data\n",
    "                                                     LSR_MV1523_NCS,  # traditional LSR data\n",
    "                                                     6, # number of fit parameters\n",
    "                                                     path,  # path to save images\n",
    "                                                     generate_images=False, # makes images when true\n",
    "                                                     foldername = 'MV1523' # name for the folder created to contain images. \n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d9379-062f-4d5f-9b89-4242e6b8300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'XXXX')\n",
    "df = pd.DataFrame(MV1523_Sigma[-2], columns=['Sigma'])\n",
    "df.to_csv('ANN3_50000_Sigma_MV1523.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499aef9d-28e1-4bab-9253-98525b0c4b4b",
   "metadata": {},
   "source": [
    "Now it is time to look at the performance of ANN4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fbbda8-a1bb-420c-b479-a93e198bdfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in mean and standard deviation of training set for each data set\n",
    "\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU STORED THE MEAN AND STANDARD DEVIATIONS FOR EACH DATA SET ####################\n",
    "############ THE MEAN AND STANDARD DEVIATIONS SHOULD HAVE BEEN MADE IN THE \"Model_Development\" WORKBOOK ########################\n",
    "\n",
    "file = 'Data_Set2_10_Mean.csv' # mean for data set of size 10\n",
    "Data_Set2_10_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_10_std.csv'  # mean standard deviation for data set of size 10\n",
    "Data_Set2_10_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_100_Mean.csv' # mean for data set of size 100\n",
    "Data_Set2_100_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_100_std.csv'  # mean standard deviation for data set of size 100\n",
    "Data_Set2_100_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_1000_Mean.csv' # mean for data set of size 1000\n",
    "Data_Set2_1000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_1000_std.csv'  # mean standard deviation for data set of size 1000\n",
    "Data_Set2_1000_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_10000_Mean.csv' # mean for data set of size 10000\n",
    "Data_Set2_10000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_10000_std.csv'  # mean standard deviation for data set of size 10000\n",
    "Data_Set2_10000_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_50000_Mean.csv' # mean for data set of size 50000\n",
    "Data_Set2_50000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_50000_std.csv'  # mean standard deviation for data set of size 50000\n",
    "Data_Set2_50000_std = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_100000_Mean.csv' # mean for data set of size 50000\n",
    "Data_Set2_100000_Mean = pd.read_csv(file) # read in files and pandas dataframe\n",
    "\n",
    "file = 'Data_Set2_100000_std.csv'  # mean standard deviation for data set of size 50000\n",
    "Data_Set2_100000_std = pd.read_csv(file) # read in files and pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838886e-c4ec-4336-a539-8e09b5c458d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all the ANNs \n",
    "\n",
    "# move to location of the pre-trained modelsANN3\n",
    "os.chdir(r\"XXXX\") ########## PLEASE PUT THE DIRECTORY TO THE FOLDER LABELED \"Models\" in the XXXX space ####################\n",
    "\n",
    "# Load in models for the different versions of ANN4\n",
    "ANN4_10 = load_model('ANN4_10.keras')\n",
    "ANN4_100 = load_model('ANN4_100.keras')\n",
    "ANN4_1000 = load_model('ANN4_1000.keras')\n",
    "ANN4_10000 = load_model('ANN4_10000.keras')\n",
    "ANN4_50000 = load_model('ANN4_50000v2.keras')\n",
    "ANN4_100000 = load_model('ANN4_100000.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff806c-131f-49ed-82e6-5f0a39e250ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the 10000 test data set.\n",
    "\n",
    "# Identify the location of the data \n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY OF THE TEST DATA OF DATA SET 2 in the XXXX space ####################\n",
    "\n",
    "# Get the data using function \"get_data_aSi_CL_E\"\n",
    "test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool =  get_data_CL(path)\n",
    "\n",
    "# Randomly Shuffle the data using function \"unison_shuffled_copies\"\n",
    "test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool  = unison_shuffled_copies( test_files, test_data, test_Ep, test_Eg, test_Eo, test_Br, test_Amp, test_Einf, test_BulkT, test_EMA_bool, test_EMAT, test_Sub_bool )\n",
    "\n",
    "\n",
    "# Save test parameters for later analysis\n",
    "y_test_DS2 = {\n",
    "\n",
    "    'Ep' :  test_Ep,\n",
    "    'Eg' :  test_Eg,\n",
    "    'Eo' :  test_Eo,\n",
    "    'Br' :  test_Br,\n",
    "    'Amp' : test_Amp,\n",
    "    'BulkT': test_BulkT,\n",
    "    'EMAT' : test_EMAT\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfa290-26e6-42d3-8cbd-7c5461453644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data \n",
    "\n",
    "ANN4_10_test = ANN4_10.predict(test_data)\n",
    "ANN4_100_test = ANN4_100.predict(test_data)\n",
    "ANN4_1000_test = ANN4_1000.predict(test_data)\n",
    "ANN4_10000_test = ANN4_10000.predict(test_data)\n",
    "ANN4_50000_test = ANN4_50000.predict(test_data)\n",
    "ANN4_100000_test = ANN4_100000.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d3290-c6d7-46c5-a1f2-d510c06212e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstandardize the data \n",
    "\n",
    "ANN4_10_test = UnStandardize_ANN_Output( Data_Set2_10_Mean, Data_Set2_10_std, ANN4_10_test)\n",
    "ANN4_100_test = UnStandardize_ANN_Output( Data_Set2_100_Mean, Data_Set2_100_std, ANN4_100_test)\n",
    "ANN4_1000_test = UnStandardize_ANN_Output( Data_Set2_1000_Mean, Data_Set2_1000_std, ANN4_1000_test)\n",
    "ANN4_10000_test = UnStandardize_ANN_Output( Data_Set2_10000_Mean, Data_Set2_10000_std, ANN4_10000_test)\n",
    "ANN4_50000_test = UnStandardize_ANN_Output( Data_Set2_50000_Mean, Data_Set2_50000_std, ANN4_50000_test)\n",
    "ANN4_100000_test = UnStandardize_ANN_Output( Data_Set2_100000_Mean, Data_Set2_100000_std, ANN4_100000_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ded0bb-7e0c-44cd-b656-bad389b4a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate predictions on the experimental data \n",
    "\n",
    "ANN4_10_MV1530 = ANN4_10.predict(MV1530)\n",
    "ANN4_100_MV1530 = ANN4_100.predict(MV1530)\n",
    "ANN4_1000_MV1530 = ANN4_1000.predict(MV1530)\n",
    "ANN4_10000_MV1530 = ANN4_10000.predict(MV1530)\n",
    "ANN4_50000_MV1530 = ANN4_50000.predict(MV1530)\n",
    "ANN4_100000_MV1530 = ANN4_100000.predict(MV1530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c4b53-04c8-451a-8fba-f39ba3c1f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now unstandardize the predictions\n",
    "\n",
    "ANN4_10_MV1530 = UnStandardize_ANN_Output( Data_Set2_10_Mean, Data_Set2_10_std, ANN4_10_MV1530)\n",
    "ANN4_100_MV1530 = UnStandardize_ANN_Output( Data_Set2_100_Mean, Data_Set2_100_std, ANN4_100_MV1530)\n",
    "ANN4_1000_MV1530 = UnStandardize_ANN_Output( Data_Set2_1000_Mean, Data_Set2_1000_std, ANN4_1000_MV1530)\n",
    "ANN4_10000_MV1530 = UnStandardize_ANN_Output( Data_Set2_10000_Mean, Data_Set2_10000_std, ANN4_10000_MV1530)\n",
    "ANN4_50000_MV1530 = UnStandardize_ANN_Output( Data_Set2_50000_Mean, Data_Set2_50000_std, ANN4_50000_MV1530)\n",
    "ANN4_100000_MV1530 = UnStandardize_ANN_Output( Data_Set2_100000_Mean, Data_Set2_100000_std, ANN4_100000_MV1530)\n",
    "\n",
    "\n",
    "# Now store the predictions\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY FOR WHERE YOU WOULD LIKE THE ANN PREDICTIONS TO BE STORED ####################\n",
    "\n",
    "df = pd.DataFrame(ANN4_10_MV1530)\n",
    "df.to_csv('ANN4_10_MV1530.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_100_MV1530)\n",
    "df.to_csv('ANN4_100_MV1530.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_1000_MV1530)\n",
    "df.to_csv('ANN4_1000_MV1530.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_10000_MV1530)\n",
    "df.to_csv('ANN4_10000_MV1530.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_50000_MV1530)\n",
    "df.to_csv('ANN4_50000_MV1530.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_100000_MV1530)\n",
    "df.to_csv('ANN4_100000_MV1530.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d51ac-470c-47d8-9197-16ff747a7bcd",
   "metadata": {},
   "source": [
    "Now the predictions ANN4 are ready. Ellipsometric spectra will be generated for each prediction and compared to the experimental data by calculating the unweighted error function between ANN and experiment. The traditional least-square regression (LSR) unweighted error function will also be calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f3071-f401-44dd-8e95-c30886b86f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ellipsometric spectra from LSR analysis\n",
    "\n",
    "LSR_MV1530_NCS =[]\n",
    "MV1530_CL_List =[]\n",
    "for i in range(len(MV1530_params['Amp'])):\n",
    "    \n",
    "    #first get Tacu-Lorentz Oscillator parameters\n",
    "    E_inf = 1 # range fixed to 1\n",
    "    Ep  = float(MV1530_params['Ep'][i])\n",
    "    Amp  = float(MV1530_params['Amp'][i])\n",
    "    Br =   float(MV1530_params['Br'][i])\n",
    "    Eo =   float(MV1530_params['Eo'][i])\n",
    "    Eg =   float(MV1530_params['Eg'][i])\n",
    "    Et = 0 #this term being 0 effectivly neglects the Urbach energy and simplifies the equations.\n",
    "    Egt = Eg + Et\n",
    "    \n",
    "    # Now we generate our Cody-Lorentz material\n",
    "    CL = Get_CL_Material(E, Ep, Eg, Eo, Br, Amp, Egt, E_inf, wv)\n",
    "    EMA = Bruggeman_EMA( CL, Void, 0.5)\n",
    "\n",
    "    EMA_Substrate =  Bruggeman_EMA( CL, SLG , 0.5 )\n",
    "    \n",
    "    MV1530_CL_List.append(CL)\n",
    "    \n",
    "    #Generate Thicknesses\n",
    "    Bulk_Thickness = float(MV1530_params['BulkT'][i])\n",
    "    EMA_Thickness =  float(MV1530_params['EMAT'][i])\n",
    "\n",
    "    \n",
    "    #define Structure \n",
    "    Structure = [Void, EMA, CL, EMA_Substrate, SLG]\n",
    "    #Define material thicknesses\n",
    "    Mat_Thick = np.array([EMA_Thickness, Bulk_Thickness, 3 ] )\n",
    "    #Define angle of incidence\n",
    "    Theta_Incident = 64.93\n",
    "    df = SE_Sim(Structure, Theta_Incident, Mat_Thick,  write_data=False, NCS=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    LSR_MV1530_NCS.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7e939-1655-48b9-bd5d-45b41e561223",
   "metadata": {},
   "source": [
    "Now define a function to generate ellipsometric spectra for the ANN4 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f55f9-8c0b-4591-8d80-1a392196b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate ellipsometric spectra for SLG substrate structure\n",
    "\n",
    "def NCS_Genorator_ANN4(ANN_List, E, wv): \n",
    "\n",
    "    \"\"\"\n",
    "    The goal of this function is to generate ellipsometric spectra for each ANN output in the ANN_List. \n",
    "    ANN_List should be a list of dictionaries. Each dictionary contains a set of predictions for a given ANN model\n",
    "    \"\"\"\n",
    "    NCS_List = [] #stores the ellipsometric specta\n",
    "    CL_List =[]\n",
    "    for j in range(len(ANN_List)): # iterate over the entire list of ANN output\n",
    "        print(j) # check if working\n",
    "        modX_NCS_List = [] # list containing values for a specific model\n",
    "        modX_CL_List =[]   \n",
    "        for i in range(len(ANN_List[j]['Eg'])): #iterate over each prediction for a given ANN\n",
    "    \n",
    "            #first get Cody-Loretnz Oscillator parameters\n",
    "            E_inf = 1 # range fixed to 1\n",
    "            Ep  = ANN_List[j]['Ep'][i]\n",
    "            Amp  = ANN_List[j]['Amp'][i]\n",
    "            Br =   ANN_List[j]['Br'][i]\n",
    "            Eo =   ANN_List[j]['Eo'][i]\n",
    "            Eg =   ANN_List[j]['Eg'][i]\n",
    "            Et = 0 #this term being 0 effectivly neglects the Urbach energy and simplifies the equations.\n",
    "            Egt = Eg + Et\n",
    "        \n",
    "            # Now we generate our Cody-Lorentz material\n",
    "            CL = Get_CL_Material(E, Ep, Eg, Eo, Br, Amp, Egt, E_inf, wv)\n",
    "            modX_CL_List.append(CL)\n",
    "            #Now we define our structure\n",
    "                \n",
    "            #Void_Frac = 0.5\n",
    "        \n",
    "            #Generate EMA\n",
    "            EMA = Bruggeman_EMA( CL, Void, 0.5)\n",
    "            EMA_Substrate =  Bruggeman_EMA( CL, SLG , 0.5 )\n",
    "        \n",
    "            #Generate Thicknesses\n",
    "            Bulk_Thickness = ANN_List[j]['BulkT'][i]\n",
    "            EMA_Thickness =  ANN_List[j]['EMAT'][i]\n",
    "              \n",
    "        \n",
    "            #define Structure \n",
    "            Structure = [Void, EMA, CL, EMA_Substrate, SLG]\n",
    "            Mat_Thick = np.array([EMA_Thickness, Bulk_Thickness, 3 ] )\n",
    "            Theta_Incident = 64.93\n",
    "            df = SE_Sim(Structure, Theta_Incident, Mat_Thick,  write_data=False, NCS=True)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            modX_NCS_List.append(df) # store prediction in the list for that model\n",
    "        \n",
    "        NCS_List.append(modX_NCS_List) # store that entire model's predictions as 1 term in this list\n",
    "        CL_List.append(modX_CL_List)\n",
    "\n",
    "    return(NCS_List, CL_List)\n",
    "        \n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a135c-fcbf-4b51-8a59-ac176cdb1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put outputs in lists \n",
    "MV1530_list = [ANN4_10_MV1530, ANN4_100_MV1530, ANN4_1000_MV1530, ANN4_10000_MV1530, ANN4_50000_MV1530, ANN4_100000_MV1530]\n",
    "\n",
    "# Calculate ellipsometric spectra\n",
    "ANN4_MV1530_NCS, ANN4_MV1530_CL = NCS_Genorator_ANN4(MV1530_list, E, wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd1693-76e3-4f13-a808-b19d464f14b5",
   "metadata": {},
   "source": [
    "Now we can compare these spectra to the experimental data and the least-square regression. The unweighted error function will be generate for each point to help make comparisons. The average error will be desplayed. A function that makes these comparisons is defined to compare the data from ANN4. This function will also export figures containing this data.\n",
    "\n",
    "Two different functions will be defined for ANN4. Each one generating different graphs, but otherwise function the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3665b-a759-4b92-8444-8c77e4fc4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCS_Comparison_ANN4_v1(ANN_NCS_List, Experimental_NCS, Least_Square_NCS, param_num, path, generate_images=False, foldername = ''):\n",
    "\n",
    "    \"\"\"\n",
    "    Goal of function is to iterate over every ANN NCS list and compare each ANN list to the corresponsinf experimental \n",
    "    and least-square regression analysis. \n",
    "\n",
    "    If generate_images is set to True, then pictures for wach spectra will be generated.\n",
    "    \"\"\"\n",
    "\n",
    "    #iterate over the ANN_NCS_list\n",
    "\n",
    "    Sigma_mod_avg_list = [] # list to store the average value of sigma for each model\n",
    "    Sigma_mod_grand_list =[] # list to store every value for sigma for each model\n",
    "\n",
    "    Sigma_lsq_avg_list =[]\n",
    "    Sigma_lsq_grand_list = []\n",
    "\n",
    "    for k in range(len(ANN_NCS_List)): #iterate for each ANN model\n",
    "\n",
    "        #create path to store images if  generate_images=True\n",
    "        folder = str(  'ANN4_' + str(10**(k + 1)) + '_' + foldername)\n",
    "        if k == 4: \n",
    "            folder = 'ANN4_' + str(50000)  + '_' + foldername\n",
    "        if k == 5: \n",
    "            folder = 'ANN4_' + str(100000)  + '_' + foldername\n",
    "        \n",
    "        if generate_images:\n",
    "            os.makedirs(path + folder, exist_ok=True)\n",
    "            os.chdir(path + folder)\n",
    "\n",
    "        #initalize values\n",
    "        Sigma_mod_list =[]\n",
    "        Sigma_mod_avg = 0 #stores the average value of Sigma \n",
    "        \n",
    "        Sigma_LeastSquare_avg = 0 # stores the average value of Sigma for least square regression \n",
    "        Sigma_LeastSquare_list = []\n",
    "\n",
    "        for i in range(len(ANN_NCS_List[k])):# iterate for each measurement. In this case, there are 81 measurements \n",
    "\n",
    "            #Calcualte Sigma between ANN prediction N,C,S spectra and real SE data\n",
    "            Sigma_mod = SE_Sigma(Experimental_NCS[i], ANN_NCS_List[k][i], param_num)\n",
    "            Sigma_mod_avg = Sigma_mod_avg + Sigma_mod\n",
    "            Sigma_mod_list.append(Sigma_mod)\n",
    "            \n",
    "            #plt.text(5, 1, 'ANN Model : ' + str(round(Sigma_mod, 5)) , fontsize=10, ha='center')\n",
    "            \n",
    "            \n",
    "            #Calcualte MSE between LSR N,C,S spectra and real SE data\n",
    "            Sigma_LSQ = SE_Sigma(Experimental_NCS[i], Least_Square_NCS[i], param_num)\n",
    "            Sigma_LeastSquare_avg = Sigma_LeastSquare_avg + Sigma_LSQ\n",
    "            Sigma_LeastSquare_list.append(Sigma_LSQ)\n",
    "\n",
    "            def format_ticks(x, _):\n",
    "                return f'{x * 1000:.0f}'  # Multiply by scale_factor\n",
    "\n",
    "            \n",
    "            # Create a figure with 3 stacked subplots\n",
    "            plt.rcParams['axes.unicode_minus'] = True\n",
    "\n",
    "            fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 5)) \n",
    "\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            # Plot N for experiment, ANN, and LSR\n",
    "            axs[0].scatter(E,  Experimental_NCS[i][:, 0], facecolor='none', edgecolor = 'blue', s =10,   label='Experiment'  )\n",
    "            axs[0].plot(E,  Least_Square_NCS[i]['N'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[0].plot(E,  ANN_NCS_List[k][i]['N'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[0].set_xlim(0.7, 5.8)\n",
    "            axs[0].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "            axs[0].set_ylim(0.2, 1)\n",
    "            axs[0].set_yticks(np.arange(0.3, 0.95, 0.3))\n",
    "            \n",
    "            axs[0].set_ylabel('$N$= $Cos2$' + r'$\\Psi$ ', fontsize=10, labelpad=9)  # Title for y-axis\n",
    "            axs[0].text(4.15, 0.85, 'ANN : ' + str(\"{:.4f}\".format(Sigma_mod)), fontsize =10)\n",
    "            axs[0].text(4.15, 0.75, 'LSR  : ' + str(\"{:.4f}\".format(Sigma_LSQ)), fontsize =10 )\n",
    "            axs[0].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[0].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[0].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "\n",
    "   \n",
    "            # Plot N for experiment, ANN, and LSR\n",
    "            #axs[1].scatter(E,  Experimental_NCS[i][:, 0], facecolor='none', edgecolor = 'blue', s =10,   label='Experiment'  )\n",
    "            axs[1].plot(E,  Experimental_NCS[i][:, 0] - Least_Square_NCS[i]['N'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[1].plot(E, Experimental_NCS[i][:, 0] - ANN_NCS_List[k][i]['N'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[1].set_xlim(0.7, 5.8)\n",
    "            axs[1].set_ylim(-0.025, 0.025)\n",
    "            axs[1].set_ylabel(r\"$(N - N') \\times 10^{3}$\", fontsize=10, labelpad=8)  # Title for y-axis\n",
    "            axs[1].set_yticks(np.arange(-0.015, 0.0151, 0.015))\n",
    "            #axs[1].set_yticks(np.arange(-0.0015, 0.0151, 0.005), minor=True)\n",
    "            axs[1].set_xticks(np.arange(1, 5.5, 1))\n",
    "            axs[1].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[1].set_xticks(np.arange(1, 5.5, 1))\n",
    "            axs[1].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[1].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            #axs[1].text( 5, 0.825, 'ANN : ' + str(round(Sigma_mod, 5)))\n",
    "            #axs[1].text(5, 0.75, 'LSR  : ' + str(round(Sigma_LSQ, 5)))\n",
    "            axs[1].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "   \n",
    "            # Plot C for experiment, ANN, and LSR\n",
    "            axs[2].scatter(E,  Experimental_NCS[i][:, 1], facecolor='none', edgecolor = 'blue', s =10,  label='Experiment'  )\n",
    "            axs[2].plot(E,  Least_Square_NCS[i]['C'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[2].plot(E,  ANN_NCS_List[k][i]['C'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[2].set_xlim(0.7, 5.8)\n",
    "            axs[2].set_ylabel('$C$= $Sin2$' + r'$\\Psi$' + '$Cos$' + r'$\\Delta$', fontsize=10, labelpad=0)  # Title for y-axis\n",
    "\n",
    "            axs[2].set_ylim(-1, 0.7)\n",
    "            axs[2].set_yticks(np.arange(-0.8, 0.7, 0.6))\n",
    "            axs[2].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[2].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            \n",
    "            axs[2].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[2].set_xticks(np.arange(1, 5.5, 1))\n",
    "            \n",
    "            \n",
    "\n",
    "            # Plot C for experiment, ANN, and LSR\n",
    "            #axs[3].scatter(E,  Experimental_NCS[i][:, 1], facecolor='none', edgecolor = 'blue', s =10,  label='Experiment'  )\n",
    "            axs[3].plot(E,  Experimental_NCS[i][:, 1] - Least_Square_NCS[i]['C'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[3].plot(E,  Experimental_NCS[i][:, 1] - ANN_NCS_List[k][i]['C'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[3].set_xlim(0.7, 5.8)\n",
    "            axs[3].set_ylabel(r\"$(C - C') \\times 10^{3}$\", fontsize=10, labelpad=0)  # Title for y-axis\n",
    "            axs[3].set_ylim(-0.15, 0.15)\n",
    "            axs[3].set_yticks(np.arange(-0.1, 0.11, 0.1))\n",
    "            #axs[3].set_yticks(np.arange(-0.1, 0.11, 0.05), minor = True)\n",
    "            axs[3].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[3].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[3].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[3].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            axs[3].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "            axs[3].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "            # Plot S for experiment, ANN, and LSR\n",
    "            axs[4].scatter(E,  Experimental_NCS[i][:, 2], facecolor='none', edgecolor = 'blue',  s =10, label='Experiment'  )\n",
    "            axs[4].plot(E,  Least_Square_NCS[i]['S'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[4].plot(E,  ANN_NCS_List[k][i]['S'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[4].set_xlim(0.7, 5.8)\n",
    "            axs[4].set_ylabel('$S$= $Sin2$' + r'$\\Psi$' + '$Sin$' + r'$\\Delta$' , fontsize=10, labelpad=0 )  # Title for y-axis\n",
    "            axs[4].set_ylim(-1, 1)\n",
    "            axs[4].set_yticks(np.arange(-0.8, 0.85, 0.8))\n",
    "            axs[4].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[4].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[4].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            axs[4].legend(loc='lower right', ncol=1, frameon=False, fontsize=10 )\n",
    "            axs[4].set_xlabel('Photon Energy (eV)', fontsize=10)  # Title for x-axis\n",
    "            axs[4].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "\n",
    "            # Plot S for experiment, ANN, and LSR\n",
    "            #axs[5].scatter(E,  Experimental_NCS[i][:, 2], facecolor='none', edgecolor = 'blue',  s =10, label='Experiment'  )\n",
    "            axs[5].plot(E,  Experimental_NCS[i][:, 2] - Least_Square_NCS[i]['S'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[5].plot(E,   Experimental_NCS[i][:, 2] -ANN_NCS_List[k][i]['S'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[5].set_xlim(0.7, 5.8)\n",
    "            axs[5].set_ylabel(r\"$(S - S') \\times 10^{3}$\", fontsize=10, labelpad=0 )  # Title for y-axis\n",
    "            axs[5].set_ylim(-0.15, 0.15)\n",
    "            axs[5].set_yticks(np.arange(-0.1, 0.11, 0.1))\n",
    "            #axs[5].set_yticks(np.arange(-0.1, 0.11, 0.05), minor = True)\n",
    "            axs[5].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[5].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[5].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            \n",
    "            #axs[5].legend(loc='best', ncol=1, frameon=False)\n",
    "            axs[5].set_xlabel('Photon Energy (eV)', fontsize=10)  # Title for x-axis\n",
    "            axs[5].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "            axs[5].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "           \n",
    "            plt.subplots_adjust(hspace=0, wspace=0.3)\n",
    "            \n",
    "\n",
    "            # Set ticks to point inwards\n",
    "            for ax in axs:\n",
    "                ax.tick_params(direction='in', length=4)  # You can adjust 'length' as needed\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_linewidth(2)  # Set thickness of axis lines\n",
    "            \n",
    "\n",
    "            \n",
    "            if  generate_images: \n",
    "                plt.savefig(\"plot_\" + str(i) + \".jpg\",  dpi=300, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "           \n",
    "            # Close the plot to free up memory (optional)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #Now time to save all of the data \n",
    "        Sigma_mod_avg = Sigma_mod_avg / len(ANN_NCS_List[k])\n",
    "        Sigma_LeastSquare_avg = Sigma_LeastSquare_avg /  len(ANN_NCS_List[k])\n",
    "\n",
    "        #save average Sigma values \n",
    "        Sigma_mod_avg_list.append(Sigma_mod_avg)\n",
    "        Sigma_lsq_avg_list.append(Sigma_LeastSquare_avg)\n",
    "\n",
    "        #save each individual value\n",
    "        Sigma_mod_grand_list.append(Sigma_mod_list)\n",
    "        Sigma_lsq_grand_list.append( Sigma_LeastSquare_list)\n",
    "        \n",
    "        print('Mod ' + str(k + 1) + ' Avg Sigma: ' + str(Sigma_mod_avg))\n",
    "        print('LSQ ' + 'Avg Sigma: ' + str(Sigma_LeastSquare_avg))\n",
    "\n",
    "\n",
    "    return (Sigma_mod_grand_list, Sigma_lsq_grand_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a6758-cf2b-4885-95da-bfa9d61dab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NCS_Comparison_ANN4_v2(ANN_NCS_List, Experimental_NCS, Least_Square_NCS, param_num, path, generate_images=False, foldername = ''):\n",
    "\n",
    "    \"\"\"\n",
    "    Goal of function is to iterate over every ANN NCS list and compare each ANN list to the corresponsinf experimental \n",
    "    and least-square regression analysis. \n",
    "\n",
    "    If generate_images is set to True, then pictures for wach spectra will be generated.\n",
    "    \"\"\"\n",
    "\n",
    "    #iterate over the ANN_NCS_list\n",
    "\n",
    "    Sigma_mod_avg_list = [] # list to store the average value of sigma for each model\n",
    "    Sigma_mod_grand_list =[] # list to store every value for sigma for each model\n",
    "\n",
    "    Sigma_lsq_avg_list =[]\n",
    "    Sigma_lsq_grand_list = []\n",
    "\n",
    "    for k in range(len(ANN_NCS_List)): #iterate for each ANN model\n",
    "\n",
    "        #create path to store images if  generate_images=True\n",
    "        folder = str(  'ANN4_' + str(10**(k + 1)) + '_' + foldername)\n",
    "        if k == 4: \n",
    "            folder = 'ANN4_' + str(50000)  + '_' + foldername\n",
    "        if k == 5: \n",
    "            folder = 'ANN4_' + str(100000)  + '_' + foldername\n",
    "        \n",
    "        if generate_images:\n",
    "            os.makedirs(path + folder, exist_ok=True)\n",
    "            os.chdir(path + folder)\n",
    "\n",
    "        #initalize values\n",
    "        Sigma_mod_list =[]\n",
    "        Sigma_mod_avg = 0 #stores the average value of Sigma \n",
    "        \n",
    "        Sigma_LeastSquare_avg = 0 # stores the average value of Sigma for least square regression \n",
    "        Sigma_LeastSquare_list = []\n",
    "\n",
    "        for i in range(len(ANN_NCS_List[k])):# iterate for each measurement. In this case, there are 81 measurements \n",
    "\n",
    "            #Calcualte Sigma between ANN prediction N,C,S spectra and real SE data\n",
    "            Sigma_mod = SE_Sigma(Experimental_NCS[i], ANN_NCS_List[k][i], param_num)\n",
    "            Sigma_mod_avg = Sigma_mod_avg + Sigma_mod\n",
    "            Sigma_mod_list.append(Sigma_mod)\n",
    "            \n",
    "            #plt.text(5, 1, 'ANN Model : ' + str(round(Sigma_mod, 5)) , fontsize=10, ha='center')\n",
    "            \n",
    "            \n",
    "            #Calcualte MSE between LSR N,C,S spectra and real SE data\n",
    "            Sigma_LSQ = SE_Sigma(Experimental_NCS[i], Least_Square_NCS[i], param_num)\n",
    "            Sigma_LeastSquare_avg = Sigma_LeastSquare_avg + Sigma_LSQ\n",
    "            Sigma_LeastSquare_list.append(Sigma_LSQ)\n",
    "\n",
    "            def format_ticks(x, _):\n",
    "                return f'{x * 1000:.0f}'  # Multiply by scale_factor\n",
    "\n",
    "            \n",
    "            # Create a figure with 3 stacked subplots\n",
    "            fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 5)) \n",
    "\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            # Plot N for experiment, ANN, and LSR\n",
    "            axs[0].scatter(E,  Experimental_NCS[i][:, 0], facecolor='none', edgecolor = 'blue', s =10,   label='Experiment'  )\n",
    "            axs[0].plot(E,  Least_Square_NCS[i]['N'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[0].plot(E,  ANN_NCS_List[k][i]['N'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[0].set_xlim(0.7, 5.8)\n",
    "            axs[0].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "            axs[0].set_ylim(0.2, 1)\n",
    "            axs[0].set_yticks(np.arange(0.3, 0.95, 0.3))\n",
    "            \n",
    "            axs[0].set_ylabel('$N$= $Cos2$' + r'$/Psi$ ', fontsize=10, labelpad=8)  # Title for y-axis\n",
    "            axs[0].text(4.15, 0.85, 'ANN : ' + str(\"{:.4f}\".format(Sigma_mod)), fontsize =10)\n",
    "            axs[0].text(4.15, 0.75, 'LSR  : ' + str(\"{:.4f}\".format(Sigma_LSQ)), fontsize =10 )\n",
    "            axs[0].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[0].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[0].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "\n",
    "   \n",
    "            # Plot N for experiment, ANN, and LSR\n",
    "            #axs[1].scatter(E,  Experimental_NCS[i][:, 0], facecolor='none', edgecolor = 'blue', s =10,   label='Experiment'  )\n",
    "            axs[1].plot(E,  Experimental_NCS[i][:, 0] - Least_Square_NCS[i]['N'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[1].plot(E, Experimental_NCS[i][:, 0] - ANN_NCS_List[k][i]['N'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[1].set_xlim(0.7, 5.8)\n",
    "            axs[1].set_ylim(-0.018, 0.018)\n",
    "            axs[1].set_ylabel(r\"$(N - N') /times 10^{3}$\", fontsize=10, labelpad=8)  # Title for y-axis\n",
    "            axs[1].set_yticks(np.arange(-0.010, 0.0101, 0.010))\n",
    "            #axs[1].set_yticks(np.arange(-0.0015, 0.0151, 0.005), minor=True)\n",
    "            axs[1].set_xticks(np.arange(1, 5.5, 1))\n",
    "            axs[1].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[1].set_xticks(np.arange(1, 5.5, 1))\n",
    "            axs[1].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[1].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            #axs[1].text( 5, 0.825, 'ANN : ' + str(round(Sigma_mod, 5)))\n",
    "            #axs[1].text(5, 0.75, 'LSR  : ' + str(round(Sigma_LSQ, 5)))\n",
    "            axs[1].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "   \n",
    "            # Plot C for experiment, ANN, and LSR\n",
    "            axs[2].scatter(E,  Experimental_NCS[i][:, 1], facecolor='none', edgecolor = 'blue', s =10,  label='Experiment'  )\n",
    "            axs[2].plot(E,  Least_Square_NCS[i]['C'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[2].plot(E,  ANN_NCS_List[k][i]['C'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[2].set_xlim(0.7, 5.8)\n",
    "            axs[2].set_ylabel('$C$= $Sin2$' + r'$/Psi$' + '$Cos$' + r'$/Delta$', fontsize=10, labelpad=0)  # Title for y-axis\n",
    "\n",
    "            axs[2].set_ylim(-1, 0.7)\n",
    "            axs[2].set_yticks(np.arange(-0.8, 0.7, 0.6))\n",
    "            axs[2].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[2].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            \n",
    "            axs[2].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[2].set_xticks(np.arange(1, 5.5, 1))\n",
    "            \n",
    "            \n",
    "\n",
    "            # Plot C for experiment, ANN, and LSR\n",
    "            #axs[3].scatter(E,  Experimental_NCS[i][:, 1], facecolor='none', edgecolor = 'blue', s =10,  label='Experiment'  )\n",
    "            axs[3].plot(E,  Experimental_NCS[i][:, 1] - Least_Square_NCS[i]['C'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[3].plot(E,  Experimental_NCS[i][:, 1] - ANN_NCS_List[k][i]['C'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[3].set_xlim(0.7, 5.8)\n",
    "            axs[3].set_ylabel(r\"$(C - C') /times 10^{3}$\", fontsize=10)  # Title for y-axis\n",
    "            axs[3].set_ylim(-0.018, 0.018)\n",
    "            axs[3].set_yticks(np.arange(-0.010, 0.0101, 0.010))\n",
    "            #axs[3].set_yticks(np.arange(-0.1, 0.11, 0.05), minor = True)\n",
    "            axs[3].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[3].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[3].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[3].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            axs[3].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "            axs[3].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "            # Plot S for experiment, ANN, and LSR\n",
    "            axs[4].scatter(E,  Experimental_NCS[i][:, 2], facecolor='none', edgecolor = 'blue',  s =10, label='Experiment'  )\n",
    "            axs[4].plot(E,  Least_Square_NCS[i]['S'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[4].plot(E,  ANN_NCS_List[k][i]['S'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[4].set_xlim(0.7, 5.8)\n",
    "            axs[4].set_ylabel('$S$= $Sin2$' + r'$/Psi$' + '$Sin$' + r'$/Delta$' , fontsize=10, labelpad=0 )  # Title for y-axis\n",
    "            axs[4].set_ylim(-1, 1)\n",
    "            axs[4].set_yticks(np.arange(-0.8, 0.85, 0.8))\n",
    "            axs[4].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[4].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[4].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            axs[4].legend(loc='lower right', ncol=1, frameon=False, fontsize=10 )\n",
    "            axs[4].set_xlabel('Photon Energy (eV)', fontsize=10)  # Title for x-axis\n",
    "            axs[4].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "\n",
    "            # Plot S for experiment, ANN, and LSR\n",
    "            #axs[5].scatter(E,  Experimental_NCS[i][:, 2], facecolor='none', edgecolor = 'blue',  s =10, label='Experiment'  )\n",
    "            axs[5].plot(E,  Experimental_NCS[i][:, 2] - Least_Square_NCS[i]['S'], color='black', linestyle='--', label='LSR', linewidth=3 )\n",
    "            axs[5].plot(E,   Experimental_NCS[i][:, 2] -ANN_NCS_List[k][i]['S'], color='red', linestyle='--', label='ANN', linewidth=3 )\n",
    "            axs[5].set_xlim(0.7, 5.8)\n",
    "            axs[5].set_ylabel(r\"$(S - S') /times 10^{3}$\", fontsize=10)  # Title for y-axis\n",
    "            axs[5].set_ylim(-0.018, 0.018)\n",
    "            axs[5].set_yticks(np.arange(-0.010, 0.0101, 0.010))\n",
    "            #axs[5].set_yticks(np.arange(-0.1, 0.11, 0.05), minor = True)\n",
    "            axs[5].tick_params(axis='both', which='major', labelsize=10)  # Adjust labelsize as needed\n",
    "            axs[5].tick_params(axis='x', which='both', top=True, bottom=True)  # Ticks on top and bottom\n",
    "            axs[5].tick_params(axis='y', which='both', left=True, right=True)  # Ticks on left and right\n",
    "            \n",
    "            #axs[5].legend(loc='best', ncol=1, frameon=False)\n",
    "            axs[5].set_xlabel('Photon Energy (eV)', fontsize=10)  # Title for x-axis\n",
    "            axs[5].yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "            axs[5].set_xticks(np.arange(1, 5.5, 1))\n",
    "\n",
    "           \n",
    "            plt.subplots_adjust(hspace=0, wspace=0.3)\n",
    "\n",
    "            # Set ticks to point inwards\n",
    "            for ax in axs:\n",
    "                ax.tick_params(direction='in', length=4)  # You can adjust 'length' as needed\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_linewidth(2)  # Set thickness of axis lines\n",
    "            \n",
    "\n",
    "            \n",
    "            if  generate_images: \n",
    "                plt.savefig(\"plot_\" + str(i) + \".jpg\",  dpi=300, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "           \n",
    "            # Close the plot to free up memory (optional)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        #Now time to save all of the data \n",
    "        Sigma_mod_avg = Sigma_mod_avg / len(ANN_NCS_List[k])\n",
    "        Sigma_LeastSquare_avg = Sigma_LeastSquare_avg /  len(ANN_NCS_List[k])\n",
    "\n",
    "        #save average Sigma values \n",
    "        Sigma_mod_avg_list.append(Sigma_mod_avg)\n",
    "        Sigma_lsq_avg_list.append(Sigma_LeastSquare_avg)\n",
    "\n",
    "        #save each individual value\n",
    "        Sigma_mod_grand_list.append(Sigma_mod_list)\n",
    "        Sigma_lsq_grand_list.append( Sigma_LeastSquare_list)\n",
    "        \n",
    "        print('Mod ' + str(k + 1) + ' Avg Sigma: ' + str(Sigma_mod_avg))\n",
    "        print('LSQ ' + 'Avg Sigma: ' + str(Sigma_LeastSquare_avg))\n",
    "\n",
    "\n",
    "    return (Sigma_mod_grand_list, Sigma_lsq_grand_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbd3f8-49d3-46be-9715-0b0fa80d3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generating error between ANN and experiment and LSR and experiment for MV1530 ################\n",
    "\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED####################\n",
    "\n",
    "# If you do not want to make plots, set \"generate_images=False\"\n",
    "# If you do want to make plots, set \"generate_images=True\"\n",
    "\n",
    "MV1530_Sigma, MV1530_LSR_Sigma = NCS_Comparison_ANN4_v1(ANN4_MV1530_NCS, # simulated spectra from ANN\n",
    "                                                        MV1530[:, :, 1:], # excperimental data\n",
    "                                                        LSR_MV1530_NCS, # traditional LSR data\n",
    "                                                        7, # number of fit parameters\n",
    "                                                        path, # path to save images\n",
    "                                                        generate_images=False, # makes images when true\n",
    "                                                        foldername = 'MV1530V1'# name for the folder created to contain images. \n",
    "                                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4b091-be1c-4fe3-81f5-d7f260592945",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'XXXX')\n",
    "df = pd.DataFrame(MV1530_Sigma[-2], columns=['Sigma'])\n",
    "df.to_csv('ANN4_50000_Sigma_MV1530.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb484b6-1a72-412e-b3d4-8d06ee765ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generating error between ANN and experiment and LSR and experiment for MV1530 ################\n",
    "\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED####################\n",
    "\n",
    "# If you do not want to make plots, set \"generate_images=False\"\n",
    "# If you do want to make plots, set \"generate_images=True\"\n",
    "\n",
    "MV1530_Sigma, MV1530_LSR_Sigma = NCS_Comparison_ANN4_v2(ANN4_MV1530_NCS, # simulated spectra from ANN\n",
    "                                                        MV1530[:, :, 1:], # excperimental data\n",
    "                                                        LSR_MV1530_NCS, # traditional LSR data\n",
    "                                                        7, # number of fit parameters\n",
    "                                                        path, # path to save images\n",
    "                                                        generate_images=False, # makes images when true\n",
    "                                                        foldername = 'MV1530V2'# name for the folder created to contain images. \n",
    "                                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e74bd6-3500-4aaa-b256-d7757c60aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Finding extreme values for the MV1519 data when predicted by ANN3_50000 #########\n",
    "\n",
    "data = MV1519_Sigma[-2]\n",
    "median_value = statistics.median(data)\n",
    "median_index = data.index(median_value)\n",
    "print(f'MV1519 Median Sigma: {median_value}, Index Sigma: {median_index}')\n",
    "\n",
    "# Finding minimum and maximum values\n",
    "min_value = min(data)\n",
    "max_value = max(data)\n",
    "\n",
    "# Finding indices of minimum and maximum values\n",
    "min_index = data.index(min_value)\n",
    "max_index = data.index(max_value)\n",
    "\n",
    "print(f'MV1519 Min Sigma: {min_value}, Index Sigma: {min_index}')\n",
    "print(f'MV1519 Max Sigma: {max_value}, Index Sigma: {max_index}')\n",
    "\n",
    "\n",
    "#### Finding extreme values for the MV1523 data when predicted by ANN3_50000 #########\n",
    "\n",
    "data = MV1523_Sigma[-2]\n",
    "median_value = statistics.median(data)\n",
    "median_index = data.index(median_value)\n",
    "print(f'MV1523 Median Sigma: {median_value}, Index Sigma: {median_index}')\n",
    "\n",
    "# Finding minimum and maximum values\n",
    "min_value = min(data)\n",
    "max_value = max(data)\n",
    "\n",
    "# Finding indices of minimum and maximum values\n",
    "min_index = data.index(min_value)\n",
    "max_index = data.index(max_value)\n",
    "\n",
    "print(f'MV1523 Min Sigma: {min_value}, Index Sigma: {min_index}')\n",
    "print(f'MV1523 Max Sigma: {max_value}, Index Sigma: {max_index}')\n",
    "\n",
    "#### Finding extreme values for the MV1530 data when predicted by ANN3_50000 #########\n",
    "\n",
    "data = MV1530_Sigma[-2]\n",
    "median_value = statistics.median(data)\n",
    "median_index = data.index(median_value)\n",
    "print(f'MV1530 Median Sigma: {median_value}, Index Sigma: {median_index}')\n",
    "\n",
    "# Finding minimum and maximum values\n",
    "min_value = min(data)\n",
    "max_value = max(data)\n",
    "\n",
    "# Finding indices of minimum and maximum values\n",
    "min_index = data.index(min_value)\n",
    "max_index = data.index(max_value)\n",
    "\n",
    "print(f'MV1530 Min Sigma: {min_value}, Index Sigma: {min_index}')\n",
    "print(f'MV1530 Max Sigma: {max_value}, Index Sigma: {max_index}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97f079-0837-47d6-9927-abc2fb5cc4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data for next figure. \n",
    "data1 = MV1519_Sigma\n",
    "data1.append(MV1519_LSR_Sigma[0])\n",
    "data2 = MV1523_Sigma\n",
    "data2.append(MV1523_LSR_Sigma[0])\n",
    "data3 = MV1530_Sigma\n",
    "data3.append(MV1530_LSR_Sigma[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229aa58a-f968-4cf0-a449-7a4fc0503ed6",
   "metadata": {},
   "source": [
    "Now plot each sigma as a function of training set size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91d386-077f-497d-ab40-e41aaff39636",
   "metadata": {},
   "source": [
    "Now make figures comparing the ANN parameter values to the LSR parameter values and the ANN parameter values to the ground truth values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b4e6c-67e0-439f-8d03-3f8f8bae31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED####################\n",
    "\n",
    "\n",
    "font_properties = {'fontsize': 11, 'fontweight': 'normal', 'family': 'sans-serif'}\n",
    "\n",
    "# Create a figure and axes for the subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8))\n",
    "\n",
    "tick_labels = ['$10^{1}$ ', '$10^{2}$', '$10^{3}$' , '$10^{4}$', r'$5\\times10^{4}$', '$10^{5}$', 'LSR']\n",
    "\n",
    "# Create box plots for the first dataset\n",
    "boxplot1 = axs[0].boxplot(data1,       \n",
    "            vert=True, \n",
    "            patch_artist=True, \n",
    "            flierprops=dict(marker='', color='red', alpha=0.5),\n",
    "            boxprops=dict(linewidth=4),\n",
    "            whiskerprops=dict(linewidth=4), \n",
    "            medianprops=dict(color='black', linewidth=4))  # Vertical box plots\n",
    "axs[0].set_title('(a)', fontsize=18, loc='left', y=0.85,x = 0.01)\n",
    "axs[0].set_ylabel(r'$ \\times 10^{3}$', fontsize=18)\n",
    "axs[0].set_ylim(-0.02, 0.22)\n",
    "axs[0].set_yticks(np.arange(0.00, 0.201, 0.1))\n",
    "axs[0].set_xticklabels([])\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)  # Adjust labelsize as needed\n",
    "#custom_ticks = np.linspace(0.05, 0.03, 6) \n",
    "#axs[0].set_ylim(-0.005, 0.03)\n",
    "#axs[0].set_yticks(custom_ticks)\n",
    "\n",
    "\n",
    "\n",
    "# Create box plots for the second dataset\n",
    "boxplot2 = axs[1].boxplot(data2,        \n",
    "            vert=True, \n",
    "            patch_artist=True, \n",
    "            flierprops=dict(marker='', color='red', alpha=0.15),\n",
    "            boxprops=dict(linewidth=4),\n",
    "            whiskerprops=dict(linewidth=4), \n",
    "            medianprops=dict(color='black', linewidth=4))\n",
    "axs[1].set_title('(b)', fontsize=18, loc='left', y=0.85,x = 0.01)\n",
    "axs[1].set_ylim(-0.02, 0.22)\n",
    "axs[1].set_yticks(np.arange(0.00, 0.201, 0.1))\n",
    "axs[1].set_ylabel(r'$ \\times 10^{3}$', fontsize=18)\n",
    "axs[1].set_xticklabels([])\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)  # Adjust labelsize as needed\n",
    "#axs[1].set_ylim(-3, 4)\n",
    "\n",
    "# Create box plots for the third dataset\n",
    "boxplot3 = axs[2].boxplot(data3,    \n",
    "            vert=True, \n",
    "            patch_artist=True, \n",
    "            flierprops=dict(marker='', color='red', alpha=0.5),\n",
    "            boxprops=dict(linewidth=4),\n",
    "            whiskerprops=dict(linewidth=4), \n",
    "            medianprops=dict(color='black', linewidth=4))\n",
    "axs[2].set_title('(c)', fontsize=18, loc='left', y=0.85,x = 0.01)\n",
    "axs[2].set_ylabel(r'$ \\times 10^{3}$', fontsize=18)\n",
    "axs[2].set_xlabel('ANN training size', fontsize=18)\n",
    "axs[2].set_xticklabels(tick_labels, fontdict=font_properties)\n",
    "axs[2].tick_params(axis='both', which='major', labelsize=18, pad=5)  # Adjust labelsize as needed\n",
    "axs[2].set_yticks(np.arange(0.00, 0.401, 0.2))\n",
    "axs[2].set_ylim(-0.04, 0.44)\n",
    "\n",
    "# Hide the box and whisker lines\n",
    "for box in boxplot1['boxes']:\n",
    "    box.set_visible(False)  # Hide the box\n",
    "\n",
    "for whisker in boxplot1['whiskers']:\n",
    "    whisker.set_visible(False)  # Hide the whiskers\n",
    "\n",
    "# Optional: You can still display the median and fliers\n",
    "for median in boxplot1['medians']:\n",
    "    median.set_visible(False)  # Keep the median line visible\n",
    "\n",
    "for flier in boxplot1['fliers']:\n",
    "    flier.set_visible(False)  # Keep the outliers visible\n",
    "\n",
    "for cap in boxplot1['caps']:\n",
    "    cap.set_visible(False)  # Hide the caps\n",
    "\n",
    "# Hide the box and whisker lines\n",
    "for box in boxplot2['boxes']:\n",
    "    box.set_visible(False)  # Hide the box\n",
    "\n",
    "for whisker in boxplot2['whiskers']:\n",
    "    whisker.set_visible(False)  # Hide the whiskers\n",
    "\n",
    "# Optional: You can still display the median and fliers\n",
    "for median in boxplot2['medians']:\n",
    "    median.set_visible(False)  # Keep the median line visible\n",
    "\n",
    "for flier in boxplot2['fliers']:\n",
    "    flier.set_visible(False)  # Keep the outliers visible\n",
    "\n",
    "for cap in boxplot2['caps']:\n",
    "    cap.set_visible(False)  # Hide the caps\n",
    "\n",
    "\n",
    "# Hide the box and whisker lines\n",
    "for box in boxplot3['boxes']:\n",
    "    box.set_visible(False)  # Hide the box\n",
    "\n",
    "for whisker in boxplot3['whiskers']:\n",
    "    whisker.set_visible(False)  # Hide the whiskers\n",
    "\n",
    "# Optional: You can still display the median and fliers\n",
    "for median in boxplot3['medians']:\n",
    "    median.set_visible(False)  # Keep the median line visible\n",
    "\n",
    "for flier in boxplot3['fliers']:\n",
    "    flier.set_visible(False)  # Keep the outliers visible\n",
    "\n",
    "for cap in boxplot3['caps']:\n",
    "    cap.set_visible(False)  # Hide the caps\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    ax.spines['top'].set_linewidth(2)\n",
    "    ax.spines['right'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    # Make ticks point inward\n",
    "    ax.tick_params(direction='in', width=2)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    axs[0].scatter([i + 1] * len(data1[i]), data1[i], color='red', marker='o', edgecolor = 'black')\n",
    "    axs[1].scatter([i + 1] * len(data2[i]), data2[i], color='red', marker='o', edgecolor = 'black')\n",
    "    axs[2].scatter([i + 1] * len(data3[i]), data3[i], color='red', marker='o', edgecolor = 'black')\n",
    "\n",
    "\n",
    "#for i, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    #if label.get_text() == 'LSR':\n",
    "        #label.set_y(label.get_position()[1] - 0.01)  # Move Group A up\n",
    "\n",
    "\n",
    "\n",
    "def format_ticks(x, _):\n",
    "    return f'{x * 1000:.1f}'  # Multiply by scale_factor\n",
    "\n",
    "for ax in axs:\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"ANN_Sigma.jpg\",  dpi=600, bbox_inches='tight' )\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01a7db-00d8-4ff5-a5c5-342ab3051fce",
   "metadata": {},
   "source": [
    "This next section will be making scatter plot figures comparing each predicted value to the either the ground truth values in the case of simulated data or the traditional least-squuare regression data in the case of experimental data. These plots will be generated for the versions of ANN3 and ANN4 that are trained with a set size of 50000. \n",
    "\n",
    "Several different error bars will be generated calculated in slightly different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6026a-ad6f-4d3e-aa38-f82f15773ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import simps\n",
    "import matplotlib.patches as patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d6bf5-64e5-4527-b98d-5afb1c1ec63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED####################\n",
    "\n",
    "df1 = pd.DataFrame(ANN3_50000_test) # Store Si_ANN test predictions\n",
    "df2 = pd.DataFrame(ANN4_50000_test) # Store SLG_ANN test predictions\n",
    "\n",
    "df3 = pd.DataFrame(y_test_DS1) # Store ground truth for Data Set 1 Test Set\n",
    "df4 = pd.DataFrame(y_test_DS2) # Store ground truth for Data Set 2 Test Set\n",
    "\n",
    "############### Ep Plot ################################################\n",
    "key = 'Ep'\n",
    "\n",
    "y = pd.concat( [df1[str(key)], df2[str(key)] ], ignore_index=True) # concatenate model predictions \n",
    "x = pd.concat( [df3[str(key)], df4[str(key)] ], ignore_index=True) # concatenate ground truth values\n",
    "\n",
    "x_line = np.linspace(min(x),max(x), 20)\n",
    "\n",
    "\n",
    "# Calculate the residuals for slope = 1\n",
    "residuals = y - x  # Since we want lines of the form y = x + b\n",
    "\n",
    "\n",
    "data = residuals\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(data, bins=500, density=True)\n",
    "\n",
    "# Step 2: Calculate the cumulative sum (CDF)\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "cumulative_counts_normalized = cumulative_counts / cumulative_counts[-1]  # Normalize\n",
    "\n",
    "\n",
    "# Step 3: Find the x value at 97.5% CDF\n",
    "threshold1 = 0.975\n",
    "x_975 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold1)]\n",
    "\n",
    "# Step 3: Find the x value at 2.5% CDF\n",
    "threshold2 = 0.025\n",
    "x_025 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold2)]\n",
    "\n",
    "\n",
    "# Step 3: Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "# Step 4: Plot the histogram and the CDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bin_centers, cumulative_counts_normalized, marker='o')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### Generate Scatter Plot ###################\n",
    "\n",
    "\n",
    "plt.xlim (min(x_line) , max(x_line)) # set limits on x in graph\n",
    "plt.ylim(min(x_line), max(x_line) ) # set limits on Y in graph\n",
    "# Calculate the upper and lower bounds for 95% of the data\n",
    "\n",
    "lower_bound = x_025\n",
    "upper_bound =  x_975\n",
    "\n",
    "# Calculate the intercepts\n",
    "intercept_lower = -lower_bound\n",
    "intercept_upper = -upper_bound\n",
    "\n",
    "l1 = plt.scatter(y_test_DS1[key], ANN3_50000_test[key], s=25,  color='blue', label='Data Set 1 Test Set')\n",
    "l2 = plt.scatter(y_test_DS2[key], ANN4_50000_test[key], s=25, color='green', label='Data Set 2 Test Set' )\n",
    "\n",
    "plt.plot(x_line, x_line, color='Black', linewidth= 3 )\n",
    "#plt.plot(x_line, .95 * x_line, color='Black', linestyle='--', linewidth= 3)\n",
    "#plt.plot(x_line, 1.05*x_line, color='Black', linestyle='--',  linewidth= 3)\n",
    "plt.plot(x_line, x_line + intercept_lower, color='Black', linestyle='--', linewidth = 3 )\n",
    "plt.plot(x_line, x_line + intercept_upper, color='Black', linestyle='--', linewidth = 3)\n",
    "\n",
    "l3 = plt.scatter(MV1519_params[str(key)], ANN3_50000_MV1519[str(key)], s=50,  edgecolor='black', color='lime', marker='o', label='MV1519', zorder=2 )\n",
    "l4 = plt.scatter(MV1523_params[str(key)], ANN3_50000_MV1523[str(key)],  s=50, edgecolor='black', color='red', marker='^', label='MV1523', zorder=2 )\n",
    "l5 = plt.scatter(MV1530_params[str(key)], ANN4_50000_MV1530[str(key)], s=50, edgecolor='black', color='purple', marker='s', label='MV1530', zorder=2)\n",
    "\n",
    "print('##### 95% Confidence ########## ')\n",
    "print( key + ' ' +  str(round(upper_bound,5)))\n",
    "print( key + ' ' + str(round(lower_bound,5)))\n",
    "\n",
    "#plt.legend(fontsize=14, frameon=False, loc=(-0.015, 0.6) )\n",
    "#plt.title(str(key) + ': ANN_50000')\n",
    "plt.xlabel('Ground Truth $E_{p}$ (eV)', fontsize=16)\n",
    "plt.ylabel('ANN $E_{p} (eV)$', fontsize=16)\n",
    "\n",
    "first_legend = plt.legend(handles=[l1, l2], fontsize=16, frameon=False, loc=(-0.015, 0.80) )\n",
    "# Manually add the first legend so it won't be overwritten\n",
    "\n",
    "\n",
    "second_legend = plt.legend(handles=[l3, l4, l5], fontsize=16, frameon=False, loc=(0.6,0) )\n",
    "\n",
    "# Plot configurations\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(first_legend)\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', width=2)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', width=2)  # Optionally adjust minor ticks\n",
    "\n",
    "\n",
    "ax.set_xticks([1.00, 2.00, 3.00])\n",
    "ax.set_yticks([1.00, 2.00, 3.00])\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', labelsize=16)\n",
    "#ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "\n",
    "# Set y-axis and x-axis format to show 2 decimal places\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "\n",
    "plt.savefig(\"Ep.jpg\", dpi=600, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "############### Ep Plot End ################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0f693-2c4e-483d-8096-11c9f42d5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(ANN3_50000_test) # Store Si_ANN test predictions\n",
    "df2 = pd.DataFrame(ANN4_50000_test) # Store SLG_ANN test predictions\n",
    "\n",
    "df3 = pd.DataFrame(y_test_DS1) # Store ground truth for Data Set 1 Test Set\n",
    "df4 = pd.DataFrame(y_test_DS2) # Store ground truth for Data Set 2 Test Set\n",
    "\n",
    "############### Eg Plot ################################################\n",
    "key = 'Eg'\n",
    "\n",
    "y = pd.concat( [df1[str(key)], df2[str(key)] ], ignore_index=True) # concatenate model predictions \n",
    "x = pd.concat( [df3[str(key)], df4[str(key)] ], ignore_index=True) # concatenate ground truth values\n",
    "\n",
    "x_line = np.linspace(min(x),max(x), 20)\n",
    "# Calculate the residuals for slope = 1\n",
    "residuals = y - x  # Since we want lines of the form y = x + b\n",
    "\n",
    "data = residuals\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(data, bins=500, density=True)\n",
    "\n",
    "# Step 2: Calculate the cumulative sum (CDF)\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "cumulative_counts_normalized = cumulative_counts / cumulative_counts[-1]  # Normalize\n",
    "\n",
    "\n",
    "# Step 3: Find the x value at 97.5% CDF\n",
    "threshold1 = 0.975\n",
    "x_975 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold1)]\n",
    "\n",
    "# Step 3: Find the x value at 2.5% CDF\n",
    "threshold2 = 0.025\n",
    "x_025 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold2)]\n",
    "\n",
    "\n",
    "# Step 3: Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "# Step 4: Plot the histogram and the CDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bin_centers, cumulative_counts_normalized, marker='o')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "########### Generate Scatter Plot ###################\n",
    "\n",
    "\n",
    "plt.xlim (min(x_line) , max(x_line)) # set limits on x in graph\n",
    "plt.ylim(min(x_line), max(x_line) ) # set limits on Y in graph\n",
    "# Calculate the upper and lower bounds for 95% of the data\n",
    "lower_bound = x_025\n",
    "upper_bound =  x_975\n",
    "\n",
    "# Calculate the intercepts\n",
    "intercept_lower = -lower_bound\n",
    "intercept_upper = -upper_bound\n",
    "\n",
    "l1 = plt.scatter(y_test_DS1[key], ANN3_50000_test[key], s=25,  color='blue', label='Data Set 1 Test Set' )\n",
    "l2 = plt.scatter(y_test_DS2[key], ANN4_50000_test[key], s=25, color='green', label='Data Set 2 Test Set'  )\n",
    "\n",
    "plt.plot(x_line, x_line, color='Black', linewidth= 3 )\n",
    "#plt.plot(x_line, .95 * x_line, color='Black', linestyle='--', linewidth= 3)\n",
    "#plt.plot(x_line, 1.05*x_line, color='Black', linestyle='--',  linewidth= 3)\n",
    "plt.plot(x_line, x_line + intercept_lower, color='Black', linestyle='--', linewidth = 3 )\n",
    "plt.plot(x_line, x_line + intercept_upper, color='Black', linestyle='--', linewidth = 3)\n",
    "\n",
    "l3 = plt.scatter(MV1519_params[str(key)], ANN3_50000_MV1519[str(key)], s=50,  edgecolor='black', color='lime', marker='o', label='MV1519', zorder=2 )\n",
    "l4 = plt.scatter(MV1523_params[str(key)], ANN3_50000_MV1523[str(key)],  s=50, edgecolor='black', color='red', marker='^', label='MV1523', zorder=2 )\n",
    "l5 = plt.scatter(MV1530_params[str(key)], ANN4_50000_MV1530[str(key)], s=50, edgecolor='black', color='purple', marker='s', label='MV1530', zorder=2)\n",
    "\n",
    "print('##### 95% Confidence ########## ')\n",
    "print( key + ' ' +  str(round(upper_bound,5)))\n",
    "print( key + ' ' + str(round(lower_bound,5)))\n",
    "\n",
    "\n",
    "#plt.title(str(key) + ': ANN_50000')\n",
    "plt.xlabel('Ground Truth $E_{g}$ (eV)', fontsize=16)\n",
    "plt.ylabel('ANN $E_{g}$ (eV)', fontsize=16)  \n",
    "\n",
    "\n",
    "first_legend = plt.legend(handles=[l1, l2], fontsize=16, frameon=False, loc=(-0.015, 0.80) )\n",
    "# Manually add the first legend so it won't be overwritten\n",
    "\n",
    "\n",
    "second_legend = plt.legend(handles=[l3, l4, l5], fontsize=16, frameon=False, loc=(0.6,0) )\n",
    "  \n",
    "# Plot configurations\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(first_legend)\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', labelsize=16)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', width=2)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', width=1)  # Optionally adjust minor ticks\n",
    "\n",
    "# Get current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in')\n",
    "\n",
    "ax.set_xticks([1.25, 1.75, 2.25])\n",
    "ax.set_yticks([1.25, 1.75, 2.25])\n",
    "#ax.set_aspect('equal')\n",
    "\n",
    "plt.savefig(\"Eg.jpg\", dpi=600, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "plt.show()\n",
    "plt.close()\n",
    "############### Eg Plot End ################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f7db31-fac4-4d34-96f2-8700a46a7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(ANN3_50000_test) # Store Si_ANN test predictions\n",
    "df2 = pd.DataFrame(ANN4_50000_test) # Store SLG_ANN test predictions\n",
    "\n",
    "df3 = pd.DataFrame(y_test_DS1) # Store ground truth for Data Set 1 Test Set\n",
    "df4 = pd.DataFrame(y_test_DS2) # Store ground truth for Data Set 2 Test Set\n",
    "\n",
    "############### Eo Plot  ################################################\n",
    "key = 'Eo'\n",
    "\n",
    "y = pd.concat( [df1[str(key)], df2[str(key)] ], ignore_index=True) # concatenate model predictions \n",
    "x = pd.concat( [df3[str(key)], df4[str(key)] ], ignore_index=True) # concatenate ground truth values\n",
    "\n",
    "x_line = np.linspace(min(x),max(x), 20)\n",
    "\n",
    "# Calculate the residuals for slope = 1\n",
    "residuals = y - x  # Since we want lines of the form y = x + b\n",
    "\n",
    "data = residuals\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(data, bins=500, density=True)\n",
    "\n",
    "# Step 2: Calculate the cumulative sum (CDF)\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "cumulative_counts_normalized = cumulative_counts / cumulative_counts[-1]  # Normalize\n",
    "\n",
    "\n",
    "# Step 3: Find the x value at 97.5% CDF\n",
    "threshold1 = 0.975\n",
    "x_975 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold1)]\n",
    "\n",
    "# Step 3: Find the x value at 2.5% CDF\n",
    "threshold2 = 0.025\n",
    "x_025 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold2)]\n",
    "\n",
    "\n",
    "# Step 3: Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "# Step 4: Plot the histogram and the CDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bin_centers, cumulative_counts_normalized, marker='o')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "########### Generate Scatter Plot ###################\n",
    "\n",
    "\n",
    "plt.xlim (min(x_line) , max(x_line)) # set limits on x in graph\n",
    "plt.ylim(min(x_line), max(x_line) ) # set limits on Y in graph\n",
    "# Calculate the upper and lower bounds for 95% of the data\n",
    "lower_bound = x_025\n",
    "upper_bound =  x_975\n",
    "\n",
    "# Calculate the intercepts\n",
    "intercept_lower = -lower_bound\n",
    "intercept_upper = -upper_bound\n",
    "\n",
    "l1 = plt.scatter(y_test_DS1[key], ANN3_50000_test[key], s=25,  color='blue', label='Data Set 1 Test Set' )\n",
    "l2 = plt.scatter(y_test_DS2[key], ANN4_50000_test[key], s=25, color='green', label='Data Set 2 Test Set'  )\n",
    "\n",
    "plt.plot(x_line, x_line, color='Black', linewidth= 3 )\n",
    "#plt.plot(x_line, .95 * x_line, color='Black', linestyle='--', linewidth= 3)\n",
    "#plt.plot(x_line, 1.05*x_line, color='Black', linestyle='--',  linewidth= 3)\n",
    "plt.plot(x_line, x_line + intercept_lower, color='Black', linestyle='--', linewidth= 3 )\n",
    "plt.plot(x_line, x_line + intercept_upper, color='Black', linestyle='--', linewidth= 3)\n",
    "\n",
    "l3 = plt.scatter(MV1519_params[str(key)], ANN3_50000_MV1519[str(key)], s=50,  edgecolor='black', color='lime', marker='o', label='MV1519', zorder=2 )\n",
    "l4 = plt.scatter(MV1523_params[str(key)], ANN3_50000_MV1523[str(key)],  s=50, edgecolor='black', color='red', marker='^', label='MV1523', zorder=2 )\n",
    "l5 = plt.scatter(MV1530_params[str(key)], ANN4_50000_MV1530[str(key)], s=50, edgecolor='black', color='purple', marker='s', label='MV1530', zorder=2)\n",
    "\n",
    "print('##### 95% Confidence ########## ')\n",
    "print( key + ' ' +  str(round(upper_bound,5)))\n",
    "print( key + ' ' + str(round(lower_bound,5)))\n",
    "\n",
    "plt.legend(fontsize=14, frameon=False, loc=(-0.015, 0.6) )\n",
    "#plt.title(str(key) + ': ANN_50000')\n",
    "plt.xlabel('Ground Truth $E_{0}$ (eV)', fontsize=16)\n",
    "plt.ylabel('ANN $E_{0}$ (eV)', fontsize=16)\n",
    "\n",
    "# Plot configurations\n",
    "\n",
    "first_legend = plt.legend(handles=[l1, l2], fontsize=16, frameon=False, loc=(-0.015, 0.80) )\n",
    "# Manually add the first legend so it won't be overwritten\n",
    "\n",
    "\n",
    "second_legend = plt.legend(handles=[l3, l4, l5], fontsize=16, frameon=False, loc=(0.6,0) )\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(first_legend)\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', width=2, labelsize=16)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', width=1, labelsize=16)  # Optionally adjust minor ticks\n",
    "\n",
    "# Get current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', labelsize=16)\n",
    "\n",
    "ax.set_xticks([3.25, 4, 4.75])\n",
    "ax.set_yticks([3.25, 4, 4.75])\n",
    "#ax.set_aspect('equal')\n",
    "\n",
    "plt.savefig(\"Eo.jpg\", dpi=600, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "############### Eo Plot End ################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe761cc-857d-4910-83c2-798552a825be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(ANN3_50000_test) # Store Si_ANN test predictions\n",
    "df2 = pd.DataFrame(ANN4_50000_test) # Store SLG_ANN test predictions\n",
    "\n",
    "df3 = pd.DataFrame(y_test_DS1) # Store ground truth for Data Set 1 Test Set\n",
    "df4 = pd.DataFrame(y_test_DS2) # Store ground truth for Data Set 2 Test Set\n",
    "\n",
    "############### Bulk Plot ################################################\n",
    "key = 'BulkT'\n",
    "\n",
    "y = pd.concat( [df1[str(key)], df2[str(key)] ], ignore_index=True) # concatenate model predictions \n",
    "x = pd.concat( [df3[str(key)], df4[str(key)] ], ignore_index=True) # concatenate ground truth values\n",
    "\n",
    "x_line = np.linspace(min(x),max(x), 20)\n",
    "# Calculate the residuals for slope = 1\n",
    "residuals = y - x  # Since we want lines of the form y = x + b\n",
    "\n",
    "data = residuals\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(data, bins=500, density=True)\n",
    "\n",
    "# Step 2: Calculate the cumulative sum (CDF)\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "cumulative_counts_normalized = cumulative_counts / cumulative_counts[-1]  # Normalize\n",
    "\n",
    "\n",
    "# Step 3: Find the x value at 97.5% CDF\n",
    "threshold1 = 0.975\n",
    "x_975 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold1)]\n",
    "\n",
    "# Step 3: Find the x value at 2.5% CDF\n",
    "threshold2 = 0.025\n",
    "x_025 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold2)]\n",
    "\n",
    "\n",
    "# Step 3: Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "# Step 4: Plot the histogram and the CDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bin_centers, cumulative_counts_normalized, marker='o')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "########### Generate Scatter Plot ###################\n",
    "\n",
    "\n",
    "plt.xlim(min(x_line) , max(x_line)) # set limits on x in graph\n",
    "plt.ylim(min(x_line), max(x_line) ) # set limits on Y in graph\n",
    "# Calculate the upper and lower bounds for 95% of the data\n",
    "lower_bound = x_025\n",
    "upper_bound =  x_975\n",
    "\n",
    "# Calculate the intercepts\n",
    "intercept_lower = -lower_bound\n",
    "intercept_upper = -upper_bound\n",
    "\n",
    "l1 = plt.scatter(y_test_DS1[key], ANN3_50000_test[key], s=25,  color='blue', label='Data Set 1 Test Set' )\n",
    "l2 = plt.scatter(y_test_DS2[key], ANN4_50000_test[key], s=25, color='green', label='Data Set 2 Test Set'  )\n",
    "\n",
    "plt.plot(x_line, x_line, color='Black', linewidth= 3 )\n",
    "#plt.plot(x_line, .95 * x_line, color='Black', linestyle='--', linewidth= 3)\n",
    "#plt.plot(x_line, 1.05*x_line, color='Black', linestyle='--',  linewidth= 3)\n",
    "plt.plot(x_line, x_line + intercept_lower, color='Black', linestyle='--', linewidth= 3 )\n",
    "plt.plot(x_line, x_line + intercept_upper, color='Black', linestyle='--', linewidth= 3)\n",
    "\n",
    "l3 = plt.scatter(MV1519_params[str(key)], ANN3_50000_MV1519[str(key)], s=50,  edgecolor='black', color='lime', marker='o', label='MV1519', zorder=2 )\n",
    "l4 = plt.scatter(MV1523_params[str(key)], ANN3_50000_MV1523[str(key)],  s=50, edgecolor='black', color='red', marker='^', label='MV1523', zorder=2 )\n",
    "l5 = plt.scatter(MV1530_params[str(key)], ANN4_50000_MV1530[str(key)], s=50, edgecolor='black', color='purple', marker='s', label='MV1530', zorder=2)\n",
    "\n",
    "print('##### 95% Confidence ########## ')\n",
    "print( key + ' ' +  str(round(upper_bound,5)))\n",
    "print( key + ' ' + str(round(lower_bound,5)))\n",
    "\n",
    "plt.legend(fontsize=14, frameon=False, loc=(-0.015, 0.6) )\n",
    "#plt.title(str(key) + ': ANN_50000')\n",
    "plt.xlabel('Ground Truth Bulk Thickness (nm)', fontsize=16)\n",
    "plt.ylabel('ANN Bulk Thickness (nm)', fontsize=16)\n",
    "\n",
    "# Plot configurations\n",
    "\n",
    "\n",
    "first_legend = plt.legend(handles=[l1, l2], fontsize=16, frameon=False, loc=(-0.015, 0.80) )\n",
    "# Manually add the first legend so it won't be overwritten\n",
    "\n",
    "\n",
    "second_legend = plt.legend(handles=[l3, l4, l5], fontsize=16, frameon=False, loc=(0.6,0) )\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(first_legend)\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', width=2)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', width=1)  # Optionally adjust minor ticks\n",
    "\n",
    "# Get current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', labelsize=16)\n",
    "\n",
    "ax.set_xticks([50, 75, 100])\n",
    "ax.set_yticks([50, 75, 100])\n",
    "#ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "plt.savefig(\"BulkT.jpg\", dpi=600, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "plt.show()\n",
    "plt.close()\n",
    "############### BulkT Plot End ################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a804e0-de4b-41d0-ad8a-fcbf5f8874d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(ANN3_50000_test) # Store Si_ANN test predictions\n",
    "df2 = pd.DataFrame(ANN4_50000_test) # Store SLG_ANN test predictions\n",
    "\n",
    "df3 = pd.DataFrame(y_test_DS1) # Store ground truth for Data Set 1 Test Set\n",
    "df4 = pd.DataFrame(y_test_DS2) # Store ground truth for Data Set 2 Test Set\n",
    "\n",
    "############### Br Plot ################################################\n",
    "\n",
    "key = 'Br'\n",
    "\n",
    "y = pd.concat( [df1[str(key)], df2[str(key)] ], ignore_index=True) # concatenate model predictions \n",
    "x = pd.concat( [df3[str(key)], df4[str(key)] ], ignore_index=True) # concatenate ground truth values\n",
    "\n",
    "x_line = np.linspace(min(x),max(x), 20)\n",
    "\n",
    "# Calculate the residuals for slope = 1\n",
    "residuals = y - x  # Since we want lines of the form y = x + b\n",
    "\n",
    "data = residuals\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(data, bins=500, density=True)\n",
    "\n",
    "# Step 2: Calculate the cumulative sum (CDF)\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "cumulative_counts_normalized = cumulative_counts / cumulative_counts[-1]  # Normalize\n",
    "\n",
    "\n",
    "# Step 3: Find the x value at 97.5% CDF\n",
    "threshold1 = 0.975\n",
    "x_975 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold1)]\n",
    "\n",
    "# Step 3: Find the x value at 2.5% CDF\n",
    "threshold2 = 0.025\n",
    "x_025 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold2)]\n",
    "\n",
    "\n",
    "# Step 3: Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "# Step 4: Plot the histogram and the CDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bin_centers, cumulative_counts_normalized, marker='o')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "########### Generate Scatter Plot ###################\n",
    "\n",
    "\n",
    "plt.xlim(min(x_line) , max(x_line)) # set limits on x in graph\n",
    "plt.ylim(min(x_line), max(x_line) ) # set limits on Y in graph\n",
    "# Calculate the upper and lower bounds for 95% of the data\n",
    "lower_bound = x_025\n",
    "upper_bound =  x_975\n",
    "\n",
    "# Calculate the intercepts\n",
    "intercept_lower = -lower_bound\n",
    "intercept_upper = -upper_bound\n",
    "\n",
    "l1 = plt.scatter(y_test_DS1[key], ANN3_50000_test[key], s=25,  color='blue', label='Data Set 1 Test Set' )\n",
    "l2 = plt.scatter(y_test_DS2[key], ANN4_50000_test[key], s=25, color='green', label='Data Set 2 Test Set'  )\n",
    "\n",
    "plt.plot(x_line, x_line, color='Black', linewidth= 3 )\n",
    "#plt.plot(x_line, .95 * x_line, color='Black', linestyle='--', linewidth= 3)\n",
    "#plt.plot(x_line, 1.05*x_line, color='Black', linestyle='--',  linewidth= 3)\n",
    "plt.plot(x_line, x_line + intercept_lower, color='Black', linestyle='--', linewidth= 3 )\n",
    "plt.plot(x_line, x_line + intercept_upper, color='Black', linestyle='--', linewidth= 3)\n",
    "\n",
    "l3 = plt.scatter(MV1519_params[str(key)], ANN3_50000_MV1519[str(key)], s=50,  edgecolor='black', color='lime', marker='o', label='MV1519', zorder=2 )\n",
    "l4 = plt.scatter(MV1523_params[str(key)], ANN3_50000_MV1523[str(key)],  s=50, edgecolor='black', color='red', marker='^', label='MV1523', zorder=2 )\n",
    "l5 = plt.scatter(MV1530_params[str(key)], ANN4_50000_MV1530[str(key)], s=50, edgecolor='black', color='purple', marker='s', label='MV1530', zorder=2)\n",
    "\n",
    "print('##### 95% Confidence ########## ')\n",
    "print( key + ' ' +  str(round(upper_bound,5)))\n",
    "print( key + ' ' + str(round(lower_bound,5)))\n",
    "\n",
    "plt.legend(fontsize=14, frameon=False, loc=(-0.015, 0.6) )\n",
    "#plt.title(str(key) + ': ANN_50000')\n",
    "plt.xlabel('Ground Truth ' + r'$\\Gamma$ (eV)',fontsize=16)\n",
    "plt.ylabel('ANN ' + r'$\\Gamma$ (eV)',fontsize=16)\n",
    "    \n",
    "# Plot configurations\n",
    "first_legend = plt.legend(handles=[l1, l2], fontsize=16, frameon=False, loc=(-0.015, 0.80) )\n",
    "# Manually add the first legend so it won't be overwritten\n",
    "\n",
    "\n",
    "second_legend = plt.legend(handles=[l3, l4, l5], fontsize=16, frameon=False, loc=(0.6,0) )\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(first_legend)\n",
    "\n",
    "\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', width=2)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', width=1)  # Optionally adjust minor ticks\n",
    "\n",
    "# Get current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', labelsize=16)\n",
    "\n",
    "ax.set_xticks([1.75, 2.5, 3.25])\n",
    "ax.set_yticks([1.75, 2.5, 3.25])\n",
    "#ax.set_aspect('equal')\n",
    "\n",
    "plt.savefig(\"Br.jpg\", dpi=600, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "############### Br Plot End ################################################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e0c32-0d07-4f46-b603-fce0d3599329",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(ANN3_50000_test) # Store Si_ANN test predictions\n",
    "df2 = pd.DataFrame(ANN4_50000_test) # Store SLG_ANN test predictions\n",
    "\n",
    "df3 = pd.DataFrame(y_test_DS1) # Store ground truth for Data Set 1 Test Set\n",
    "df4 = pd.DataFrame(y_test_DS2) # Store ground truth for Data Set 2 Test Set\n",
    "\n",
    "############### Br Plot ################################################\n",
    "\n",
    "key = 'Amp'\n",
    "\n",
    "y = pd.concat( [df1[str(key)], df2[str(key)] ], ignore_index=True) # concatenate model predictions \n",
    "x = pd.concat( [df3[str(key)], df4[str(key)] ], ignore_index=True) # concatenate ground truth values\n",
    "\n",
    "x_line = np.linspace(min(x),max(x), 20)\n",
    "\n",
    "# Calculate the residuals for slope = 1\n",
    "residuals = y - x  # Since we want lines of the form y = x + b\n",
    "\n",
    "data = residuals\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(data, bins=500, density=True)\n",
    "\n",
    "# Step 2: Calculate the cumulative sum (CDF)\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "cumulative_counts_normalized = cumulative_counts / cumulative_counts[-1]  # Normalize\n",
    "\n",
    "\n",
    "# Step 3: Find the x value at 97.5% CDF\n",
    "threshold1 = 0.975\n",
    "x_975 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold1)]\n",
    "\n",
    "# Step 3: Find the x value at 2.5% CDF\n",
    "threshold2 = 0.025\n",
    "x_025 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold2)]\n",
    "\n",
    "\n",
    "# Step 3: Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "# Step 4: Plot the histogram and the CDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bin_centers, cumulative_counts_normalized, marker='o')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "########### Generate Scatter Plot ###################\n",
    "\n",
    "\n",
    "plt.xlim(min(x_line) , max(x_line)) # set limits on x in graph\n",
    "plt.ylim(min(x_line), max(x_line) ) # set limits on Y in graph\n",
    "# Calculate the upper and lower bounds for 95% of the data\n",
    "lower_bound = x_025\n",
    "upper_bound =  x_975\n",
    "\n",
    "# Calculate the intercepts\n",
    "intercept_lower = -lower_bound\n",
    "intercept_upper = -upper_bound\n",
    "\n",
    "l1 = plt.scatter(y_test_DS1[key], ANN3_50000_test[key], s=25,  color='blue', label='Data Set 1 Test Set' )\n",
    "l2 = plt.scatter(y_test_DS2[key], ANN4_50000_test[key], s=25, color='green', label='Data Set 2 Test Set'  )\n",
    "\n",
    "plt.plot(x_line, x_line, color='Black', linewidth= 3 )\n",
    "#plt.plot(x_line, .95 * x_line, color='Black', linestyle='--', linewidth= 3)\n",
    "#plt.plot(x_line, 1.05*x_line, color='Black', linestyle='--',  linewidth= 3)\n",
    "plt.plot(x_line, x_line + intercept_lower, color='Black', linestyle='--', linewidth= 3 )\n",
    "plt.plot(x_line, x_line + intercept_upper, color='Black', linestyle='--', linewidth= 3)\n",
    "\n",
    "l3 = plt.scatter(MV1519_params[str(key)], ANN3_50000_MV1519[str(key)], s=50,  edgecolor='black', color='lime', marker='o', label='MV1519', zorder=2 )\n",
    "l4 = plt.scatter(MV1523_params[str(key)], ANN3_50000_MV1523[str(key)],  s=50, edgecolor='black', color='red', marker='^', label='MV1523', zorder=2 )\n",
    "l5 = plt.scatter(MV1530_params[str(key)], ANN4_50000_MV1530[str(key)], s=50, edgecolor='black', color='purple', marker='s', label='MV1530', zorder=2)\n",
    "\n",
    "\n",
    "\n",
    "print('##### 95% Confidence ########## ')\n",
    "print( key + ' ' +  str(round(upper_bound,5)))\n",
    "print( key + ' ' + str(round(lower_bound,5)))\n",
    "\n",
    "plt.legend(fontsize=14, frameon=False, loc=(-0.015, 0.6) )\n",
    "#plt.title(str(key) + ': ANN_50000')\n",
    "plt.xlabel('Ground Truth $A$ (eV)', fontsize=16)\n",
    "plt.ylabel('ANN $A$ (eV)', fontsize=16)\n",
    "\n",
    "first_legend = plt.legend(handles=[l1, l2], fontsize=16, frameon=False, loc=(-0.015, 0.80) )\n",
    "# Manually add the first legend so it won't be overwritten\n",
    "\n",
    "\n",
    "second_legend = plt.legend(handles=[l3, l4, l5], fontsize=16, frameon=False, loc=(0.6,0) )\n",
    "\n",
    "    \n",
    "# Plot configurations\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.add_artist(first_legend)\n",
    "\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', width=2)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', width=1)  # Optionally adjust minor ticks\n",
    "\n",
    "# Get current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', labelsize =16)\n",
    "\n",
    "ax.set_xticks([60, 100, 140])\n",
    "ax.set_yticks([60, 100, 140])\n",
    "#ax.set_aspect('equal')\n",
    "\n",
    "plt.savefig(\"Amp.jpg\", dpi=600, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "############### Amp Plot End ################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f230f-f896-4d5a-8ac5-d118413423b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(ANN4_50000_test) # Store SLG_ANN test predictions\n",
    "df4 = pd.DataFrame(y_test_DS2) # Store ground truth for Data Set 2 Test Set\n",
    "\n",
    "############### EMAT Plot ################################################\n",
    "key = 'EMAT'\n",
    "\n",
    "y = df2[str(key)]\n",
    "x = df4[str(key)]\n",
    "\n",
    "x_line = np.linspace(min(x),max(x), 20)\n",
    "residuals = y - x  # Since we want lines of the form y = x + b\n",
    "\n",
    "data = residuals\n",
    "\n",
    "# Step 1: Calculate the histogram\n",
    "counts, bin_edges = np.histogram(data, bins=500, density=True)\n",
    "\n",
    "# Step 2: Calculate the cumulative sum (CDF)\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "cumulative_counts_normalized = cumulative_counts / cumulative_counts[-1]  # Normalize\n",
    "\n",
    "\n",
    "# Step 3: Find the x value at 97.5% CDF\n",
    "threshold1 = 0.975\n",
    "x_975 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold1)]\n",
    "\n",
    "# Step 3: Find the x value at 2.5% CDF\n",
    "threshold2 = 0.025\n",
    "x_025 = bin_edges[np.searchsorted(cumulative_counts_normalized, threshold2)]\n",
    "\n",
    "\n",
    "# Step 3: Calculate bin centers for plotting\n",
    "bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "# Step 4: Plot the histogram and the CDF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(bin_centers, counts, width=bin_edges[1]-bin_edges[0], edgecolor='black', alpha=0.7)\n",
    "plt.title('Histogram')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bin_centers, cumulative_counts_normalized, marker='o')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "########### Generate Scatter Plot ###################\n",
    "\n",
    "plt.xlim(min(x_line) , max(x_line)) # set limits on x in graph\n",
    "plt.ylim(min(x_line), max(x_line) ) # set limits on Y in graph\n",
    "# Calculate the upper and lower bounds for 95% of the data\n",
    "lower_bound = x_025\n",
    "upper_bound =  x_975\n",
    "\n",
    "# Calculate the intercepts\n",
    "intercept_lower = -lower_bound\n",
    "intercept_upper = -upper_bound\n",
    "\n",
    "l1 = plt.scatter(y_test_DS2[key], ANN4_50000_test[key], s=25, color='green', label='Data Set 2 Test Set'  )\n",
    "\n",
    "plt.plot(x_line, x_line, color='Black', linewidth= 3 )\n",
    "#plt.plot(x_line, .95 * x_line, color='Black', linestyle='--', linewidth= 3)\n",
    "#plt.plot(x_line, 1.05*x_line, color='Black', linestyle='--',  linewidth= 3)\n",
    "l2 = plt.plot(x_line, x_line + intercept_lower, color='Black', linestyle='--', linewidth= 3 )\n",
    "l3 = plt.plot(x_line, x_line + intercept_upper, color='Black', linestyle='--', linewidth= 3)\n",
    "\n",
    "plt.scatter(MV1530_params[str(key)], ANN4_50000_MV1530[str(key)], s=50, edgecolor='black', color='purple', marker='s', label='MV1530', zorder=2)\n",
    "\n",
    "print('##### 95% Confidence ########## ')\n",
    "print( key + ' ' +  str(round(upper_bound,5)))\n",
    "print( key + ' ' + str(round(lower_bound,5)))\n",
    "\n",
    "plt.legend(fontsize=16, frameon=False, loc=(-0.015, 0.8) )\n",
    "#plt.title(str(key) + ': ANN_50000')\n",
    "plt.xlabel('Ground Truth Surface Layer Thickness (nm)', fontsize=16)\n",
    "plt.ylabel('ANN Surface Layer Thickness (nm)', fontsize=16)\n",
    "\n",
    "    \n",
    "# Plot configurations\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', width=2)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', width=1)  # Optionally adjust minor ticks\n",
    "\n",
    "# Get current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', labelsize=16)\n",
    "\n",
    "ax.set_xticks([0.25, 1, 1.75])\n",
    "ax.set_yticks([0.25, 1, 1.75])\n",
    "#ax.set_aspect('equal')\n",
    "\n",
    "plt.savefig(\"EMAT.jpg\", dpi=600, bbox_inches='tight' )  # Saves the plot as a PDF file\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "############### EMAT Plot End ################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d819f974-bbc5-45f4-8c54-c88921547976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a4ee73e-e718-4a7a-991c-a8967f67f548",
   "metadata": {},
   "source": [
    "For the final exercises, the values predicted by ANN3 and ANN4 will be used as input for a least-square regression in python to confirm that the ANN predictions can lead to a good fit. \n",
    "\n",
    "Now a function that can take predictions from ANN3 and perfrom least-square regression will be defined as well as a least-square procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15205c86-03f3-4d64-b540-410029fdbe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The least-square regression will be done using python curve-fit\n",
    "\n",
    "# First an LSR will be built for handeling the different structural models \n",
    "\n",
    "def LSR_ANN3_model_func(x, Ep, Eg, Eo, Br, Amp, BulkT):\n",
    "\n",
    "    # Step 1: Take Parameters and make the CL material\n",
    "    CL = Get_CL_Material(E, Ep, Eg, Eo, Br, Amp, Eg, 1, wv) # make sure \"E\" and \"wv\" are defined before running this function\n",
    "    \n",
    "    # Step 2: Get Bulk Thickness\n",
    "    Bulk_Thickness = BulkT\n",
    "    NTVE_Thickness = NTVE_T # make sure to define NTVE_T before running\n",
    "    \n",
    "    # Step 3: Define Structure\n",
    "    Structure = [Void,  CL, NTVE_JAW, Si_JAW]\n",
    "    \n",
    "    # Step 4: Creat ellipsometric spectra \n",
    "    Mat_Thick = np.array( [ Bulk_Thickness, NTVE_Thickness])\n",
    "    Theta_Incident = 64.93\n",
    "    df = SE_Sim(Structure, Theta_Incident, Mat_Thick, write_data=False, NCS=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    data = df.to_numpy()\n",
    "    data = data.ravel(order='F') # lines up data in format [N1,N2,N3....C1,C2,C3....S1,S2,S3....]\n",
    "    data = data.tolist()\n",
    "    \n",
    "    return(data) # store prediction in the list for that model\n",
    "\n",
    "def LSR_ANN3(exp_data, initial_params):\n",
    "\n",
    "    x_data = np.concatenate((E, E, E))\n",
    "    y_data = exp_data.ravel(order='F')\n",
    "\n",
    "    lower_limit = 0.01\n",
    "    upper_limit = 1000\n",
    "\n",
    "    lower_bound =  np.full(initial_params.shape, lower_limit)\n",
    "    upper_bound =  np.full(initial_params.shape, upper_limit)\n",
    "     \n",
    "    params, covariance = curve_fit( LSR_ANN3_model_func, x_data, y_data, p0=initial_params, ftol=1e-3, xtol=1e-3, gtol=1e-3, bounds=(lower_bound, upper_bound) )\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    return(params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def LSR_ANN4_model_func(x, Ep, Eg, Eo, Br, Amp, BulkT, EMAT):\n",
    "\n",
    "    # Step 1: Take Parameters and make the CL material\n",
    "    CL = Get_CL_Material(E, Ep, Eg, Eo, Br, Amp, Eg, 1, wv) # make sure \"E\" and \"wv\" are defined before running this function\n",
    "    Surface_Layer = Bruggeman_EMA( CL, Void , 0.5 )\n",
    "    SLG_Interface = Bruggeman_EMA( CL, SLG , 0.5 )\n",
    "    \n",
    "    \n",
    "    # Step 2: Get Bulk Thickness\n",
    "    Bulk_Thickness = BulkT\n",
    "    Surface_Layer_Thickness = EMAT\n",
    "    SLG_Interface_Thickness = 3\n",
    "    \n",
    "    # Step 3: Define Structure\n",
    "    Structure = [Void, Surface_Layer, CL, SLG_Interface, SLG]\n",
    "    \n",
    "    # Step 4: Creat ellipsometric spectra \n",
    "    Mat_Thick = np.array( [  Surface_Layer_Thickness, Bulk_Thickness, SLG_Interface_Thickness])\n",
    "    Theta_Incident = 64.93\n",
    "    df = SE_Sim(Structure, Theta_Incident, Mat_Thick, write_data=False, NCS=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    data = df.to_numpy()\n",
    "    data = data.ravel(order='F') # lines up data in format [N1,N2,N3....C1,C2,C3....S1,S2,S3....]\n",
    "    data = data.tolist()\n",
    "    \n",
    "    return(data) # store prediction in the list for that model\n",
    "\n",
    "\n",
    "\n",
    "def LSR_ANN4(exp_data, initial_params):\n",
    "\n",
    "    x_data = np.concatenate((E, E, E))\n",
    "    y_data = exp_data.ravel(order='F')\n",
    "\n",
    "    lower_limit = 0.01\n",
    "    upper_limit = 1000\n",
    "\n",
    "    lower_bound =  np.full(initial_params.shape, lower_limit)\n",
    "    upper_bound =  np.full(initial_params.shape, upper_limit)\n",
    "     \n",
    "    params, covariance = curve_fit( LSR_ANN4_model_func, x_data, y_data, p0=initial_params, ftol=1e-3, xtol=1e-3, gtol=1e-3, bounds=(lower_bound, upper_bound) )\n",
    "    \n",
    "    print(params)\n",
    "    \n",
    "    return(params)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac9637-86fb-45c5-b5ef-8ae7a07eb246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to test the LSR functions \n",
    "\n",
    "NTVE_T = 1.68\n",
    "# Take predictions from ANN\n",
    "initial_params = np.array([ # initial parameters from ANNN3_100000 for 1st point in MV1519 map.\n",
    "     ANN3_100000_MV1519['Ep'][0], \n",
    "     ANN3_100000_MV1519['Eg'][0], \n",
    "     ANN3_100000_MV1519['Eo'][0], \n",
    "     ANN3_100000_MV1519['Br'][0],\n",
    "     ANN3_100000_MV1519['Amp'][0],\n",
    "     ANN3_100000_MV1519['BulkT'][0],\n",
    "     ])\n",
    "\n",
    "exp_data = MV1519[0][:, 1:] # 1st point on sample MV1519 map\n",
    "\n",
    "params = LSR_ANN3(exp_data, initial_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7747b7-1d65-45e0-9723-92165a04f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make one more set of functions to time the LSR. The time can be used to estimate performance. \n",
    "\n",
    "def LSR_ANN3_Timed(ANN3_predictions, experimental_data): \n",
    "\n",
    "    LSR_Params = [] # store final parameters after the LSR\n",
    "    LSR_Times = [] # store the times it takes to fit each point\n",
    "\n",
    "    for i in range(len(ANN3_predictions['Ep'])): # iteratre over every prediction\n",
    "\n",
    "        # Take predictions from ANN\n",
    "        initial_params = np.array([\n",
    "             ANN3_predictions['Ep'][i], \n",
    "             ANN3_predictions['Eg'][i], \n",
    "             ANN3_predictions['Eo'][i], \n",
    "             ANN3_predictions['Br'][i],\n",
    "             ANN3_predictions['Amp'][i],\n",
    "             ANN3_predictions['BulkT'][i],\n",
    "             ])\n",
    "        \n",
    "        timestamp1 = time.time() # start timer\n",
    "         # Perform least-square regression\n",
    "        params = LSR_ANN3(experimental_data[i][:, 1:] , initial_params )\n",
    "        timestamp2 = time.time() # end timer\n",
    "\n",
    "        iteration_time = timestamp2 - timestamp1 # calculate time delta\n",
    "        print(\"Iteration time: \" + str(iteration_time)) # print time delta\n",
    "\n",
    "        LSR_Params.append(params)  # save parameters \n",
    "        LSR_Times.append(iteration_time)  # save parameters \n",
    "\n",
    "    return(LSR_Params, LSR_Times)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7335236-74ed-4fb3-a8e5-5037c81537d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 100000 for MV1519\n",
    "NTVE_T = 1.68\n",
    "ANN3_100000_MV1519_LSR_params, ANN3_100000_MV1519_LSR_times = LSR_ANN3_Timed(ANN3_100000_MV1519, MV1519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9500e-9f0d-46c6-95b8-91d7daa83fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 50000 for MV1519\n",
    "NTVE_T = 1.68\n",
    "ANN3_50000_MV1519_LSR_params, ANN3_50000_MV1519_LSR_times = LSR_ANN3_Timed(ANN3_50000_MV1519, MV1519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42183a3a-456a-43d6-b24e-9e822491f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 10000 for MV1519\n",
    "NTVE_T = 1.68\n",
    "ANN3_10000_MV1519_LSR_params, ANN3_10000_MV1519_LSR_times = LSR_ANN3_Timed(ANN3_10000_MV1519, MV1519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb874a8-b9e4-4b73-af66-76e543f91400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 1000 for MV1519\n",
    "NTVE_T = 1.68\n",
    "ANN3_1000_MV1519_LSR_params, ANN3_1000_MV1519_LSR_times = LSR_ANN3_Timed(ANN3_1000_MV1519, MV1519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a000a7-cfb0-4be0-aa55-5483dd330d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 100 for MV1519\n",
    "NTVE_T = 1.68\n",
    "ANN3_100_MV1519_LSR_params, ANN3_100_MV1519_LSR_times = LSR_ANN3_Timed(ANN3_100_MV1519, MV1519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f9613-ec85-4893-b1f1-2bf6b1a1125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 10 for MV1519\n",
    "NTVE_T = 1.68\n",
    "ANN3_10_MV1519_LSR_params, ANN3_10_MV1519_LSR_times = LSR_ANN3_Timed(ANN3_10_MV1519, MV1519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83f084-8263-4aa8-aa4e-82aeef2bdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 100000 for MV1523\n",
    "NTVE_T = 1.65\n",
    "ANN3_100000_MV1523_LSR_params, ANN3_100000_MV1523_LSR_times = LSR_ANN3_Timed(ANN3_100000_MV1523, MV1523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a874d-fae0-4f78-ac73-34289730c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 50000 for MV1523\n",
    "NTVE_T = 1.65\n",
    "ANN3_50000_MV1523_LSR_params, ANN3_50000_MV1523_LSR_times = LSR_ANN3_Timed(ANN3_50000_MV1523, MV1523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5c1668-1383-49c9-83ad-a0980494c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 10000 for MV1523\n",
    "NTVE_T = 1.65\n",
    "ANN3_10000_MV1523_LSR_params, ANN3_10000_MV1523_LSR_times = LSR_ANN3_Timed(ANN3_10000_MV1523, MV1523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60936013-a5ee-4ca7-a936-6371e8c558cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 1000 for MV1523\n",
    "NTVE_T = 1.65\n",
    "ANN3_1000_MV1523_LSR_params, ANN3_1000_MV1523_LSR_times = LSR_ANN3_Timed(ANN3_1000_MV1523, MV1523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19663826-31f9-471e-8b14-c53a07a8350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 100 for MV1523\n",
    "NTVE_T = 1.65\n",
    "ANN3_100_MV1523_LSR_params, ANN3_100_MV1523_LSR_times = LSR_ANN3_Timed(ANN3_100_MV1523, MV1523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d99250-09f5-417f-87d5-4240c66a5759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN3 trained on 10 for MV1523\n",
    "NTVE_T = 1.65\n",
    "ANN3_10_MV1523_LSR_params, ANN3_10_MV1523_LSR_times = LSR_ANN3_Timed(ANN3_10_MV1523, MV1523)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c63b0-a42a-4efb-8de4-bbe4d2bd9d3e",
   "metadata": {},
   "source": [
    "Now a function that can take predictions from ANN4 and perfrom least-square regression will be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce32171-0fb1-4c37-9366-71eca2fa8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LSR_ANN4_Timed(ANN4_predictions, experimental_data): \n",
    "\n",
    "    LSR_Params = [] # store final parameters after the LSR\n",
    "    LSR_Times = [] # store the times it takes to fit each point\n",
    "\n",
    "    for i in range(len(ANN4_predictions['Ep'])): # iteratre over every prediction\n",
    "\n",
    "        # Take predictions from ANN\n",
    "        initial_params = np.array([\n",
    "             ANN4_predictions['Ep'][i], \n",
    "             ANN4_predictions['Eg'][i], \n",
    "             ANN4_predictions['Eo'][i], \n",
    "             ANN4_predictions['Br'][i],\n",
    "             ANN4_predictions['Amp'][i],\n",
    "             ANN4_predictions['BulkT'][i],\n",
    "             ANN4_predictions['EMAT'][i],\n",
    "             ])\n",
    "        \n",
    "        timestamp1 = time.time() # start timer\n",
    "         # Perform least-square regression\n",
    "        params = LSR_ANN4(experimental_data[i][:, 1:] , initial_params )\n",
    "        timestamp2 = time.time() # end timer\n",
    "\n",
    "        iteration_time = timestamp2 - timestamp1 # calculate time delta\n",
    "        print(\"Iteration time: \" + str(iteration_time)) # print time delta\n",
    "\n",
    "        LSR_Params.append(params)  # save parameters \n",
    "        LSR_Times.append(iteration_time)  # save parameters \n",
    "\n",
    "    return(LSR_Params, LSR_Times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d47213-0b02-492e-9fc5-d20ca5efaf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN4 trained on 100000 for MV1530\n",
    "ANN4_100000_MV1530_LSR_params, ANN4_100000_MV1530_LSR_times = LSR_ANN4_Timed(ANN4_100000_MV1530, MV1530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655dc21-deba-4817-b1d6-f7596b7e6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN4 trained on 50000 for MV1530\n",
    "ANN4_50000_MV1530_LSR_params, ANN4_50000_MV1530_LSR_times =  LSR_ANN4_Timed(ANN4_50000_MV1530, MV1530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786aeff0-0624-4e68-b7df-811d62e56bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN4 trained on 10000 for MV1530\n",
    "ANN4_10000_MV1530_LSR_params, ANN4_10000_MV1530_LSR_times = LSR_ANN4_Timed(ANN4_10000_MV1530, MV1530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba099baa-d754-40fe-8a64-7a01938445c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN4 trained on 1000 for MV1530\n",
    "ANN4_1000_MV1530_LSR_params, ANN4_1000_MV1530_LSR_times = LSR_ANN4_Timed(ANN4_1000_MV1530, MV1530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b4951-2d92-45ba-b7c2-8a6e6028c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN4 trained on 100 for MV1530\n",
    "ANN4_100_MV1530_LSR_params, ANN4_100_MV1530_LSR_times = LSR_ANN4_Timed(ANN4_100_MV1530, MV1530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4035a-cbd0-461a-9837-d9754e9c64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSR for ANN4 trained on 10 for MV1530\n",
    "ANN4_10_MV1530_LSR_params, ANN4_10_MV1530_LSR_times = LSR_ANN4_Timed(ANN4_10_MV1530, MV1530)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9921aa77-43b7-4c6a-bd94-2bc4da15bff5",
   "metadata": {},
   "source": [
    "Now all the data from this least-square regression process can be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab59c23-96a3-412a-8cc7-13e503d2475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data here from MV1530\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THE ANN + LSR DATA STORED####################\n",
    "\n",
    "columns = [ 'Ep', 'Eg', 'Eo', 'Br', 'Amp', 'BulkT', 'EMAT']\n",
    "\n",
    "####### Store Parameters from MV1530 analysis ###########\n",
    "ANN4_10_MV1530_LSR  = pd.DataFrame(ANN4_10_MV1530_LSR_params, columns = columns) \n",
    "ANN4_10_MV1530_LSR.to_csv('ANN4_10_MV1530_LSR_params.csv')\n",
    "\n",
    "ANN4_100_MV1530_LSR = pd.DataFrame(ANN4_100_MV1530_LSR_params, columns = columns) \n",
    "ANN4_100_MV1530_LSR.to_csv('ANN4_100_MV1530_LSR_params.csv')\n",
    "\n",
    "ANN4_1000_MV1530_LSR = pd.DataFrame(ANN4_1000_MV1530_LSR_params, columns = columns) \n",
    "ANN4_1000_MV1530_LSR.to_csv('ANN4_1000_MV1530_LSR_params.csv')\n",
    "\n",
    "ANN4_10000_MV1530_LSR = pd.DataFrame(ANN4_10000_MV1530_LSR_params, columns = columns) \n",
    "ANN4_10000_MV1530_LSR.to_csv('ANN4_10000_MV1530_LSR_params.csv')\n",
    "\n",
    "ANN4_50000_MV1530_LSR = pd.DataFrame(ANN4_50000_MV1530_LSR_params, columns = columns) \n",
    "ANN4_50000_MV1530_LSR.to_csv('ANN4_50000_MV1530_LSR_params.csv')\n",
    "\n",
    "ANN4_100000_MV1530_LSR = pd.DataFrame(ANN4_100000_MV1530_LSR_params, columns = columns) \n",
    "ANN4_100000_MV1530_LSR.to_csv('ANN4_100000_MV1530_LSR_params.csv')\n",
    "\n",
    "####### Store times from MV1530 analysis ###########\n",
    "df = pd.DataFrame(ANN4_10_MV1530_LSR_times) \n",
    "df.to_csv('ANN4_10_MV1530_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_100_MV1530_LSR_times) \n",
    "df.to_csv('ANN4_100_MV1530_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_1000_MV1530_LSR_times) \n",
    "df.to_csv('ANN4_1000_MV1530_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_10000_MV1530_LSR_times) \n",
    "df.to_csv('ANN4_10000_MV1530_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_50000_MV1530_LSR_times) \n",
    "df.to_csv('ANN4_50000_MV1530_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN4_100000_MV1530_LSR_times) \n",
    "df.to_csv('ANN4_100000_MV1530_LSR_times.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6e905-a468-4012-a70a-e7e77c15be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data here from MV1519\n",
    "\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THE ANN + LSR DATA STORED####################\n",
    "\n",
    "columns = [ 'Ep', 'Eg', 'Eo', 'Br', 'Amp', 'BulkT']\n",
    "\n",
    "####### Store Parameters from MV1519 analysis ###########\n",
    "ANN3_10_MV1519_LSR = pd.DataFrame(ANN3_10_MV1519_LSR_params, columns = columns) \n",
    "ANN3_10_MV1519_LSR.to_csv('ANN3_10_MV1519_LSR_params.csv')\n",
    "\n",
    "ANN3_100_MV1519_LSR = pd.DataFrame(ANN3_100_MV1519_LSR_params, columns = columns) \n",
    "ANN3_100_MV1519_LSR.to_csv('ANN3_100_MV1519_LSR_params.csv')\n",
    "\n",
    "ANN3_1000_MV1519_LSR = pd.DataFrame(ANN3_1000_MV1519_LSR_params, columns = columns) \n",
    "ANN3_1000_MV1519_LSR.to_csv('ANN3_1000_MV1519_LSR_params.csv')\n",
    "\n",
    "ANN3_10000_MV1519_LSR = pd.DataFrame(ANN3_10000_MV1519_LSR_params, columns = columns) \n",
    "ANN3_10000_MV1519_LSR.to_csv('ANN3_10000_MV1519_LSR_params.csv')\n",
    "\n",
    "ANN3_50000_MV1519_LSR = pd.DataFrame(ANN3_50000_MV1519_LSR_params, columns = columns) \n",
    "ANN3_50000_MV1519_LSR.to_csv('ANN3_50000_MV1519_LSR_params.csv')\n",
    "\n",
    "ANN3_100000_MV1519_LSR = pd.DataFrame(ANN3_100000_MV1519_LSR_params, columns = columns) \n",
    "ANN3_100000_MV1519_LSR.to_csv('ANN3_100000_MV1519_LSR_params.csv')\n",
    "\n",
    "####### Store times from MV1519 analysis ###########\n",
    "df = pd.DataFrame(ANN3_10_MV1519_LSR_times) \n",
    "df.to_csv('ANN3_10_MV1519_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100_MV1519_LSR_times) \n",
    "df.to_csv('ANN3_100_MV1519_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_1000_MV1519_LSR_times) \n",
    "df.to_csv('ANN3_1000_MV1519_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_10000_MV1519_LSR_times) \n",
    "df.to_csv('ANN3_10000_MV1519_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_50000_MV1519_LSR_times) \n",
    "df.to_csv('ANN3_50000_MV1519_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100000_MV1519_LSR_times) \n",
    "df.to_csv('ANN3_100000_MV1519_LSR_times.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd647e0a-2bd3-4556-a76d-43e59896a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data here from MV1523 \n",
    "\n",
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THE ANN + LSR DATA STORED####################\n",
    "\n",
    "columns = [ 'Ep', 'Eg', 'Eo', 'Br', 'Amp', 'BulkT']\n",
    "\n",
    "####### Store Parameters from MV1523 analysis ###########\n",
    "ANN3_10_MV1523_LSR = pd.DataFrame(ANN3_10_MV1523_LSR_params, columns = columns) \n",
    "ANN3_10_MV1523_LSR.to_csv('ANN3_10_MV1523_LSR_params.csv')\n",
    "\n",
    "ANN3_100_MV1523_LSR = pd.DataFrame(ANN3_100_MV1523_LSR_params, columns = columns) \n",
    "ANN3_100_MV1523_LSR.to_csv('ANN3_100_MV1523_LSR_params.csv')\n",
    "\n",
    "ANN3_1000_MV1523_LSR = pd.DataFrame(ANN3_1000_MV1523_LSR_params, columns = columns) \n",
    "ANN3_1000_MV1523_LSR.to_csv('ANN3_1000_MV1523_LSR_params.csv')\n",
    "\n",
    "ANN3_10000_MV1523_LSR = pd.DataFrame(ANN3_10000_MV1523_LSR_params, columns = columns) \n",
    "ANN3_10000_MV1523_LSR.to_csv('ANN3_10000_MV1523_LSR_params.csv')\n",
    "\n",
    "ANN3_50000_MV1523_LSR = pd.DataFrame(ANN3_50000_MV1523_LSR_params, columns = columns) \n",
    "ANN3_50000_MV1523_LSR.to_csv('ANN3_50000_MV1523_LSR_params.csv')\n",
    "\n",
    "ANN3_100000_MV1523_LSR = pd.DataFrame(ANN3_100000_MV1523_LSR_params, columns = columns) \n",
    "ANN3_100000_MV1523_LSR.to_csv('ANN3_100000_MV1523_LSR_params.csv')\n",
    "\n",
    "####### Store times from MV1523 analysis ###########\n",
    "df = pd.DataFrame(ANN3_10_MV1523_LSR_times) \n",
    "df.to_csv('ANN3_10_MV1523_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100_MV1523_LSR_times) \n",
    "df.to_csv('ANN3_100_MV1523_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_1000_MV1523_LSR_times) \n",
    "df.to_csv('ANN3_1000_MV1523_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_10000_MV1523_LSR_times) \n",
    "df.to_csv('ANN3_10000_MV1523_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_50000_MV1523_LSR_times) \n",
    "df.to_csv('ANN3_50000_MV1523_LSR_times.csv')\n",
    "\n",
    "df = pd.DataFrame(ANN3_100000_MV1523_LSR_times) \n",
    "df.to_csv('ANN3_100000_MV1523_LSR_times.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39931e9-4a8b-4635-b112-7a2183a4ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the total time spent iterating each sample\n",
    "\n",
    "Total_Time_MV1519 = [\n",
    "    sum(ANN3_100000_MV1519_LSR_times),\n",
    "    sum(ANN3_50000_MV1519_LSR_times),\n",
    "    sum(ANN3_10000_MV1519_LSR_times),\n",
    "    sum(ANN3_1000_MV1519_LSR_times),\n",
    "    sum(ANN3_100_MV1519_LSR_times),\n",
    "    sum(ANN3_10_MV1519_LSR_times)\n",
    "]\n",
    "\n",
    "Total_Time_MV1523 = [\n",
    "    sum(ANN3_100000_MV1523_LSR_times),\n",
    "    sum(ANN3_50000_MV1523_LSR_times),\n",
    "    sum(ANN3_10000_MV1523_LSR_times),\n",
    "    sum(ANN3_1000_MV1523_LSR_times),\n",
    "    sum(ANN3_100_MV1523_LSR_times),\n",
    "    sum(ANN3_10_MV1523_LSR_times)\n",
    "]\n",
    "\n",
    "Total_Time_MV1530 = [\n",
    "    sum(ANN4_100000_MV1530_LSR_times),\n",
    "    sum(ANN4_50000_MV1530_LSR_times),\n",
    "    sum(ANN4_10000_MV1530_LSR_times),\n",
    "    sum(ANN4_1000_MV1530_LSR_times),\n",
    "    sum(ANN4_100_MV1530_LSR_times),\n",
    "    sum(ANN4_10_MV1530_LSR_times)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae109b23-2467-4f44-88ad-28fd3d840fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_Time_MV1530"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77cabf-2f0c-4098-a91d-26667371049b",
   "metadata": {},
   "source": [
    "Now make a figure to compare computational times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8c5a0-aabb-41a7-8dc8-8c0a6371cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'XXXX') ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THE FIGURES STORED####################\n",
    "\n",
    "training_data = [100000, 50000, 10000, 1000, 100, 10]\n",
    "plt.scatter(training_data,Total_Time_MV1519, s=100, edgecolor='black', color='lime' , marker='o', label='MV1519')\n",
    "plt.scatter(training_data,Total_Time_MV1523, s=100,edgecolor='black', color='red', marker='^', label='MV1523')\n",
    "plt.scatter(training_data,Total_Time_MV1530, s=100, edgecolor='black', color='purple', marker='s', label='MV1530')\n",
    "plt.xscale('log')\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the linewidth of the axis lines\n",
    "ax.spines['top'].set_linewidth(2)\n",
    "ax.spines['right'].set_linewidth(2)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "plt.yticks([250, 500, 750])  # Specify the desired tick values\n",
    "plt.ylim(0, 1000)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', direction='in', width=3, labelsize =18, length=4)  # Adjust 'width' for thickness\n",
    "plt.tick_params(axis='both', which='minor', direction='in', width=3, labelsize =18, length=4)  # Optionally adjust minor ticks\n",
    "\n",
    "plt.legend(fontsize=18, frameon=False, loc=(0.6, 0.675))\n",
    "#plt.title(str(key) + ': ANN_10')\n",
    "plt.xlabel('Training Set Size', fontsize=18)\n",
    "plt.ylabel('Computation Time (s)', fontsize=18)\n",
    "\n",
    "\n",
    "# Get current axes\n",
    "ax = plt.gca()\n",
    "\n",
    "# Set the tick marks to point inside\n",
    "ax.tick_params(axis='both', direction='in', pad = 5, labelsize=18)\n",
    "plt.savefig(\"ANN_LSR_Time.jpg\", dpi=600, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5a311-af40-4eb7-98b5-0c7dbd7e08f7",
   "metadata": {},
   "source": [
    "Now lets take the new LSR predictions and calculate the unweighted error.\n",
    "\n",
    "The next cell re-loads stored data from the previous LSR analysis. This does not need to be used if the varaibles are already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26050150-7d76-4060-a44a-1bab65330223",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE THE ANN + LSR DATA IS STORED IF YOU RUN THIS BLOCK ####################\n",
    "\n",
    "# Start with MV1519 params\n",
    "ANN3_10_MV1519_LSR_params = pd.read_csv('ANN3_10_MV1519_LSR_params.csv')\n",
    "ANN3_100_MV1519_LSR_params = pd.read_csv('ANN3_100_MV1519_LSR_params.csv')\n",
    "ANN3_1000_MV1519_LSR_params = pd.read_csv('ANN3_1000_MV1519_LSR_params.csv')\n",
    "ANN3_10000_MV1519_LSR_params = pd.read_csv('ANN3_10000_MV1519_LSR_params.csv')\n",
    "ANN3_50000_MV1519_LSR_params = pd.read_csv('ANN3_50000_MV1519_LSR_params.csv')\n",
    "ANN3_100000_MV1519_LSR_params = pd.read_csv('ANN3_100000_MV1519_LSR_params.csv')\n",
    "\n",
    "# MV1519 times\n",
    "\n",
    "ANN3_10_MV1519_LSR_times = pd.read_csv( 'ANN3_10_MV1519_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_100_MV1519_LSR_times = pd.read_csv( 'ANN3_100_MV1519_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_1000_MV1519_LSR_times = pd.read_csv( 'ANN3_1000_MV1519_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_10000_MV1519_LSR_times = pd.read_csv( 'ANN3_10000_MV1519_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_50000_MV1519_LSR_times = pd.read_csv( 'ANN3_50000_MV1519_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_100000_MV1519_LSR_times = pd.read_csv( 'ANN3_100000_MV1519_LSR_times.csv')['0'].to_numpy()\n",
    "\n",
    "# MV1523 params\n",
    "\n",
    "ANN3_10_MV1523_LSR_params = pd.read_csv('ANN3_10_MV1523_LSR_params.csv')\n",
    "ANN3_100_MV1523_LSR_params = pd.read_csv('ANN3_100_MV1523_LSR_params.csv')\n",
    "ANN3_1000_MV1523_LSR_params = pd.read_csv('ANN3_1000_MV1523_LSR_params.csv')\n",
    "ANN3_10000_MV1523_LSR_params = pd.read_csv('ANN3_10000_MV1523_LSR_params.csv')\n",
    "ANN3_50000_MV1523_LSR_params = pd.read_csv('ANN3_50000_MV1523_LSR_params.csv')\n",
    "ANN3_100000_MV1523_LSR_params = pd.read_csv('ANN3_100000_MV1523_LSR_params.csv')\n",
    "\n",
    "# MV1523 times\n",
    "\n",
    "ANN3_10_MV1523_LSR_times = pd.read_csv( 'ANN3_10_MV1523_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_100_MV1523_LSR_times = pd.read_csv( 'ANN3_100_MV1523_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_1000_MV1523_LSR_times = pd.read_csv( 'ANN3_1000_MV1523_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_10000_MV1523_LSR_times = pd.read_csv( 'ANN3_10000_MV1523_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_50000_MV1523_LSR_times = pd.read_csv( 'ANN3_50000_MV1523_LSR_times.csv')['0'].to_numpy()\n",
    "ANN3_100000_MV1523_LSR_times = pd.read_csv( 'ANN3_100000_MV1523_LSR_times.csv')['0'].to_numpy()\n",
    "\n",
    "# MV1530 params\n",
    "\n",
    "ANN4_10_MV1530_LSR_params = pd.read_csv('ANN4_10_MV1530_LSR_params.csv')\n",
    "ANN4_100_MV1530_LSR_params = pd.read_csv('ANN4_100_MV1530_LSR_params.csv')\n",
    "ANN4_1000_MV1530_LSR_params = pd.read_csv('ANN4_1000_MV1530_LSR_params.csv')\n",
    "ANN4_10000_MV1530_LSR_params = pd.read_csv('ANN4_10000_MV1530_LSR_params.csv')\n",
    "ANN4_50000_MV1530_LSR_params = pd.read_csv('ANN4_50000_MV1530_LSR_params.csv')\n",
    "ANN4_100000_MV1530_LSR_params = pd.read_csv('ANN4_100000_MV1530_LSR_params.csv')\n",
    "\n",
    "# MV1539 times\n",
    "\n",
    "ANN4_10_MV1530_LSR_times = pd.read_csv( 'ANN4_10_MV1530_LSR_times.csv')['0'].to_numpy()\n",
    "ANN4_100_MV1530_LSR_times = pd.read_csv( 'ANN4_100_MV1530_LSR_times.csv')['0'].to_numpy()\n",
    "ANN4_1000_MV1530_LSR_times = pd.read_csv( 'ANN4_1000_MV1530_LSR_times.csv')['0'].to_numpy()\n",
    "ANN4_10000_MV1530_LSR_times = pd.read_csv( 'ANN4_10000_MV1530_LSR_times.csv')['0'].to_numpy()\n",
    "ANN4_50000_MV1530_LSR_times = pd.read_csv( 'ANN4_50000_MV1530_LSR_times.csv')['0'].to_numpy()\n",
    "ANN4_100000_MV1530_LSR_times = pd.read_csv( 'ANN4_100000_MV1530_LSR_times.csv')['0'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d928808-fc1e-408c-a4d7-75e587bdf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MV1519_list_LSR = [\n",
    "    ANN3_10_MV1519_LSR_params, \n",
    "    ANN3_100_MV1519_LSR_params, \n",
    "    ANN3_1000_MV1519_LSR_params,\n",
    "    ANN3_10000_MV1519_LSR_params, \n",
    "    ANN3_50000_MV1519_LSR_params, \n",
    "    ANN3_100000_MV1519_LSR_params,\n",
    "]\n",
    "\n",
    "ANN3_MV1519_LSR_NCS, ANN3_MV1519_LSR_CL = NCS_Genorator_ANN3(MV1519_list_LSR, 1.68, E, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c50b45-bd52-4ea1-8757-4f8ce720ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generating error between ANN3 + LSR and experiment and LSR and experiment ################\n",
    "\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED ####################\n",
    "\n",
    "# If you do not want to make plots, set \"generate_images=False\"\n",
    "# If you do want to make plots, set \"generate_images=True\"\n",
    "\n",
    "MV1519_Sigma_PostLSR, MV1519_LSR_Sigma_PostLSR = NCS_Comparison_ANN3(ANN3_MV1519_LSR_NCS, # simulated spectra from ANN + LSR\n",
    "                                                                     MV1519[:, :, 1:], # excperimental data\n",
    "                                                                     LSR_MV1519_NCS,  # traditional LSR data\n",
    "                                                                     6, # number of fit parameters\n",
    "                                                                     path, # path to save images\n",
    "                                                                     generate_images=False, # makes images when true\n",
    "                                                                     foldername = 'MV1519_PostLSR') # name for the folder created to contain images. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc7774-516c-4c99-b5e3-0908e2c0400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MV1523_list_LSR = [\n",
    "    ANN3_10_MV1523_LSR_params, \n",
    "    ANN3_100_MV1523_LSR_params, \n",
    "    ANN3_1000_MV1523_LSR_params,\n",
    "    ANN3_10000_MV1523_LSR_params, \n",
    "    ANN3_50000_MV1523_LSR_params, \n",
    "    ANN3_50000_MV1523_LSR_params,\n",
    "]\n",
    "\n",
    "ANN3_MV1523_LSR_NCS, ANN3_MV1523_LSR_CL = NCS_Genorator_ANN3(MV1523_list_LSR, 1.65, E, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eae076-89d9-4e7f-94f0-499805a8b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generating error between ANN3 + LSR and experiment and LSR and experiment ################\n",
    "\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED ####################\n",
    "\n",
    "# If you do not want to make plots, set \"generate_images=False\"\n",
    "# If you do want to make plots, set \"generate_images=True\"\n",
    "\n",
    "MV1523_Sigma_PostLSR, MV1523_LSR_Sigma_PostLSR = NCS_Comparison_ANN3(ANN3_MV1523_LSR_NCS, # simulated spectra from ANN + LSR\n",
    "                                                                     MV1523[:, :, 1:], # excperimental data\n",
    "                                                                     LSR_MV1523_NCS,  # traditional LSR data\n",
    "                                                                     6, # number of fit parameters\n",
    "                                                                     path, # path to save images\n",
    "                                                                     generate_images=False, # makes images when true\n",
    "                                                                     foldername = 'MV1523_PostLSR') # name for the folder created to contain images. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11af6a0-e01e-4302-9f0b-71ad63dad0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MV1530_list_LSR = [\n",
    "    ANN4_10_MV1530_LSR_params, \n",
    "    ANN4_100_MV1530_LSR_params, \n",
    "    ANN4_1000_MV1530_LSR_params,\n",
    "    ANN4_10000_MV1530_LSR_params, \n",
    "    ANN4_50000_MV1530_LSR_params, \n",
    "    ANN4_50000_MV1530_LSR_params,\n",
    "]\n",
    "\n",
    "ANN4_MV1530_LSR_NCS, ANN4_MV1530_LSR_CL =  NCS_Genorator_ANN4(MV1530_list_LSR, E, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b9a9c-4917-4ee7-8600-60b1da8577f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Generating error between ANN4 + LSR and experiment and LSR and experiment ################\n",
    "\n",
    "path = r'XXXX'  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED ####################\n",
    "\n",
    "# If you do not want to make plots, set \"generate_images=False\"\n",
    "# If you do want to make plots, set \"generate_images=True\"\n",
    "\n",
    "MV1530_Sigma_PostLSR, MV1530_LSR_Sigma_PostLSR = NCS_Comparison_ANN4_v2(ANN4_MV1530_LSR_NCS,  # simulated spectra from ANN + LSR\n",
    "                                                                        MV1530[:, :, 1:], # excperimental data\n",
    "                                                                        LSR_MV1530_NCS, # traditional LSR data\n",
    "                                                                        7, # number of fit parameters\n",
    "                                                                        path, # path to save images\n",
    "                                                                        generate_images=False, # makes images when true\n",
    "                                                                        foldername = 'MV1530_PostLSR' # name for the folder created to contain images. \n",
    "                                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b9863-2ce9-442d-97ac-925f5eb41752",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = MV1519_Sigma_PostLSR\n",
    "data1.append(MV1519_LSR_Sigma[0])\n",
    "\n",
    "data2 = MV1523_Sigma_PostLSR\n",
    "data2.append(MV1523_LSR_Sigma[0])\n",
    "\n",
    "data3 = MV1530_Sigma_PostLSR\n",
    "data3.append(MV1530_LSR_Sigma[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4300ee-861c-4449-9bcf-fe87fd3deed0",
   "metadata": {},
   "outputs": [],
   "source": [
    " MV1519_Sigma_PostLSR[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba210ae-8af7-4ac4-a9fe-db319ebbb465",
   "metadata": {},
   "source": [
    "The following code displays the sigma values for each data point after the ANN predictions are fed into the least-square regression process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce9d2d-c2ad-49e0-bcd8-659cb2aaaabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'XXXX')  ########## PLEASE PUT THE DIRECTORY WHERE YOU WOULD LIKE TO HAVE THESE PLOTS STORED ####################\n",
    "\n",
    "font_properties = {'fontsize': 16, 'fontweight': 'normal', 'family': 'sans-serif'}\n",
    "\n",
    "# Create a figure and axes for the subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8))\n",
    "\n",
    "tick_labels = ['$10^{1}$ ', '$10^{2}$', '$10^{3}$' , '$10^{4}$', r'$5 \\times 10^{4}$', '$10^{5}$', 'LSR']\n",
    "\n",
    "# Create box plots for the first dataset\n",
    "boxplot1 = axs[0].boxplot(data1,       \n",
    "            vert=True, \n",
    "            patch_artist=True, \n",
    "            flierprops=dict(marker='', color='red', alpha=0.5),\n",
    "            boxprops=dict(linewidth=3),\n",
    "            whiskerprops=dict(linewidth=3), \n",
    "            medianprops=dict(color='black', linewidth=3))  # Vertical box plots\n",
    "axs[0].set_title('(a)', fontsize=18, loc='left', y=0.85,x = 0.01)\n",
    "axs[0].set_ylabel(r'$ \\times 10^{3}$', fontsize=18, labelpad=10)\n",
    "axs[0].set_ylim(-0.002, 0.032)\n",
    "axs[0].set_yticks(np.arange(0.00, 0.031, 0.015))\n",
    "axs[0].set_xticklabels([])\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)  # Adjust labelsize as needed\n",
    "#custom_ticks = np.linspace(0.05, 0.03, 6) = 14\n",
    "#axs[0].set_ylim(-0.005, 0.03)\n",
    "#axs[0].set_yticks(custom_ticks)\n",
    "\n",
    "\n",
    "# Create box plots for the second dataset\n",
    "boxplot2 = axs[1].boxplot(data2,        \n",
    "            vert=True, \n",
    "            patch_artist=True, \n",
    "            flierprops=dict(marker='', color='red', alpha=0.5),\n",
    "            boxprops=dict(linewidth=3),\n",
    "            whiskerprops=dict(linewidth=3), \n",
    "            medianprops=dict(color='black', linewidth=3))\n",
    "axs[1].set_title('(b)', fontsize=18, loc='left', y=0.85,x = 0.01)\n",
    "axs[1].set_ylim(-0.002, 0.032)\n",
    "axs[1].set_yticks(np.arange(0.00, 0.031, 0.015))\n",
    "axs[1].set_ylabel(r'$ \\times 10^{3}$', fontsize=18)\n",
    "axs[1].set_xticklabels([])\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)  # Adjust labelsize as needed\n",
    "#axs[1].set_ylim(-3, 4)\n",
    "\n",
    "# Create box plots for the third dataset\n",
    "boxplot3 = axs[2].boxplot(data3,    \n",
    "            vert=True, \n",
    "            patch_artist=True, \n",
    "            flierprops=dict(marker='', color='red', alpha=0.5),\n",
    "            boxprops=dict(linewidth=3),\n",
    "            whiskerprops=dict(linewidth=3), \n",
    "            medianprops=dict(color='black', linewidth=3))\n",
    "axs[2].set_title('(c)', fontsize=18, loc='left', y=0.85,x = 0.01)\n",
    "axs[2].set_ylabel(r'$ \\times 10^{3}$', fontsize=18, labelpad=10)\n",
    "axs[2].set_xticklabels(tick_labels, fontdict=font_properties)\n",
    "axs[2].tick_params(axis='both', which='major', labelsize=18, pad=5)  # Adjust labelsize as needed\n",
    "custom_ticks = np.linspace(0.0015, 0.0035, 3) \n",
    "axs[2].set_xlabel('ANN training size', fontsize=18)\n",
    "axs[2].set_yticks(custom_ticks)\n",
    "axs[2].set_ylim(0.0013, 0.0037)\n",
    "\n",
    "# Hide the box and whisker lines\n",
    "for box in boxplot1['boxes']:\n",
    "    box.set_visible(False)  # Hide the box\n",
    "\n",
    "for whisker in boxplot1['whiskers']:\n",
    "    whisker.set_visible(False)  # Hide the whiskers\n",
    "\n",
    "# Optional: You can still display the median and fliers\n",
    "for median in boxplot1['medians']:\n",
    "    median.set_visible(False)  # Keep the median line visible\n",
    "\n",
    "for flier in boxplot1['fliers']:\n",
    "    flier.set_visible(False)  # Keep the outliers visible\n",
    "\n",
    "for cap in boxplot1['caps']:\n",
    "    cap.set_visible(False)  # Hide the caps\n",
    "\n",
    "# Hide the box and whisker lines\n",
    "for box in boxplot2['boxes']:\n",
    "    box.set_visible(False)  # Hide the box\n",
    "\n",
    "for whisker in boxplot2['whiskers']:\n",
    "    whisker.set_visible(False)  # Hide the whiskers\n",
    "\n",
    "# Optional: You can still display the median and fliers\n",
    "for median in boxplot2['medians']:\n",
    "    median.set_visible(False)  # Keep the median line visible\n",
    "\n",
    "for flier in boxplot2['fliers']:\n",
    "    flier.set_visible(False)  # Keep the outliers visible\n",
    "\n",
    "for cap in boxplot2['caps']:\n",
    "    cap.set_visible(False)  # Hide the caps\n",
    "\n",
    "\n",
    "# Hide the box and whisker lines\n",
    "for box in boxplot3['boxes']:\n",
    "    box.set_visible(False)  # Hide the box\n",
    "\n",
    "for whisker in boxplot3['whiskers']:\n",
    "    whisker.set_visible(False)  # Hide the whiskers\n",
    "\n",
    "# Optional: You can still display the median and fliers\n",
    "for median in boxplot3['medians']:\n",
    "    median.set_visible(False)  # Keep the median line visible\n",
    "\n",
    "for flier in boxplot3['fliers']:\n",
    "    flier.set_visible(False)  # Keep the outliers visible\n",
    "\n",
    "for cap in boxplot3['caps']:\n",
    "    cap.set_visible(False)  # Hide the caps\n",
    "\n",
    "\n",
    "for ax in axs:\n",
    "    ax.spines['top'].set_linewidth(2)\n",
    "    ax.spines['right'].set_linewidth(2)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    # Make ticks point inward\n",
    "    ax.tick_params(direction='in', width=2)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(data1)):\n",
    "    axs[0].scatter([i + 1] * len(data1[i]), data1[i], color='red', marker='o', edgecolor = 'black')\n",
    "    axs[1].scatter([i + 1] * len(data2[i]), data2[i], color='red', marker='o', edgecolor = 'black')\n",
    "    axs[2].scatter([i + 1] * len(data3[i]), data3[i], color='red', marker='o', edgecolor = 'black')\n",
    "\n",
    "\n",
    "#for i, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    #if label.get_text() == 'LSR':\n",
    "        #label.set_y(label.get_position()[1] - 0.01)  # Move Group A up\n",
    "\n",
    "\n",
    "\n",
    "def format_ticks(x, _):\n",
    "    return f'{x * 1000:.1f}'  # Multiply by scale_factor\n",
    "\n",
    "for ax in axs:\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
    "\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(\"ANN_Condensed_Post_LSR\" + str(i) + \".jpg\",  dpi=600, bbox_inches='tight' )\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19678e5-b750-4988-9b35-f980843f3b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
